token_embeddings.weight — shape: torch.Size([211, 128])
[[-1.9640104  -0.7940494  -1.3458602  ... -0.26555422  0.13671151
  -0.9488208 ]
 [-0.18930769  0.19312952  0.4339469  ... -1.6247053  -1.8029634
   0.70762295]
 [-0.8242424   0.22980615 -0.36797497 ... -0.35732427  0.11197942
   0.20689589]
 ...
 [-2.394869   -0.1570585   0.8777702  ...  0.7536662  -0.17559053
  -1.2533938 ]
 [-0.31669176  0.179971    0.28040582 ... -0.74047446  0.16919419
   1.0568763 ]
 [ 0.08392887 -1.3334854   0.06674457 ...  1.3604987   0.23573202
   0.26477262]]

position_embeddings.weight — shape: torch.Size([1024, 128])
[[ 0.54626554 -0.99413854 -1.4114693  ...  0.8466793   2.1064243
  -0.21438012]
 [ 1.9454776  -1.2985336   1.1296368  ... -2.0213215  -0.79167044
  -0.7476671 ]
 [ 0.51876324 -0.7427199   1.328911   ...  0.8734615  -0.19223511
  -1.4166    ]
 ...
 [-0.65732205 -0.37755877 -1.755893   ... -0.14761324 -0.6508729
  -0.38722393]
 [ 0.40622962 -1.4260135  -0.07639732 ... -0.4452757   0.89089066
  -0.18630499]
 [-0.02379882 -0.32566795 -1.4120972  ...  0.6175935   0.6466119
  -0.0815871 ]]

transformer_blocks.0.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.08779027 -0.00786298  0.01325329 ...  0.0331051   0.06269176
  -0.03091722]
 [-0.0957406   0.03955565 -0.06679429 ... -0.02542054 -0.01058556
   0.04197661]
 [ 0.03169679  0.03088368  0.08001851 ...  0.04427264  0.06753088
  -0.0374663 ]
 ...
 [ 0.085207   -0.0278345  -0.00538642 ...  0.00598726 -0.02100793
  -0.08756535]
 [-0.02569207 -0.0182762   0.03408768 ...  0.03721775 -0.06907699
  -0.03257693]
 [-0.08528081 -0.01464439 -0.03206809 ...  0.02206252  0.01234716
   0.01982289]]

transformer_blocks.0.attention.query_key_value.bias — shape: torch.Size([384])
[-0.02827242 -0.05502935  0.04289993  0.0982301  -0.03116957 -0.09221645
  0.0392142   0.00622615 -0.03557331  0.07457003  0.07565211  0.0288842
 -0.02250188 -0.0059111   0.1009472   0.06801909 -0.07923229 -0.01295423
  0.05297421 -0.012642    0.01609857  0.05372676 -0.00228686 -0.01094592
  0.01214291 -0.06972459 -0.03096882 -0.03289066  0.00407547  0.04025349
 -0.06226655 -0.06107694 -0.03408132  0.05452878  0.01600358  0.02455806
  0.04909788 -0.00363706 -0.08695348  0.04589483 -0.01155289  0.04793195
 -0.04502265  0.01784064 -0.04767204  0.04259897 -0.06865824 -0.02778001
  0.08168194  0.05011348 -0.00517773  0.06784257 -0.06857599 -0.04760737
 -0.07787083  0.06977084  0.00625556 -0.06572246 -0.05753555 -0.03280285
 -0.06260335 -0.08795476 -0.00029665  0.03121291  0.02171053  0.02232643
  0.00629703 -0.00081846  0.07318391  0.01397921  0.08903963  0.01029693
  0.09531721  0.06101802  0.08389515 -0.06745379  0.05543305  0.04640669
  0.0583988   0.0499061  -0.02941399 -0.00953963  0.01284834 -0.10245153
 -0.047802   -0.07029501  0.08169541 -0.05861944 -0.04140187 -0.0792166
  0.02701044  0.06849031 -0.04640881  0.0647537  -0.03338651 -0.0891663
 -0.04008346  0.01986651 -0.09067243 -0.0040242  -0.09321783  0.01118878
  0.01445255 -0.00483255 -0.04276847 -0.06925498  0.04651751 -0.05271048
  0.06208326 -0.00018454  0.09503991 -0.03938675  0.02364009 -0.05450099
 -0.06319138  0.00370984  0.02596064 -0.03061924 -0.07709076  0.0658294
 -0.02926297  0.05867723  0.02659009 -0.01733215 -0.04481209 -0.08165787
 -0.06005143  0.02936873 -0.04499917 -0.02927563 -0.01239698 -0.0051164
  0.04423709  0.07814749 -0.00714618 -0.02134218 -0.07480919  0.04872525
 -0.07906043 -0.04214479 -0.048366    0.03415408  0.00632536  0.04780528
 -0.01938004  0.05939581  0.04395831 -0.0412207  -0.07912073 -0.04128416
  0.08047041  0.04268037 -0.00444691 -0.04074461  0.0847111   0.00023279
  0.02789071 -0.02660434  0.0399292  -0.02525062  0.03078199 -0.02208496
  0.05470333 -0.05052645  0.03625413  0.08078488 -0.04687252  0.00124878
  0.0709896  -0.00511143  0.009324   -0.08445039  0.00424546  0.08376078
  0.04518644 -0.03487504  0.01937295  0.03946529  0.05264568 -0.04220203
 -0.01066054  0.07399575  0.08060006 -0.05532166 -0.00379315 -0.0663461
 -0.02957427  0.05661963 -0.0286151  -0.04180573  0.05964128  0.04350743
  0.03223396 -0.00021841  0.0856291  -0.002163    0.02434285 -0.06978524
 -0.04215078 -0.00424814  0.07066503  0.0172245   0.08372099 -0.0764122
  0.05974667 -0.00213206 -0.05590489 -0.02805327 -0.08089292 -0.04932458
 -0.00974718  0.07410012  0.06697057  0.03881502  0.05839991  0.08118675
  0.05996129 -0.04475727 -0.02157802 -0.03845252  0.02245844 -0.01923669
 -0.07290392  0.0302083  -0.00490738 -0.00627473 -0.08542918 -0.01145946
  0.07477532  0.00838068 -0.01336861  0.04319456 -0.08205865 -0.05879568
 -0.0618252  -0.02523407 -0.07274619 -0.07437765  0.03262692 -0.05116076
 -0.07161501 -0.02078142 -0.0089272   0.04906997  0.05908109  0.07006935
  0.0394976   0.03615751  0.03083677 -0.07188165  0.04006462  0.06016651
 -0.08314049 -0.01060047 -0.08600958  0.02665967 -0.08247703  0.05620239
 -0.03925796  0.03000642  0.01430114  0.04256048 -0.0652253   0.06911414
  0.02492299 -0.03459141 -0.03997273 -0.03400001  0.02126026  0.00589999
  0.06193445  0.05668314 -0.00628855 -0.08028247  0.06334163 -0.01439718
  0.03254187 -0.07098225 -0.0746721   0.00359894 -0.03170367  0.08194368
  0.06670741 -0.01165387  0.07951412 -0.03584682 -0.02110681 -0.0738264
 -0.03309347  0.06111896  0.06848006  0.07930408  0.06863838 -0.04200065
 -0.02030237 -0.04474601  0.03471433  0.06280323  0.03767622  0.01716208
  0.0312527  -0.04647097  0.02915503 -0.08065141  0.0172187   0.0633757
 -0.07028077  0.03091771  0.06606814 -0.06608992  0.07098044  0.00058307
  0.04886633 -0.03618467 -0.02461923 -0.06473488 -0.03647305  0.01707269
 -0.00951983  0.04083293  0.04737609 -0.04630509 -0.02208615  0.06872184
  0.05686527 -0.06459637  0.06666496 -0.04905735  0.05913443 -0.04573136
 -0.00594684 -0.04049547 -0.06969783  0.04127668  0.0272095   0.07941806
 -0.05359853  0.06388029  0.03702997  0.00980706 -0.0187473  -0.04188414
  0.07313434 -0.02939531 -0.04752917  0.03077641  0.04217755  0.03114958
  0.02064162 -0.0361568   0.01930762 -0.0789582   0.00402545  0.05603359
  0.0695698  -0.00432039 -0.04841428  0.02499478 -0.0195214   0.03039332
 -0.06777614  0.08309692 -0.0152522  -0.04540912  0.07312507  0.08337909
 -0.02016743  0.01360454  0.0713942  -0.03770941 -0.00094076 -0.01014593
  0.01467633 -0.04277777  0.07835597 -0.0304248  -0.0102921   0.05589702
  0.04844045 -0.01119364 -0.04428716  0.04948313 -0.02269116  0.05769342]

transformer_blocks.0.attention.output_projection.weight — shape: torch.Size([128, 128])
[[-0.07768236 -0.03942581  0.00159683 ...  0.01004082 -0.05805137
  -0.07691561]
 [ 0.03807045 -0.04131815  0.02169647 ...  0.03446102  0.02412117
   0.05566613]
 [ 0.03138627 -0.0686492  -0.06024098 ... -0.09279198 -0.06994457
   0.06760779]
 ...
 [ 0.00115485  0.02295663 -0.04716292 ...  0.05292595  0.06672196
  -0.04050032]
 [ 0.0164815  -0.09060989 -0.02660946 ...  0.0327528  -0.08005995
   0.07358789]
 [ 0.03466828  0.03173908  0.02831841 ...  0.00455035 -0.06795003
  -0.02034524]]

transformer_blocks.0.attention.output_projection.bias — shape: torch.Size([128])
[ 6.3022032e-02  1.2247860e-02  1.7404256e-02 -2.9624704e-02
  5.8090664e-02 -5.8951255e-02  4.7646973e-02 -1.7615722e-03
 -8.4390327e-02 -5.7473119e-02  2.9721795e-02  2.0657634e-02
  9.3300291e-04 -1.7722899e-02  7.6672575e-03  8.8705510e-02
  7.4744947e-02 -1.9831194e-02 -3.2772586e-02 -4.5315310e-02
 -1.2702721e-02  4.6787586e-02  4.1853067e-02  5.8995895e-02
  2.6492620e-02 -7.2241582e-02  2.9661218e-02 -1.6448457e-02
  8.0704041e-02 -2.0142177e-02 -4.0033456e-02  8.7134717e-03
 -4.9325936e-02  2.7375612e-02  1.9592267e-02  3.4598202e-02
  3.0584462e-02 -6.5055829e-03 -1.5640950e-02  3.9617326e-02
  3.4125436e-02  4.8959017e-02 -8.8787012e-02 -5.8159038e-02
 -6.0948137e-02 -5.7863507e-02  7.3015966e-02  6.8033873e-03
  7.4993096e-02 -3.3181168e-02  7.6239863e-03  2.4977079e-02
 -5.6145466e-03  4.0473949e-02  6.3127363e-03 -8.8647055e-03
 -2.5412207e-02  2.3872703e-02  1.4622831e-02 -7.6232344e-02
 -1.9971574e-02 -6.6102266e-02  7.1134441e-02  8.3592124e-02
 -2.5602780e-02  3.8859867e-02 -5.6501679e-02  1.7192354e-02
 -7.3167272e-02  7.2435848e-02  7.9831675e-02  6.2768483e-03
  2.3113536e-03 -3.4071285e-02  3.2794271e-02  1.0588552e-03
  4.6726983e-02 -8.8201150e-02  3.5001997e-02  1.8685009e-02
 -2.8102273e-02 -3.3522405e-02 -3.8122743e-02  1.6893491e-02
  3.3494714e-03  3.8050637e-02 -6.6296831e-02  2.5318243e-02
  5.0757088e-02  2.5863186e-02 -1.9614296e-02  3.4621269e-02
 -7.6178625e-02  6.7562997e-02 -8.3370790e-02  2.6562404e-02
 -6.7790143e-02 -6.2820488e-03 -2.0936804e-02  6.7917455e-05
  5.2625798e-02  5.6624651e-02 -4.6652626e-02  5.5085197e-03
 -7.8509390e-02 -6.7640908e-02  5.7999656e-02 -4.8792433e-02
  4.6251034e-03  6.9281369e-02 -5.2242387e-02  3.8258135e-02
 -8.2262337e-02 -6.8833850e-02 -1.9736841e-02 -4.4493433e-02
 -3.6834177e-02 -2.3793388e-02 -8.2093298e-02  5.2187480e-02
  3.6142815e-02  3.5084806e-02 -7.6625958e-02 -5.4015215e-02
 -2.2754494e-02 -7.1594521e-02  7.7784457e-03 -5.4195207e-02]

transformer_blocks.0.attention_layer_norm.weight — shape: torch.Size([128])
[1.028512  1.0270723 1.0458995 1.0395695 1.0225334 1.0321292 1.0094153
 1.0212528 1.0184616 1.0310029 1.0226146 1.0258927 1.0324234 1.0323375
 1.033752  1.0395367 1.0247457 1.0218472 1.0361341 1.0190461 1.0224193
 1.0211805 1.0328407 1.028199  1.0336144 1.0245845 1.0302695 1.0238333
 1.0314944 1.030245  1.0225277 1.028135  1.0205534 1.0320226 1.031567
 1.0324184 1.0052252 1.0398804 1.0370619 1.026346  1.0155249 1.0342196
 1.0163453 1.0228719 1.0223899 1.0310435 1.0251714 1.0186269 1.0249238
 1.031705  1.0102046 1.0042648 1.0210317 1.0298045 1.0321755 1.0285344
 1.0350935 1.0357815 1.0323004 1.0139049 1.035865  1.041059  1.034466
 1.0394146 1.0070095 1.0289854 1.0337024 1.0235744 1.0257766 1.0169833
 1.0275875 1.0290518 1.0271914 1.0260319 1.007041  1.0252588 1.0365855
 1.0008985 1.0381018 1.0233277 1.0304806 1.0219074 1.0321887 1.0034552
 1.0237213 1.0362718 1.0348834 1.0276009 1.0167823 1.0298331 1.0291709
 1.0355104 1.0257441 1.0263839 1.021743  1.0325657 1.0290006 1.0335587
 1.0331886 1.026869  1.0306746 1.0256121 1.0308169 1.0254495 1.0298392
 1.0323554 1.0335772 1.0293292 1.0295377 1.0298373 1.0163821 1.0418744
 1.0286641 1.0283104 1.0091822 1.0270998 1.0152228 1.0297363 1.0315351
 1.0221194 1.0200925 1.0293937 1.034163  1.0396631 1.0180655 1.0134832
 1.027457  1.0327969]

transformer_blocks.0.attention_layer_norm.bias — shape: torch.Size([128])
[-6.44798530e-03  6.82096463e-03  4.88749892e-03  5.47655299e-03
  2.47756811e-03 -4.72364714e-03  4.43730364e-03  9.14803334e-03
  9.88353789e-03  4.13602171e-03  1.80859829e-03 -2.64294166e-03
 -6.59028394e-03 -1.17977019e-02 -5.11393836e-03 -3.62909655e-03
 -1.17676537e-02  3.18159489e-03  5.45712747e-03  2.84260255e-03
 -1.17460443e-02 -2.15767464e-03 -6.05393946e-03 -2.80631916e-03
  5.50390279e-04  3.51652759e-03 -2.65668117e-04 -1.36867715e-02
 -4.09062859e-03  7.99088087e-03 -6.69581816e-03 -1.51261716e-04
  6.73668878e-03  8.76969472e-03  1.36002316e-03  6.36972580e-03
  9.57730971e-03 -7.07676122e-03 -4.34349338e-03  5.99389244e-03
 -6.73915725e-03  3.33308638e-03 -6.91664964e-03  1.19812414e-02
  4.73481341e-05 -9.78959026e-04 -4.82658902e-03  1.49731068e-05
 -8.23769160e-03  2.66361330e-03  8.88226181e-03  5.42158540e-03
 -4.32150066e-03 -4.92032897e-03 -1.72602665e-03 -9.40747652e-03
 -5.14307013e-03 -7.74898333e-03  4.05752426e-03 -2.32729735e-03
  9.76872537e-03 -4.26512212e-03 -1.02525428e-02  8.27187952e-03
 -5.69884432e-03 -4.66550607e-03  4.74836631e-03  3.33058619e-04
  4.02906444e-03 -1.13373273e-03  8.62271059e-03  9.69027076e-03
 -3.44593567e-03  1.56058259e-02 -4.54527233e-03 -2.75688898e-03
  3.64770973e-03 -1.46959303e-02 -5.03746001e-03 -1.12198470e-02
 -8.28444865e-03  5.43709612e-03 -4.15056886e-04  6.10008510e-03
 -4.71851381e-04 -3.05004301e-03  1.17207831e-02 -5.08210331e-04
 -3.68060125e-03  1.14123302e-03 -6.18934305e-03  4.51448723e-04
  5.42393886e-03  6.47155289e-03  8.90911091e-03  3.20877298e-03
 -1.15110939e-02  4.30365652e-03  1.19090024e-02  5.19372476e-03
 -1.08338497e-03  1.38626909e-02 -8.39748885e-03  2.69852055e-04
 -1.50904001e-04 -7.05875643e-03 -9.29286296e-04  1.59101025e-03
 -9.67492117e-04  2.58276006e-03  1.98617438e-03  4.07566410e-03
 -9.73457471e-03  7.94379041e-03 -1.53730605e-02  1.02339860e-03
  7.29607465e-03 -2.16130307e-03 -2.13356339e-03 -8.08445178e-03
  1.10290824e-02  5.89624466e-03 -1.68927927e-02 -1.19990192e-03
 -1.75796896e-02 -2.78861751e-03  8.34895950e-03 -4.65702871e-03]

transformer_blocks.0.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0218034  1.0234193  1.0324426  1.0408272  1.0260538  1.0267678
 1.0318929  1.0189477  1.0216353  1.0259037  1.0272334  1.0288537
 0.9979738  1.0296308  1.0070869  1.0065253  1.0327464  0.9875496
 1.033701   1.0205891  1.0332344  1.0088835  1.0276587  1.0288972
 1.0198733  1.0280168  1.021944   1.0333543  1.0342004  1.0278186
 1.0223937  1.0245234  0.9918534  1.0302168  1.0300353  1.0295174
 1.0293195  1.0267278  1.0165237  1.027858   1.0325063  1.0167524
 1.028446   1.0308876  1.0322129  0.99636304 1.0250233  1.0205351
 1.0006822  1.0312588  1.0000843  1.0153323  1.0307539  1.0006887
 1.0011601  1.0301723  1.0288529  1.0185937  0.98153716 1.0248127
 1.0267438  1.0322632  1.012626   1.0302052  1.0225643  1.0383588
 1.0197889  1.0340215  1.0268322  0.97358096 0.99301606 1.0355513
 1.0189655  1.023793   0.9911072  1.0117872  1.0158306  1.0238335
 1.0232617  1.0265467  1.0287358  1.0264558  1.03169    1.0303427
 1.0087937  1.0055591  1.015683   1.0231957  1.0297585  1.0140632
 1.0333623  1.033377   1.0242239  1.0277743  1.0279692  1.0306704
 1.0209283  1.0267594  1.0085906  1.0267272  1.0329735  1.0138063
 1.0315549  1.026639   1.0032198  1.0249592  1.0006282  1.03115
 1.0258594  1.0355701  1.0273404  1.0296288  1.0097471  1.0245882
 1.0297782  1.0277219  0.97877073 0.9844295  1.0114855  1.0249056
 0.988789   0.99958646 1.0382838  1.0323548  1.0217285  1.0065485
 1.029246   1.0285577 ]

transformer_blocks.0.feed_forward_layer_norm.bias — shape: torch.Size([128])
[ 1.06359106e-02 -1.43993022e-02 -3.72396759e-03 -9.22416570e-04
 -1.62219834e-02  9.74471681e-03 -2.73585971e-03  2.12742183e-02
 -1.39057077e-02  4.42045880e-03 -3.83594586e-03 -1.29691260e-02
  9.54230689e-03 -6.57726079e-04 -9.36553255e-03  1.47254160e-02
  2.00934410e-02 -8.82938597e-03  1.53723788e-02  2.87098475e-02
  1.25208888e-02 -2.13621752e-04  1.93604920e-02 -5.67167113e-03
  6.77975174e-03 -3.92201357e-03 -5.13174012e-03 -8.98203434e-05
 -2.02692468e-02  1.12000182e-02  1.76296071e-05 -2.03802846e-02
 -4.53950954e-04  3.59025528e-03 -9.22865700e-03 -1.20894089e-02
 -1.25961592e-02  4.90088901e-03  1.88953499e-03  1.23408064e-02
  8.83575529e-03  2.82895099e-03  4.01272019e-03 -3.60910693e-04
  3.42323328e-03 -5.56515297e-03  2.66649714e-03  1.78794824e-02
  8.87855422e-03  1.76715199e-02 -1.22858780e-02  1.27097908e-02
 -1.17129600e-02  1.83397587e-02 -1.46937771e-02  7.20510166e-03
 -6.36964291e-03  9.36934073e-03 -1.16336923e-02 -1.19122886e-03
 -6.72566611e-03  1.25010358e-02 -8.35323799e-03  2.24443190e-02
 -1.67872664e-02 -1.20777215e-04 -3.47804814e-03  4.26663598e-03
 -6.78132521e-03  1.48437349e-02 -1.30850989e-02  6.19884767e-03
  4.24043508e-03 -3.77895799e-03 -1.57710779e-02  4.16990416e-03
 -2.11368012e-03  6.74420735e-03  1.81427808e-03  7.20962090e-03
 -2.19692066e-02 -1.48543837e-02 -1.50088742e-02  1.02506373e-02
 -3.57428775e-03 -2.45578755e-02  1.86833006e-03 -8.58346000e-03
 -4.03071102e-03  9.35467135e-04  1.67223234e-02 -9.30767623e-04
  1.68549661e-02 -7.42650311e-03  2.11544451e-04  9.52994544e-03
  7.38965860e-03 -1.89048350e-02 -7.23545207e-04 -1.36894058e-03
  4.71134437e-03  2.30219923e-02 -5.81829809e-03 -4.76793060e-03
  1.27228638e-02  4.43239929e-03  8.84541031e-03 -6.18250668e-03
  4.70822165e-03  2.16314755e-02 -1.33728022e-02  1.19276205e-02
 -4.51710494e-03  7.86419306e-03 -1.36189982e-02  1.00569651e-02
 -1.44287562e-02  6.43641548e-03 -2.64525705e-04  1.16937784e-02
  1.69327855e-02  1.95670058e-03  8.55527539e-03 -1.47673646e-02
 -6.14758255e-03  1.44287981e-02  7.36990757e-03 -1.40434653e-02]

transformer_blocks.0.feed_forward.0.weight — shape: torch.Size([128, 128])
[[ 0.06549581 -0.07710126  0.01010163 ...  0.03174836 -0.01723512
   0.1205173 ]
 [ 0.06047324 -0.00711519  0.07880933 ...  0.02380734  0.00637271
  -0.08660142]
 [ 0.10312752  0.10220136  0.04626482 ... -0.01394294  0.03471625
   0.04145765]
 ...
 [ 0.01333971 -0.06398349  0.08004022 ... -0.07238147  0.00708784
   0.00718776]
 [ 0.01696339 -0.05052282  0.05058958 ... -0.01734904  0.03145164
  -0.02283074]
 [ 0.08952919 -0.06956425 -0.02434589 ... -0.10688208 -0.03650966
  -0.02576689]]

transformer_blocks.0.feed_forward.0.bias — shape: torch.Size([128])
[-0.00621714 -0.04641778 -0.0788238   0.0682786  -0.03885768  0.04180339
  0.0409102   0.07634893  0.01197183  0.04344837  0.05432294 -0.01831266
 -0.07800635 -0.01038493  0.07979763 -0.0520132   0.04683438 -0.05883082
 -0.04894383  0.08227356  0.05562507  0.06642682 -0.01594192  0.02398684
 -0.04149966 -0.01463593  0.09395888 -0.02001239  0.00848617 -0.01910995
  0.03977622 -0.05758272  0.04655883 -0.07589449  0.07643979 -0.02283345
  0.05588726 -0.02918396 -0.05641305  0.00981135  0.02450794  0.01715967
  0.0319657   0.06423881  0.0862651  -0.05104804 -0.04252281  0.0105239
 -0.04507677  0.06198992 -0.06817166 -0.04778595  0.06518629 -0.04497963
 -0.07609186 -0.05324083 -0.03675232  0.00738221  0.05736421  0.05802754
  0.01307246 -0.07223841 -0.02032636 -0.01456803  0.0842337  -0.0227685
  0.02933708 -0.0689982  -0.02155692 -0.02254945  0.02222401 -0.05723761
 -0.03975073  0.03238268  0.05093021  0.03611287 -0.0146503   0.07536557
  0.06271695 -0.02773025  0.05344749 -0.04002342  0.09112246  0.03956839
  0.00813245  0.0159955  -0.04244826 -0.04155296 -0.04477642 -0.01641507
 -0.0276462  -0.01791626  0.04062747  0.0010337   0.03496538  0.07511412
  0.01571004 -0.04027577 -0.01389196  0.02342256  0.09436681 -0.03740611
  0.04720791 -0.00780816  0.0020642  -0.01215238 -0.08693111  0.07991061
 -0.04542195  0.06116029 -0.01634164 -0.06390153 -0.05101389  0.02458591
 -0.06221893  0.02009185 -0.00781109 -0.01922586  0.07408309 -0.08435685
 -0.08654078 -0.04256665 -0.0278673   0.02018233 -0.06179818  0.03711396
 -0.06030954 -0.02465403]

transformer_blocks.0.feed_forward.2.weight — shape: torch.Size([128, 128])
[[ 0.07201439  0.04956424  0.04201201 ...  0.0662709   0.04990391
   0.00531583]
 [ 0.00603733 -0.02430941 -0.03677055 ...  0.04901652  0.04714756
  -0.03276587]
 [ 0.01575819 -0.05710308  0.03462535 ...  0.03354206 -0.04332288
  -0.02220274]
 ...
 [-0.07927364 -0.05567288 -0.05185452 ...  0.05580564 -0.06692272
   0.00870047]
 [-0.05053658  0.07282285  0.05477812 ... -0.04857078  0.05003446
  -0.03709259]
 [ 0.06538579 -0.04526921  0.05439178 ...  0.04171162  0.05161214
  -0.04315239]]

transformer_blocks.0.feed_forward.2.bias — shape: torch.Size([128])
[-0.07254128  0.02760371 -0.03809321 -0.04561466 -0.0236175  -0.0641469
 -0.02445695 -0.00272052 -0.03604086 -0.07360052  0.01355888 -0.03318516
  0.06140551  0.05822738 -0.04190294  0.00501664  0.04598326  0.02217319
  0.05461954  0.08776738 -0.06571238 -0.03141966 -0.02369571 -0.04452815
 -0.0015524  -0.01059603  0.02113182 -0.00592991 -0.03871172 -0.07523525
 -0.02198284  0.01292828  0.0390515   0.06141694 -0.05805057 -0.05044765
 -0.06262361  0.02828118  0.05132272  0.03846612  0.04416629 -0.08195678
  0.03647444  0.0516027   0.0495713  -0.08391104 -0.04729838  0.0568679
 -0.07718869  0.01901305 -0.06508076 -0.00834122 -0.0375635  -0.01385077
 -0.01813457 -0.00857942 -0.06149494  0.05778364 -0.0428595  -0.0458557
 -0.08445404 -0.04142243  0.03836083  0.06469031 -0.01862307  0.01987795
 -0.05350642 -0.00531995  0.06902149 -0.06656611 -0.06451901  0.02111576
 -0.05438404 -0.08795846  0.00301358  0.04278463  0.06021129  0.01346163
  0.04271906  0.06351499  0.01615667  0.06330886  0.00619027 -0.03490432
  0.09284903  0.01271391 -0.01660972 -0.07760726 -0.04662395  0.06854055
  0.01066099 -0.03548998 -0.00948256 -0.08590563  0.02985931 -0.06229891
 -0.07629225 -0.03830858 -0.0171927  -0.06403518  0.05289059 -0.01089145
  0.0244464   0.00306711 -0.05124265  0.05067414  0.0711419  -0.06624758
 -0.03386747 -0.05268983 -0.07553068  0.02723419  0.02469794 -0.02623986
 -0.02526459 -0.04571773 -0.07268505 -0.07093108  0.06673892  0.054305
 -0.04263939 -0.03771169 -0.04112596 -0.03974139  0.04134964  0.00936101
 -0.06038523 -0.07973453]

transformer_blocks.1.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.06257346 -0.01114212  0.08614565 ... -0.10151073 -0.06314855
   0.02371329]
 [-0.06716364  0.0421508  -0.01986573 ... -0.05686199 -0.07607123
   0.02521631]
 [-0.04370843 -0.00054542 -0.11835072 ...  0.03197642  0.04097603
   0.07715973]
 ...
 [ 0.01740568  0.05710996  0.01008751 ...  0.01038078  0.0347972
  -0.07084389]
 [-0.00862685  0.01129279 -0.05463037 ...  0.02803373 -0.02806919
   0.05837959]
 [-0.00432503 -0.03569688  0.09096583 ...  0.02556431 -0.03108905
   0.06526296]]

transformer_blocks.1.attention.query_key_value.bias — shape: torch.Size([384])
[-0.02484581 -0.02126732  0.06299245 -0.00999841  0.03837925 -0.07198224
 -0.07484027 -0.03715048  0.01185893 -0.06120783  0.0694773  -0.09659299
  0.0775491  -0.06299304 -0.07277929  0.05070783 -0.03389233 -0.06514475
  0.08446959  0.07250071  0.03871295 -0.04800379  0.0397191   0.02736558
 -0.07873525  0.08513581 -0.04107957 -0.02577745  0.0416479   0.07732246
 -0.05489179 -0.03709321  0.07713923  0.07591712 -0.0406682  -0.06302591
 -0.08388106 -0.00766293  0.01661354  0.03223954 -0.07393286  0.06976336
  0.00974621  0.02125339  0.04951667  0.05523533  0.06270763  0.02719162
 -0.0483296   0.07084575  0.05216386 -0.0543873   0.06931869 -0.0319667
  0.05781761  0.05062969 -0.08112005  0.03645    -0.07006926 -0.05923609
  0.07488786 -0.02969681 -0.09047431  0.05212021  0.06193208  0.06209154
  0.03062986 -0.04058189 -0.05915556  0.03813036 -0.03332097 -0.00213901
  0.06937016 -0.08815105 -0.06289948  0.04070046  0.03310098  0.05656083
 -0.06753905  0.0109897   0.03073743 -0.08846246  0.08165107 -0.03818892
  0.02698328  0.00082103  0.05108642  0.05509942 -0.04521744  0.0855459
 -0.01990535  0.04451412 -0.06382202  0.05963922 -0.07418685 -0.01696818
 -0.05140315 -0.07224551 -0.04394359  0.04029943 -0.0237549  -0.02558265
  0.04876522 -0.00135483  0.03816704 -0.00235519 -0.08234876  0.03322363
 -0.03120116  0.03108612  0.06415303  0.02410967  0.04859956 -0.0181316
 -0.08627515  0.06127182  0.07857278 -0.05296347  0.05654456 -0.02578918
  0.05827775  0.00951652  0.07642613  0.07754265  0.0482652  -0.02245877
  0.01389008 -0.00462842  0.07153655 -0.0610827  -0.05084816  0.06266481
 -0.00607085 -0.02972084 -0.0717533   0.01793063 -0.05374722 -0.0750264
  0.00788102  0.01571052  0.04491938 -0.0222679   0.04099586  0.02290556
 -0.0232506  -0.03345978 -0.01468044 -0.08257713  0.03060514  0.08758435
  0.01903281  0.01836647 -0.04714539 -0.04150473  0.00596191  0.01258998
 -0.06535283  0.04124615 -0.05352502  0.08585502 -0.07087456 -0.05326346
  0.017149   -0.08796494  0.03857039  0.05405128  0.06938206 -0.04058483
  0.03800798  0.01284977  0.00578373 -0.0010712  -0.00349542 -0.0002901
 -0.05259214 -0.04342791 -0.08674829 -0.08133212 -0.07295946  0.05466967
 -0.08058575  0.06585098  0.02333756 -0.04941389  0.00616104  0.04136577
 -0.02007992  0.03807041  0.00607329  0.04339917  0.06438141 -0.08298576
  0.04558744 -0.03497237 -0.0155604  -0.0042508   0.01201845  0.07318985
 -0.00041705  0.02514701  0.06922137  0.08060762  0.0569561   0.02655431
 -0.03415923  0.01568725 -0.01768401  0.08281759  0.0490143   0.04730086
 -0.01714518 -0.04159719  0.08824662  0.0423238   0.04171206  0.03321606
 -0.0613713   0.00756992  0.04186231  0.00423313 -0.07395795 -0.06488667
  0.04804933  0.05121325 -0.0137511   0.08329275  0.04306836  0.00904183
 -0.08263393  0.02285007 -0.03976149  0.06674675  0.00587538 -0.02995364
 -0.08697414  0.07773702  0.07641289  0.0231891   0.03396653 -0.07848574
 -0.00712044  0.01621415 -0.07914985  0.04567103  0.06045162  0.00623326
  0.0423885   0.02526398 -0.03200635  0.05700155 -0.03600549  0.05347062
 -0.00782592  0.05469035  0.03956338 -0.0874461   0.00271543  0.03010758
  0.04836367  0.00381484  0.04960877 -0.02311029 -0.03244327 -0.05583555
 -0.02621329  0.04532922  0.0442134   0.00131747 -0.05517259  0.01425431
 -0.0314468  -0.00563887  0.04235848  0.04313549  0.0321049  -0.0789391
 -0.03820843  0.06136644  0.08842127 -0.05269398  0.05868957 -0.04873708
  0.06632514 -0.04303945 -0.00315237  0.0337133   0.05530392 -0.00916735
 -0.04942944 -0.06412937 -0.05041792  0.00253951 -0.04391306 -0.0235581
 -0.06478539  0.06941274  0.04678954 -0.04714391 -0.04640117  0.00861728
  0.05038299 -0.0649482   0.05356283  0.07077479 -0.03466296  0.02805494
 -0.00036796 -0.03553011  0.04816282 -0.03951046 -0.01728591 -0.02846048
  0.02126228 -0.02226828  0.00509302 -0.05953945  0.01492779  0.0492596
  0.0200279  -0.00260462 -0.0622877   0.01464842 -0.00531922 -0.04660928
 -0.03003485  0.05332862 -0.00071932  0.01991319  0.03163491  0.06247791
 -0.0816848   0.01922626  0.05972104 -0.02259614 -0.04602602 -0.06103427
 -0.04768213 -0.04441343 -0.06840926 -0.07837114 -0.0663799   0.04240783
 -0.00479403 -0.0488716  -0.01872025 -0.07881544 -0.04476285  0.02591002
 -0.06455965 -0.04645102 -0.07808295 -0.01311009 -0.0779339  -0.01499101
  0.00175967 -0.02749838 -0.02943229 -0.05165798  0.0307646  -0.05640499
  0.06477547  0.03642749 -0.00483447  0.01942746  0.08019164  0.05066722
  0.06345811 -0.06373144 -0.04146926  0.05105146  0.05459367 -0.06394063
  0.00811399 -0.03915858 -0.08267511  0.01396804  0.05781942  0.01546951
 -0.08148377 -0.06887866 -0.07951294  0.06300864  0.00869517  0.04554876]

transformer_blocks.1.attention.output_projection.weight — shape: torch.Size([128, 128])
[[ 0.07049382  0.03429873  0.01565773 ... -0.07041402 -0.04543841
   0.07063312]
 [ 0.07428534  0.03536878  0.01403278 ...  0.08815043 -0.09527462
  -0.0453012 ]
 [-0.01516213 -0.0005533  -0.05724872 ...  0.02717417  0.09877649
  -0.08479246]
 ...
 [ 0.0560324   0.03086717  0.00461248 ... -0.07942698 -0.06714947
   0.00118912]
 [-0.05209355  0.03433963  0.06555547 ... -0.01573795  0.05416541
  -0.00186933]
 [-0.0702455  -0.07934486  0.01745619 ... -0.01125809 -0.0252521
   0.08893747]]

transformer_blocks.1.attention.output_projection.bias — shape: torch.Size([128])
[-0.02558497 -0.02555572 -0.02459668  0.08021821  0.0866992   0.05139819
 -0.06988766  0.08072782  0.01911353  0.07915252 -0.06353328  0.04949936
  0.0611362  -0.05733358  0.02433693 -0.07916325  0.05067716 -0.01859053
  0.03153719  0.00921445 -0.06870346  0.00905107  0.07298001 -0.02151956
 -0.08152166  0.03072466 -0.08531443 -0.04840631 -0.08171718 -0.05888856
  0.05497345  0.07192728  0.04882962  0.00718721 -0.07228164  0.06450305
 -0.00028678 -0.08047838  0.05250124 -0.0632471   0.06129468  0.04404378
 -0.05345923 -0.07818817  0.00616641  0.06938542  0.01130452  0.03978017
 -0.04586986 -0.04415054 -0.07402087  0.08607243  0.04689281 -0.09090496
 -0.05214789  0.03304842  0.02783372  0.08004765 -0.07883874  0.07984003
  0.04065606 -0.06541114  0.0211155   0.08860678 -0.01926454 -0.05970574
 -0.06746949  0.02867084 -0.02949794  0.08025083  0.02079247 -0.02401387
 -0.05634485 -0.01999721 -0.06693692 -0.08459377 -0.04961356 -0.04824635
 -0.07876503 -0.05684482  0.05553329 -0.06552083 -0.01310676 -0.04544768
 -0.04237921  0.03177704 -0.07541876 -0.00588286  0.05772867 -0.05032637
 -0.07786628 -0.01780876 -0.08883381  0.01158941 -0.00914509 -0.0664343
 -0.05579176  0.01941992  0.06879633 -0.08147489  0.00443961  0.03930269
 -0.02223042 -0.08033016  0.00579414  0.07832429 -0.02515154 -0.05165554
 -0.03062722 -0.03691846 -0.02679702 -0.00406416 -0.07266869 -0.02420296
 -0.04272723 -0.05438292  0.05477464  0.09002928  0.07851867  0.03487063
 -0.0644761  -0.0769688   0.04568505  0.03994002 -0.03343745 -0.09129372
  0.01873205  0.05762023]

transformer_blocks.1.attention_layer_norm.weight — shape: torch.Size([128])
[1.029261   1.0309013  1.0334215  1.0319769  1.0180746  1.0337937
 1.0128235  1.0298032  1.0123936  1.029682   1.0330861  1.0391538
 1.0246177  1.014224   1.0255061  1.036235   1.0096672  1.0117644
 1.0254437  1.0163234  1.0201865  1.0349212  1.0181947  1.0239096
 1.042522   1.0340191  1.0293298  1.03079    1.0313423  1.0167544
 1.0326133  1.0302215  1.0348122  1.0186572  1.0237455  1.0227386
 1.0228271  1.0290927  1.0305024  1.023633   1.0234926  1.0361176
 1.0351591  1.0361252  1.0231497  1.0317924  1.035136   1.0430169
 1.0272825  1.0370704  0.99610347 1.0308255  1.0344229  1.0338945
 1.018568   1.0362712  1.0290891  1.027142   1.0184848  1.0364616
 1.0297289  1.0318637  1.031934   1.0243583  1.0151647  1.0261401
 1.0351627  1.0310123  1.0314192  0.9964769  1.020284   1.0279146
 1.0349891  1.0075885  1.0067077  1.0291022  1.0364492  1.0199847
 1.0182811  1.0219165  1.0315642  1.0295169  1.0328958  0.99368125
 1.020636   1.0255166  1.0239247  1.0304393  1.0145243  1.0297332
 1.0284898  1.0346437  1.0267807  1.003136   1.0268474  1.0330741
 1.0244056  1.0181437  1.0403965  1.0354211  1.0282111  1.0215989
 1.0309749  1.0197338  1.0281198  1.0149851  1.0176625  1.0287656
 1.0381899  1.0291536  1.0067894  1.0236104  1.011117   1.01879
 1.0255239  1.0289321  1.0336659  1.031059   1.035732   1.0230603
 1.0170801  1.0221646  1.0151948  1.0259018  1.0330703  1.0162508
 1.0238914  1.0322359 ]

transformer_blocks.1.attention_layer_norm.bias — shape: torch.Size([128])
[-0.00138589 -0.01059473  0.00017413 -0.00720831  0.00113281  0.00542375
  0.00506642  0.00558971  0.0054482   0.00015026  0.00075821 -0.0019888
  0.00518893 -0.01340631  0.00193815 -0.00769274 -0.00157873  0.0068761
  0.00769993 -0.00748329 -0.01226281 -0.00876436  0.00217792  0.00699127
  0.00664088  0.00696591 -0.00359059 -0.01012629  0.00040339  0.00653768
 -0.00793662 -0.00366441  0.00386898 -0.00911681  0.00092214  0.0064738
  0.0101518  -0.01266618  0.00063261  0.01389552 -0.00404568 -0.00085643
 -0.00222999  0.00343444  0.00138806  0.00089738 -0.00828787  0.00456284
  0.00279881 -0.00440685  0.01192008  0.00692645 -0.00320099 -0.00508718
 -0.00314372  0.00461172  0.00779859 -0.01156984 -0.0068464  -0.01321854
 -0.00223421  0.00663797 -0.00135616 -0.00091399 -0.00905802 -0.02036408
 -0.00555702 -0.00049801 -0.00554853  0.01191378 -0.00566244  0.00110503
  0.00370454  0.00417771 -0.00344722 -0.0025847  -0.00781333 -0.00275957
 -0.00923131  0.00795505 -0.00254923 -0.00157222  0.00759915  0.01476563
  0.0063868  -0.01036137  0.00690753  0.00797371 -0.00301764 -0.00124963
 -0.00628261  0.00815467  0.00767478  0.00661487  0.0024272  -0.00706136
  0.00367304  0.00010831  0.01039726  0.00486368 -0.00232426 -0.01211379
 -0.00116866  0.01031357 -0.00568613 -0.01405522  0.01013185 -0.00216372
  0.00795689 -0.00523511  0.00985335  0.00312434 -0.00716658  0.01052769
 -0.00663864  0.0022757   0.00657086  0.01097488 -0.00045761 -0.01402003
  0.01554625  0.00453499 -0.00156011 -0.00746792 -0.00682807 -0.00083977
  0.00670739  0.00494274]

transformer_blocks.1.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0342332  1.0282397  1.0294113  1.013037   1.0273492  1.0249808
 1.0117872  1.0301039  1.0340605  1.0304041  1.027216   1.0370119
 1.0280215  1.0049939  1.0328401  1.0297725  1.0135787  1.0176471
 1.0143636  1.0313665  1.022947   1.0335239  1.0293471  0.9940441
 1.0299851  1.031262   1.019731   1.0242273  0.9941375  1.0325651
 1.0308512  1.0194466  1.037954   1.0233958  1.0278962  1.0298613
 1.0292952  1.0335099  1.0246105  1.0260165  1.0345432  1.0227319
 1.0345547  1.0272138  1.0355778  1.0243709  1.0074558  1.0306233
 1.0327307  1.0301839  1.0227649  1.044618   1.0316164  1.0170583
 1.0302339  1.0321343  0.98578167 1.0342239  1.0040114  1.0036092
 1.0283254  1.0360196  1.0335017  1.0312171  1.0335616  1.033483
 1.0085253  1.0361718  1.0282316  1.0070922  1.042396   1.0306939
 1.0206668  1.0200725  1.0296057  1.0240984  1.0340257  1.0098152
 1.034314   0.99746734 1.0245209  1.0335046  1.0279408  0.9983408
 1.0237435  1.0306802  1.023038   1.0196351  1.0244789  1.0119079
 1.0216595  1.0293927  1.0211678  1.0146827  1.0089675  1.0095092
 1.0355331  1.0210626  1.038328   1.019746   1.0410836  1.0311836
 1.0375059  1.0356348  1.0289086  1.0285482  1.0291861  1.0308304
 1.0217915  1.0186766  1.0305037  1.0236665  0.99584943 1.0235612
 1.0070069  1.0335141  0.98725265 1.022816   0.9892366  1.0240571
 1.034419   1.031069   1.024507   1.0311197  1.0273713  1.0333343
 1.0291886  1.0208243 ]

transformer_blocks.1.feed_forward_layer_norm.bias — shape: torch.Size([128])
[-0.00835985 -0.00862146  0.00963701 -0.00989326 -0.0119576  -0.00654894
  0.01117189 -0.01046542 -0.00841742  0.01758414  0.00718113  0.0010111
 -0.0086341  -0.00779311  0.01126918 -0.01169656 -0.00734242 -0.01150217
 -0.01151246  0.00090125  0.00669589  0.00143753 -0.00217778 -0.00691754
 -0.01341667  0.01612977  0.00705953  0.00013838 -0.00303678 -0.0037072
  0.01300137  0.00132402  0.0057309  -0.00470464 -0.00623469  0.02095927
 -0.0157875  -0.00671286  0.00039182 -0.00403142 -0.01397891 -0.01094587
  0.00478123  0.01495988 -0.0011442  -0.01433149  0.01066456  0.01235626
  0.00987981  0.01310216 -0.00308371  0.02512464 -0.00814097 -0.00733301
 -0.01323477  0.0054988   0.00624343  0.00174798 -0.00880282 -0.00461732
 -0.02145145  0.00854881  0.0083752   0.01952992 -0.02211416 -0.02036502
 -0.0117647   0.00952666 -0.00344057  0.00800001  0.00192894  0.00957972
 -0.01766408 -0.00527994 -0.00514439 -0.00597596  0.01184227 -0.00058315
 -0.0036802  -0.00255107  0.00731949 -0.01020769  0.00161438 -0.00188802
  0.01062392 -0.00986423  0.00804542  0.00481541 -0.00067375  0.00632839
  0.01564241  0.00096025 -0.00960124 -0.00900535 -0.00801209 -0.01890902
 -0.00894676  0.01068916 -0.00727422  0.01081555  0.01155027  0.00479105
  0.020322    0.00961974 -0.00204101  0.00477737 -0.00409182  0.00939884
  0.0130686  -0.00863843  0.01186857 -0.00903016  0.02163961  0.00974875
  0.01019586  0.00542987 -0.00512792 -0.00019085  0.00966141 -0.01284048
 -0.0228733   0.00752003 -0.00533226  0.00458197  0.00357296 -0.00069509
 -0.01887053  0.0110951 ]

transformer_blocks.1.feed_forward.0.weight — shape: torch.Size([128, 128])
[[-0.04480094 -0.03591385  0.03841445 ... -0.05485558  0.10722112
  -0.03391941]
 [-0.05977598 -0.03571381 -0.00583164 ...  0.00850796  0.00586159
   0.02317765]
 [ 0.09728576  0.01370721 -0.00082608 ... -0.01766551 -0.06971423
  -0.00616689]
 ...
 [ 0.02546153  0.03003455 -0.02191608 ...  0.02883371 -0.07342792
  -0.00672503]
 [-0.0176648  -0.0010369  -0.09763674 ... -0.0120268  -0.04335427
   0.01382563]
 [-0.02228037  0.03805146  0.05944552 ... -0.02602082  0.10233389
  -0.1087155 ]]

transformer_blocks.1.feed_forward.0.bias — shape: torch.Size([128])
[-0.06246706  0.06843657  0.00390619  0.08665161 -0.07536943  0.06966354
 -0.05159216 -0.04945166  0.06493118 -0.02240308 -0.0469723   0.03263445
 -0.06710718  0.00324702 -0.06299102 -0.0542286   0.00190362  0.0141779
 -0.07402242 -0.05705305 -0.01624182  0.00939924 -0.06550147 -0.03782775
 -0.0325456  -0.02285125  0.07406813  0.05414438  0.01487435  0.09225683
 -0.02363056 -0.0396779  -0.05422574 -0.00030825  0.04486277 -0.00367548
  0.0817935   0.08541609 -0.05596156 -0.04903768  0.01717871 -0.02528258
 -0.06773661 -0.06212277 -0.04393879  0.03175978  0.01585456 -0.03351551
 -0.02062274 -0.06326345  0.01808393 -0.04048069  0.06117148 -0.02471484
  0.07141201 -0.04931527 -0.03528878  0.08935497  0.02381278  0.08070073
 -0.08047216  0.08175194  0.05631215  0.09928612 -0.04139842  0.03439644
  0.07360259  0.075527    0.02082467  0.0010282   0.03723764 -0.03936892
  0.08996835 -0.03033506  0.05679082  0.05016276  0.0396139  -0.00196841
 -0.03587541 -0.00860269  0.02996263 -0.08446155  0.05918732 -0.05914398
  0.06370897 -0.01353146  0.01739096  0.04753166  0.0013814  -0.03667923
  0.04181026  0.07593437 -0.02318714  0.06042762  0.0562682   0.02521361
 -0.04034406  0.03711783  0.06698602 -0.06879196  0.05614237  0.0642722
  0.04628242  0.02847562 -0.0688151  -0.01657306  0.00010331  0.08859297
  0.09263515  0.01867168  0.06194443 -0.06100636  0.05373291  0.09085426
  0.07505727 -0.00412316 -0.0554961   0.03250438  0.04126231  0.05015369
  0.06848188  0.04389124 -0.04557405 -0.03136297  0.08178388  0.10259403
  0.07634059  0.01015514]

transformer_blocks.1.feed_forward.2.weight — shape: torch.Size([128, 128])
[[-0.04947929  0.07130611  0.05256502 ... -0.01171739 -0.06089154
  -0.00188837]
 [ 0.00075671 -0.03995385 -0.02794247 ... -0.00546944 -0.01349054
   0.09069481]
 [-0.00543449 -0.04079679 -0.06778219 ... -0.09049016 -0.01686526
  -0.05884987]
 ...
 [-0.05607754 -0.07911753  0.05601365 ...  0.09653135  0.07530258
  -0.06049491]
 [-0.00253791 -0.03991291  0.01801863 ...  0.11091892 -0.01741217
  -0.06135686]
 [-0.03388847  0.00364895  0.02611416 ...  0.0488359   0.01387159
  -0.09487117]]

transformer_blocks.1.feed_forward.2.bias — shape: torch.Size([128])
[ 0.03161142  0.02600324 -0.0591716  -0.05642405  0.07365489  0.04104263
  0.08226798  0.0631329  -0.06586744 -0.02357637  0.00356857 -0.00373002
 -0.05930242  0.05229392 -0.04215269  0.04137565  0.03048112 -0.00135299
  0.03488098 -0.03043121 -0.05292799  0.04103571 -0.0354041  -0.08763731
 -0.05876454  0.0061153   0.06499732  0.0342977  -0.03116511 -0.07381079
  0.02770534 -0.0130929  -0.0497586  -0.08621649 -0.03327476 -0.07571419
  0.03211941 -0.03702188  0.06783139  0.08954046  0.05156716 -0.08519471
 -0.00302373  0.00327983  0.07299753 -0.00083556  0.01844673  0.03229524
  0.0644407   0.03525786 -0.04920422 -0.05098784  0.03875953  0.00373097
  0.05094582 -0.07255869 -0.06489824  0.0004397   0.07618979  0.07488596
 -0.01193429 -0.0023239  -0.03202023 -0.07542794 -0.0413327   0.04122835
  0.00237619 -0.03874406  0.04330558 -0.00165719 -0.07760187  0.08893335
  0.0671582  -0.01029864 -0.07923865  0.05232791 -0.02064906 -0.03950959
 -0.06480335 -0.01224164 -0.09228817 -0.08764109 -0.02909899 -0.06453159
 -0.07342009 -0.04170471 -0.07962118  0.0439072   0.04911511 -0.02444958
 -0.0116045   0.02652435  0.01729255 -0.07764226 -0.02773813  0.00084073
  0.03135976  0.0307258  -0.05460655  0.06007251  0.06966864 -0.08640875
 -0.06929626  0.08216545  0.06512591  0.02533485 -0.01777039 -0.06176413
 -0.05741458 -0.06967979  0.01437002 -0.07718553  0.00884721 -0.03284301
 -0.0225444   0.07949952 -0.02131597  0.08990241 -0.04412571 -0.08753085
  0.08896151 -0.02743699  0.03306756 -0.04493404  0.00292304 -0.03680528
  0.06061793  0.04778431]

transformer_blocks.2.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.00644518  0.1003141   0.09460285 ...  0.01889247  0.08598191
   0.08465663]
 [ 0.07071302 -0.02882017  0.0154998  ...  0.05635479  0.11468267
  -0.04278516]
 [ 0.09067951 -0.04048495  0.05106537 ...  0.10559894  0.08385029
  -0.05548999]
 ...
 [-0.07060083  0.06276388 -0.0955488  ... -0.01643733  0.10115679
  -0.05636336]
 [-0.01781988  0.00954525 -0.0138839  ... -0.07905546 -0.04766009
  -0.02349593]
 [ 0.00759155 -0.06903977  0.07139046 ...  0.10499215 -0.0449053
   0.04509521]]

transformer_blocks.2.attention.query_key_value.bias — shape: torch.Size([384])
[ 0.05615227 -0.05936986  0.01485729  0.05093537 -0.04514831  0.01384384
 -0.02193698  0.06178222  0.03385491  0.03784024  0.02786053  0.00096225
  0.02097409  0.07257468 -0.07265403 -0.08551602 -0.026063    0.00770702
 -0.0654636   0.06547683  0.01687091  0.03141335 -0.01125442  0.08666712
 -0.07320333  0.08332103 -0.07891928 -0.08243006 -0.0134779   0.03901514
  0.07463261 -0.04786515  0.06428349  0.04651659 -0.06867675 -0.03203185
 -0.00707868 -0.05267942 -0.03217412 -0.08648082  0.0370095  -0.06318232
 -0.08312429  0.04567658  0.05096559  0.02865668  0.03290748 -0.00458472
  0.0061047  -0.07984818 -0.00275343 -0.032883   -0.03731886  0.00960679
  0.03505554  0.0282577   0.0633857   0.06027174 -0.07979758 -0.08317569
 -0.06986811  0.04437708  0.06073016  0.03001677 -0.07519307 -0.00466493
  0.01716247 -0.08193516  0.07949604 -0.04288268  0.06291098 -0.00163207
  0.04701528  0.07591733  0.03309722 -0.07532427 -0.04579122  0.03925497
 -0.02015251 -0.02064202 -0.02775727 -0.03947414  0.08175139  0.02323906
 -0.03597025 -0.06213694 -0.0856574   0.07403771 -0.02022103 -0.02781744
 -0.07345756  0.06343965  0.03705371 -0.08113413  0.00095386 -0.08669559
 -0.02214534  0.02046241 -0.02836625 -0.07457491  0.01359212 -0.07738508
  0.05362063  0.05478494 -0.0352757   0.06949642  0.02242205  0.02412557
  0.06801531  0.07809531 -0.04192564 -0.00046094  0.06487623  0.05148743
 -0.01763402 -0.08380177 -0.03938763  0.04458025  0.05702396  0.02497669
  0.08861338  0.00773126  0.0319861  -0.00960602 -0.03920202 -0.08374213
 -0.03291766 -0.03730605 -0.04225088 -0.03766131  0.04107947  0.02665865
 -0.06019272  0.01961192  0.01881894 -0.07080204 -0.07227227 -0.06198343
  0.00682666 -0.08090997  0.02037505 -0.0116298   0.05243523 -0.06921741
 -0.0380045  -0.02629577 -0.06570811  0.06105157  0.07546775 -0.03874996
  0.06776384  0.02200359  0.08209776 -0.04793523 -0.00516968  0.0511865
  0.00170557  0.00116032  0.00282425  0.0145811   0.04811937 -0.04018022
  0.03927209  0.03315126 -0.0607271   0.06532922 -0.02590241  0.04268643
 -0.02554128 -0.02453298 -0.05634983 -0.00454947  0.01996828  0.04432683
  0.02510943  0.02764333 -0.04031364 -0.0239523   0.07844341 -0.02399567
  0.04771545  0.04667027 -0.04172882  0.04624004 -0.04949798 -0.0453544
 -0.03194462  0.07909563  0.08594839  0.01593157  0.01139044 -0.07834721
 -0.00354184  0.01018921 -0.08054947  0.07547189 -0.06798416 -0.07538856
 -0.00600425  0.01445069  0.03374944 -0.05616284  0.08259144 -0.00325217
 -0.06425814  0.01887133  0.04847425 -0.03695286 -0.06693641  0.04662162
  0.06705048 -0.06050424  0.03014202 -0.0817804   0.06729954  0.0031411
 -0.01410493  0.05719066 -0.05876814  0.05708409 -0.03601165  0.03673711
  0.08092929  0.01222746  0.03977106  0.04035532 -0.02263104 -0.00154617
  0.01573777  0.02479956 -0.06340286 -0.01866644 -0.00931738 -0.08773537
 -0.02991813  0.03388535  0.05563715  0.06449181 -0.07384998  0.06259033
  0.02654671 -0.06105122 -0.08399794  0.05450383  0.06245834  0.00036109
  0.05064594 -0.04061651  0.04518662  0.02295899  0.07922121  0.07468119
  0.00742919 -0.01551422  0.02519715  0.08366767 -0.01218743 -0.03776222
  0.01012836 -0.04876439  0.06808356  0.04789226  0.05661784 -0.07807672
 -0.01588196  0.03407655 -0.03890555 -0.06293023  0.06616085 -0.05817792
  0.0634557  -0.04523708  0.01643537  0.0529249  -0.06361368 -0.01387997
  0.07376232 -0.06410073 -0.04379911 -0.03464407  0.02282993 -0.05631812
  0.01977614 -0.0215062   0.07030541 -0.00545637 -0.05562531 -0.05274807
 -0.06160009 -0.06439762 -0.03477352  0.03450189 -0.0542876   0.06031421
 -0.02482708  0.0161039   0.01277159  0.04143889 -0.08465541  0.00686756
  0.06818482  0.08533701 -0.04187264 -0.06289077  0.01372847 -0.06618889
  0.05408911 -0.00884159  0.04758653  0.0009392  -0.06977805  0.01351164
  0.01028378  0.01703091 -0.03075015 -0.07642543 -0.0512655  -0.01217232
 -0.07863509  0.06496768 -0.08807102  0.0334293   0.01243022  0.01171824
  0.00699129 -0.03143376  0.04510817  0.0076556  -0.00989173 -0.01049236
  0.02950272 -0.0055706  -0.08473372  0.00267867  0.00462375  0.07005989
 -0.01857087  0.03796209  0.02426306  0.02958804 -0.06295265 -0.06100371
 -0.03195044 -0.03403372 -0.03642103 -0.04808209 -0.03546499 -0.05904341
 -0.05208867  0.00295556  0.03341685 -0.08066642 -0.0699006  -0.0840084
  0.05325606  0.03816503 -0.0390501   0.07555135  0.08598118  0.08193649
 -0.02177005 -0.04827425 -0.01429761 -0.07083907  0.03956078 -0.00628912
  0.04599624  0.02901132 -0.01749858  0.04683017 -0.02552764  0.03569309
  0.05498509 -0.06234064 -0.08394785  0.01778045  0.04418437 -0.00278682
 -0.06043974  0.07162821  0.0346805  -0.06684348 -0.06794179  0.05624422]

transformer_blocks.2.attention.output_projection.weight — shape: torch.Size([128, 128])
[[ 0.04601068 -0.04799979  0.06534322 ... -0.0211048   0.05441925
   0.01046799]
 [ 0.02262248  0.00784063  0.06760105 ...  0.04467066 -0.04594465
  -0.00619644]
 [ 0.01841236  0.09550319  0.0898263  ...  0.08377922  0.0528028
  -0.03057934]
 ...
 [-0.04312217  0.01281383  0.05934    ... -0.0125398   0.07278161
  -0.09377062]
 [-0.00912556  0.02862311  0.03788133 ... -0.01239867  0.06238243
   0.06398194]
 [-0.06042404 -0.09265798  0.07888332 ...  0.04719841  0.04625065
  -0.01989797]]

transformer_blocks.2.attention.output_projection.bias — shape: torch.Size([128])
[-0.0531669   0.04250975  0.084466   -0.05818257 -0.06360319  0.03793613
 -0.0105762   0.0762655   0.03664024 -0.03502132  0.03065398  0.04077065
 -0.0091086  -0.06073445 -0.0026111   0.00632113  0.05597515  0.00218697
  0.08611366 -0.0209532  -0.07387418  0.01050234  0.05026218  0.04237544
 -0.00380685  0.06815235  0.01257988  0.0392657   0.06521136 -0.04855461
 -0.0138769  -0.05968796  0.03535878 -0.0538638   0.03345271  0.02388868
  0.02677655  0.02068952 -0.01419685 -0.0631325  -0.08319813 -0.04363055
 -0.05654381 -0.00722962 -0.06998914  0.06467224  0.05326758 -0.04689151
 -0.03076808  0.07581704 -0.07762968  0.08538353  0.0534457  -0.07438104
  0.04674026  0.08194282 -0.05192795  0.02932969  0.06822786 -0.02061566
 -0.03112271 -0.04490202  0.03815477 -0.05064885 -0.08165765  0.00454315
  0.00340351 -0.00027129 -0.05398061 -0.05972068 -0.00895765  0.01839543
 -0.05015982  0.06118568  0.04285772 -0.06522083  0.00456397  0.03953517
 -0.03521027  0.02322406  0.07293163  0.01994802  0.07260504  0.07840151
 -0.04820743  0.08353631  0.00039318  0.00737899  0.04950069  0.0124269
  0.04501927  0.04378301  0.02857535 -0.02750298 -0.07191733 -0.06542737
 -0.05587625  0.01053454 -0.06820532 -0.02867439  0.01539396 -0.01706859
  0.08061039 -0.06353297  0.0916985   0.03623453 -0.07639156  0.01513408
  0.02661045 -0.01343418  0.0482209   0.0115692   0.05565522  0.00451712
 -0.06875402 -0.04739277  0.04336971 -0.06748477 -0.04807597  0.01786545
 -0.02640048  0.04063309  0.0146688  -0.04062162 -0.01822548  0.04304512
  0.00489479  0.0886802 ]

transformer_blocks.2.attention_layer_norm.weight — shape: torch.Size([128])
[1.0219622 1.0188779 1.0324491 1.0158106 1.0313783 1.028808  1.026455
 1.0358558 1.0237122 1.027065  1.0154865 1.0333439 1.0355289 1.0224842
 1.0255239 1.0299934 1.0229983 1.0148894 1.0316882 1.0331571 1.0205089
 1.0365254 1.02965   1.0338842 1.0173907 1.028718  1.0375497 1.0253804
 1.003776  1.0074142 1.0240306 1.0298272 1.0171264 1.0140831 1.010759
 1.0318357 1.0294749 1.0331601 1.0184814 1.0282369 1.0187887 1.0325471
 1.0323154 1.0192776 1.0394478 1.0293145 1.0156342 1.0249238 1.0307943
 1.031543  1.0112765 1.0368457 1.0274154 1.0257521 1.0352817 1.033559
 1.0077766 1.0209053 1.023796  1.027944  1.0393685 1.0264235 1.02802
 1.033884  1.0291557 1.0239861 1.0320297 1.0221422 1.0333124 1.0230454
 1.0250798 1.0305566 1.021421  1.0297483 1.0297011 1.0340261 1.0283345
 1.0214522 1.0337883 1.0352675 1.0192857 1.0251691 1.0268232 0.9949611
 1.0163219 1.031115  1.0205284 1.0163618 1.0006665 1.0245346 1.0186129
 1.022761  1.0257797 1.0237571 1.0316724 1.0139419 1.0370743 1.0199449
 1.0245818 1.0266535 1.0243719 1.0345501 1.0283564 1.0224298 1.0183619
 1.0372411 1.0324492 1.0245228 1.021012  1.0261303 1.0205325 1.0327551
 1.0192589 1.0179203 1.0295899 1.0365722 1.0215746 1.0202492 1.0356829
 1.0334651 1.0199502 1.0279964 1.0275843 1.0371324 1.027781  1.031138
 1.0281512 1.0206738]

transformer_blocks.2.attention_layer_norm.bias — shape: torch.Size([128])
[ 0.00017259  0.00221209  0.00050695 -0.00219636  0.00910482 -0.00035471
  0.00632271  0.0024789  -0.00642021  0.00958246 -0.0015933  -0.00706324
  0.00433305 -0.00488235 -0.0046902   0.01305181 -0.01583103 -0.00072173
  0.00261969 -0.00145287 -0.0025905  -0.00542287  0.00065649 -0.00556093
  0.00465391  0.00046547  0.00087985 -0.00593632  0.00884937  0.0065684
 -0.00570543  0.00809744  0.00069856 -0.00036619  0.01010189  0.00305275
  0.00728609 -0.00054291 -0.00146029 -0.00148274  0.00169698 -0.01179114
  0.00391692  0.00480606 -0.00491209 -0.00279608 -0.00621307 -0.00815884
 -0.00125063  0.00360404  0.00694225 -0.00786714  0.00351537 -0.00222613
 -0.00032141  0.00397472  0.00853646 -0.00364377 -0.00882277 -0.00500871
 -0.00087818 -0.00086795 -0.01715155  0.00083049  0.0070917   0.00027245
 -0.0063583  -0.00751129 -0.00113966  0.0015389  -0.0005946  -0.00591791
  0.01369754  0.00152458 -0.00171792  0.00040598  0.00223585 -0.00021834
 -0.00293429 -0.00436318 -0.00842089 -0.00666642  0.00670283  0.0094649
  0.00143073  0.00392055  0.0077644  -0.00016678 -0.00888278  0.00164028
  0.00599102  0.00618069  0.00377022  0.00573516 -0.00104376 -0.00382813
  0.00896577  0.00297299 -0.00064965  0.0086622  -0.00870514 -0.00414105
  0.00428036  0.00736635  0.00133239 -0.00535861  0.0051253  -0.00426225
 -0.00101249  0.00030439  0.00470219 -0.0019945  -0.00198443  0.00742506
 -0.00600778  0.00722017  0.00715491 -0.00652576 -0.00284796 -0.0034326
  0.00360343  0.00070308 -0.00624812 -0.00584666 -0.00613913 -0.0057138
  0.00822149  0.00563304]

transformer_blocks.2.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0270957  1.0298215  1.0330285  1.0269833  1.0299096  1.0300953
 1.0205975  1.0350986  1.0236765  1.036216   1.0154717  1.0169382
 1.0237659  1.0318471  1.0313803  1.0160658  1.0362163  1.0385503
 1.0264874  1.0084392  1.0309066  1.0118774  1.0308568  1.0233388
 1.0366482  0.9929201  1.0245565  1.0343547  1.0267539  0.99607074
 1.0101931  1.016957   1.0261415  1.0297394  1.0213901  1.024601
 1.027175   1.0054452  1.0264832  1.0119514  1.0317798  1.0287604
 1.0116402  1.0183845  1.0336189  1.0174655  1.0239586  1.0303221
 1.0297923  0.98814195 1.0326843  1.0141497  1.0254962  1.0271878
 1.0221798  1.0326717  1.0185919  1.0217665  1.0205027  1.0235033
 1.0121062  1.0325193  1.0324274  1.0232543  1.0205085  1.0261877
 1.0001817  1.0133772  1.034083   1.0311481  1.0287709  1.0283659
 0.9983255  1.0348163  1.0243318  1.0326546  1.0337123  1.0268575
 1.0346104  1.0280361  1.0294498  1.0293975  1.0280596  1.0406895
 1.0291501  1.02961    1.0088713  1.032149   0.99133754 1.0180994
 1.0159132  1.0008448  1.0270404  1.0224     1.0207416  1.0331689
 1.0300454  1.0086974  1.0088193  1.0314891  1.0154252  1.0214638
 1.023332   1.019      1.0267837  1.0274704  1.0033922  1.0244153
 1.036631   1.0284826  1.028697   1.0327554  1.0265374  1.0340048
 1.0212381  1.0314656  1.0286951  1.0264019  1.0328408  0.9999206
 1.0301487  1.0393211  1.0196717  1.0300436  1.039818   1.0207933
 1.025015   1.0289893 ]

transformer_blocks.2.feed_forward_layer_norm.bias — shape: torch.Size([128])
[-0.00771974 -0.00120087  0.00276666 -0.0155328   0.00634879 -0.01105653
  0.00557933  0.00488746 -0.01693134 -0.00970997 -0.01158208 -0.00681417
 -0.00162054  0.0096345  -0.00882872 -0.01652895 -0.01066773 -0.00052956
  0.01826379  0.00673279  0.0015432   0.01185008 -0.00498815 -0.00839041
  0.0221511  -0.00849515  0.0053592   0.0168249   0.01287897  0.0121809
  0.00311023 -0.00513067  0.01997529  0.01603684  0.01835843 -0.01651471
  0.01366963 -0.00019287 -0.01778401  0.01477566 -0.02053892  0.01887926
 -0.00937948 -0.01347585  0.02004513 -0.0201107   0.00401026  0.0008506
 -0.01241233 -0.00871134 -0.00046808  0.01564564  0.0150464  -0.02263927
  0.0022103  -0.00780777  0.01355943  0.02142661 -0.01129444  0.01009946
  0.00529108  0.00474383  0.00722282 -0.01138387  0.01993179  0.01989232
 -0.00995194  0.0118136   0.01964203 -0.01887169 -0.00497361 -0.02309564
 -0.01468831 -0.00138749  0.00058216 -0.00787884 -0.00202807 -0.00300947
 -0.01444643 -0.0023541  -0.00105986  0.00412879 -0.00066004 -0.01122817
 -0.0097037   0.00671098 -0.00245986  0.00904305 -0.01904415  0.02184298
 -0.00912542  0.00659424 -0.00096193 -0.0059904   0.0091755   0.01049182
 -0.00168499 -0.00573408  0.01621309  0.01034634 -0.0021185   0.00194245
  0.0215232   0.01901571  0.00067728 -0.0104205   0.02512169  0.00106288
  0.02063267  0.02140691 -0.00173938 -0.00077372 -0.00382066  0.02135615
 -0.00039472  0.00749641 -0.02212349  0.00219657 -0.00474413 -0.01119525
 -0.00618248 -0.00562814  0.01960376 -0.01500329  0.01353826  0.01446101
 -0.00241127  0.0064582 ]

transformer_blocks.2.feed_forward.0.weight — shape: torch.Size([128, 128])
[[ 0.07639865 -0.05783563 -0.06957586 ... -0.01800874  0.00038256
   0.03581957]
 [ 0.06238215  0.01042352 -0.04221322 ...  0.09301981 -0.00797382
   0.02891447]
 [ 0.05563811 -0.089167    0.09238426 ...  0.0424826  -0.10373078
  -0.05578308]
 ...
 [ 0.0687473  -0.10407832 -0.04971909 ... -0.03303967 -0.04443917
   0.01143293]
 [-0.0810084   0.09842331  0.06384128 ...  0.10300018  0.05789525
  -0.08535293]
 [ 0.0178364  -0.0443381   0.01255683 ...  0.06448249  0.04676363
  -0.00473249]]

transformer_blocks.2.feed_forward.0.bias — shape: torch.Size([128])
[ 0.07464853  0.00154954 -0.06503636 -0.02880883  0.02392975  0.07215375
  0.0655027   0.04130229 -0.05990452 -0.04000009 -0.06217096 -0.0542789
  0.10026938  0.02451159  0.01246347  0.01068336 -0.0399594   0.08787346
  0.08511341  0.08362012  0.05042091  0.02116184 -0.01472757  0.04884379
  0.00720791 -0.06331276 -0.01857136  0.07138852 -0.0442898   0.0631078
  0.00840267 -0.06799933  0.04775604  0.06251998  0.01566076  0.04769761
 -0.01681156  0.0278733  -0.04632051 -0.04611328  0.01883199 -0.02420039
 -0.06169837  0.03973584  0.00994309  0.07995961 -0.04931679  0.10611951
 -0.06520379  0.09140276  0.02795597  0.09158911 -0.07775303  0.04642026
 -0.0092929   0.0656371   0.0681931  -0.08307234 -0.08376539  0.06708819
 -0.02795185  0.0511265   0.08923233  0.08711793 -0.01029557 -0.01827068
  0.02099042  0.04061719 -0.02986846 -0.03107901 -0.02108462  0.01443638
  0.05673125 -0.02053288  0.00619659  0.04213421 -0.03287118  0.02149395
 -0.02537175  0.02044789  0.00537233 -0.01013498 -0.04234173 -0.00296632
  0.02612956  0.04597649  0.04074582  0.08507175  0.01744081  0.03672992
 -0.0610247   0.02611255 -0.0041359   0.05042631  0.05586505  0.04308481
  0.01130188  0.00246101  0.03283156 -0.00095966 -0.05870114 -0.0182364
  0.05991316 -0.00776793 -0.00034795  0.0021324   0.0623345  -0.05315756
 -0.03998737 -0.01115197  0.00556791 -0.00176578 -0.01944287 -0.0306079
  0.07495102 -0.00917039 -0.05544545  0.00952842 -0.05855699  0.02165032
  0.04252072  0.07971025  0.0889855   0.03451224  0.01308735 -0.03334608
 -0.02153956  0.04709166]

transformer_blocks.2.feed_forward.2.weight — shape: torch.Size([128, 128])
[[ 0.06447701 -0.08603951 -0.01286148 ...  0.00415266 -0.0522233
   0.05768546]
 [ 0.0176844   0.00289977  0.07941218 ... -0.06589533 -0.0690171
   0.02764487]
 [ 0.08312713 -0.08441677 -0.06502343 ... -0.00532167  0.01515211
   0.01116328]
 ...
 [-0.07829352  0.05338015  0.08173674 ...  0.02186409 -0.04930285
   0.06333286]
 [ 0.08289674  0.08753846  0.0906353  ...  0.09709364 -0.04935151
  -0.00351159]
 [-0.03324288  0.06657837  0.02342445 ...  0.0456791  -0.07499177
   0.07003442]]

transformer_blocks.2.feed_forward.2.bias — shape: torch.Size([128])
[ 0.04813725 -0.00547059 -0.05236062  0.00158791  0.01138308 -0.04828103
 -0.02640007 -0.07607282 -0.02739005  0.02159062 -0.06522234 -0.06395878
  0.07766529 -0.03653881 -0.05959855  0.01110652 -0.01028031  0.08834331
  0.03352115 -0.05694713 -0.00199838  0.06882351  0.00671698  0.05597306
  0.01023724 -0.0642877  -0.03963087  0.07796212 -0.02167442 -0.04368506
 -0.02643817  0.03388898 -0.08471739 -0.05071249 -0.04731857  0.04169028
 -0.06209627  0.04561158  0.08088297  0.04774855 -0.06023563  0.05010932
  0.07334989  0.00649992  0.05084116 -0.02064014  0.02314968 -0.07423755
  0.08439917  0.0724123  -0.0423007  -0.00849964 -0.03657207 -0.05452738
  0.06324466 -0.03594328 -0.07687207  0.07157005  0.03640115  0.03707742
  0.07610649  0.00130984  0.05619616  0.08490887  0.04064379  0.04472625
 -0.06739169  0.05247395  0.05257102  0.07084864 -0.01902951  0.04319324
 -0.04550523 -0.06836612 -0.03659859 -0.04715696 -0.08043606 -0.02813248
 -0.08248912  0.07921682 -0.09050324 -0.08368481  0.05162358  0.07782065
 -0.04406797  0.0285355  -0.01721301  0.07110247  0.04952571  0.03207569
  0.04145855  0.034306   -0.07654598  0.02989002  0.05372263  0.08945885
  0.08151529 -0.01519079  0.028434   -0.01895594  0.01162804  0.05394373
 -0.04575114 -0.08317444  0.08811618  0.07219481 -0.08008301 -0.03887426
  0.03448163 -0.01255023  0.01212045 -0.00570557  0.06367566  0.01280248
 -0.08247502  0.00485107  0.02685874 -0.02088598  0.06107245  0.06452041
  0.01884679  0.06812963  0.00635729  0.01377078  0.08024885  0.03138326
 -0.04086789 -0.00054804]

transformer_blocks.3.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.06126404  0.09644505 -0.02210505 ... -0.02329664  0.10053875
   0.05518061]
 [-0.0161269  -0.0702375   0.07373574 ...  0.05158519  0.06150791
   0.09581304]
 [ 0.04752429 -0.07481062  0.03305493 ... -0.01149824 -0.04204347
  -0.03029809]
 ...
 [-0.03367611  0.04630019 -0.00915389 ... -0.02729502 -0.04644414
  -0.06163608]
 [-0.06058768 -0.04825903  0.04871326 ...  0.0102819  -0.08609573
  -0.05949716]
 [ 0.06513672 -0.02363595  0.01278303 ... -0.08545756 -0.0592071
  -0.05539159]]

transformer_blocks.3.attention.query_key_value.bias — shape: torch.Size([384])
[-0.00857741  0.00662854 -0.07923046 -0.05624833 -0.08770359  0.02664248
  0.01071443  0.03514159 -0.07785238  0.01860766  0.05477085  0.03234871
 -0.04717344 -0.09602188  0.04909084  0.00051092  0.01940452  0.00356076
  0.01882115  0.04130204  0.0273746   0.01656066 -0.06659     0.0394724
 -0.03575239 -0.02936463 -0.09225094  0.05300503 -0.00037245  0.01039613
 -0.05125185 -0.05104632 -0.03686589 -0.02992459 -0.02327896 -0.05936972
  0.04176989  0.05757008  0.07747748 -0.03973057  0.06114345 -0.06073411
  0.00597772  0.03368352  0.05655994  0.03067132  0.03858565 -0.06127332
 -0.00371902  0.0175773   0.03102689 -0.0335045   0.04908155  0.06284916
 -0.00250138 -0.0229721   0.06021779  0.05643623 -0.06032952 -0.11642932
  0.00881153  0.06999161 -0.00076057  0.0117012  -0.05996833 -0.06656367
  0.01699617 -0.0447475   0.07044912 -0.04233211 -0.0566804  -0.03670068
 -0.02189277 -0.01840881  0.01851213  0.06370225 -0.01771231  0.04811687
 -0.03019457 -0.01440979 -0.01859504  0.04497629 -0.07615641 -0.07721964
 -0.07865115  0.00655118 -0.04488879 -0.05706672 -0.08943877 -0.04605392
  0.07420691  0.07057463  0.08943513 -0.04736321 -0.04649447 -0.0595904
  0.00146555  0.02404321 -0.01920284 -0.00390668  0.05844916  0.05009271
  0.05796258 -0.05012488  0.02155259 -0.05800339 -0.07994688  0.06938569
 -0.08429606 -0.01468629 -0.0714292  -0.04204018  0.05774079 -0.01059378
  0.00018276 -0.01179612  0.06108524 -0.07763278 -0.04095842 -0.0407424
 -0.02934457 -0.0439969   0.00545591  0.05084622  0.06241466  0.01131082
  0.0570649  -0.07971067 -0.05214791  0.02844937  0.03360974  0.08689059
  0.05546691 -0.00103562  0.08541564 -0.00547393 -0.03194054 -0.08446136
  0.00267559 -0.04068781  0.04353313  0.07720093 -0.02497074 -0.0205637
  0.00432308 -0.06015873 -0.07400776 -0.01634714 -0.04191633  0.00370142
 -0.04756261 -0.07099298  0.0878979  -0.06643545  0.08256048  0.02008832
 -0.07894939 -0.0137478  -0.02087918  0.01325184  0.00559315  0.0692668
 -0.0509057  -0.06333554 -0.08508707  0.0443602  -0.03001784 -0.03304333
  0.03870642 -0.01721263 -0.05828039 -0.0374717   0.07810298 -0.0798305
 -0.04952205  0.01089961 -0.00611589 -0.06789916  0.07402351 -0.05444905
 -0.07479405  0.05554572  0.02705954 -0.00090064  0.02889093 -0.00811934
  0.05815527  0.00982353 -0.05943674  0.04110446  0.01568582 -0.08164798
  0.01547955  0.02509137 -0.07049093 -0.07486111  0.01189651  0.02418695
  0.07623116  0.02353368  0.02949659  0.07681355 -0.04102192  0.01504577
  0.07336137  0.04049934 -0.05307782  0.04431076 -0.02543605  0.0054522
  0.0265041  -0.08228972 -0.04925017 -0.01624052  0.06063711 -0.04047967
  0.06745576  0.00170507  0.04592422  0.07057703 -0.01692929 -0.01958208
 -0.03155768  0.06585996  0.00862892 -0.07427095  0.08800126 -0.08294249
 -0.00306365  0.01550288 -0.00348055 -0.04498612  0.05277441  0.07086372
 -0.06202798 -0.08748414  0.05611437  0.02378125 -0.04256339  0.05978163
  0.05218858 -0.03438965 -0.05856605 -0.04191351  0.02923082  0.08329792
 -0.01358953 -0.07270401 -0.01953995  0.08229832  0.0063755   0.02022692
  0.02881485  0.06445203  0.08555092  0.04095724  0.00460064  0.07522841
 -0.05482482  0.00037769  0.01627434  0.04709414  0.07782736 -0.04188371
 -0.03579918 -0.05081603  0.00542516 -0.05731839 -0.03881992  0.05086703
  0.01406248  0.03286418  0.03641189 -0.05341172 -0.00073644 -0.03511817
 -0.00588027 -0.00600321  0.04832178 -0.02818369 -0.08919991  0.00931791
 -0.02743235  0.01192671 -0.01311151  0.05754101  0.05489217 -0.03588811
 -0.03210626  0.0398666  -0.03298417  0.08190248 -0.00510324 -0.0043813
 -0.0669347   0.01573712 -0.02798185 -0.02417194  0.04230011 -0.00652259
  0.03795825 -0.02024376 -0.02281228  0.0770851  -0.0523237   0.08054207
  0.00752291 -0.00702841  0.01185955  0.04828     0.05945457 -0.00780655
  0.04721171  0.08296982 -0.05895254  0.06353614  0.05546924 -0.00498204
 -0.06516658  0.03545792 -0.06483573  0.00637482 -0.01207443 -0.00089556
 -0.04295735 -0.01408312 -0.06929828 -0.08790709 -0.03536517  0.04538861
 -0.07155851  0.07101858 -0.02212157 -0.08431948  0.06034747 -0.00676905
 -0.04458269  0.02463923  0.03835128  0.00230444  0.07576371 -0.0837303
 -0.08093568 -0.04470062 -0.02745038  0.08238724 -0.01800237  0.02127641
  0.00340132 -0.04932314  0.07186865 -0.08009308 -0.06655042 -0.0903472
  0.03869864 -0.07408995 -0.07176583  0.05474351 -0.05136734  0.03925318
  0.04687769  0.03506587 -0.07055357  0.0689056   0.00967378 -0.06446523
 -0.05318036 -0.03181334 -0.04886543  0.00368384  0.03309248  0.06846478
 -0.06022388 -0.03450664  0.06428263 -0.01066942 -0.02123324  0.02172961
 -0.0388326   0.03544775  0.04836725 -0.08478389 -0.04114439 -0.07553037]

transformer_blocks.3.attention.output_projection.weight — shape: torch.Size([128, 128])
[[ 0.00053084 -0.05194875  0.02243965 ... -0.07900263  0.06878266
  -0.03032811]
 [-0.04338054  0.00347252  0.02380515 ... -0.03764405 -0.02740676
   0.00893868]
 [-0.03806087  0.01440993  0.01745286 ... -0.06423019 -0.05081748
   0.03925021]
 ...
 [-0.07076024  0.01440064  0.0678087  ... -0.00087065  0.02951093
   0.05271541]
 [-0.02484303 -0.06235759  0.0336252  ... -0.06615292 -0.0531903
   0.07938745]
 [-0.02927031 -0.07755656 -0.02217654 ... -0.05995201 -0.07276885
   0.07490196]]

transformer_blocks.3.attention.output_projection.bias — shape: torch.Size([128])
[ 0.0371335  -0.05530357 -0.01190773  0.05113144  0.0757666   0.01454126
 -0.06931759 -0.04832663  0.05260719 -0.01623016  0.02033804  0.07153123
 -0.02820349 -0.01365052 -0.08549879  0.00165299  0.07569194 -0.04887938
 -0.07757697  0.02103147 -0.00069023 -0.00252512  0.02621286 -0.0378192
  0.08628044  0.07480467 -0.06992539  0.00785296  0.08161363  0.02426117
  0.04448934  0.04689986  0.06696275 -0.02114621  0.05750137  0.07974067
 -0.0150273   0.02163147 -0.01629486 -0.02820424  0.00386306  0.08626742
  0.0803891  -0.08342543  0.02870924  0.07301177  0.02617858 -0.00277579
 -0.0103657  -0.05906425 -0.01737943  0.06757323 -0.07984837  0.02580629
  0.03329572  0.08540243  0.08228452 -0.05194013  0.01473426  0.02124965
 -0.02879545  0.08220801  0.00985316  0.00349195 -0.06423134  0.00898158
  0.05739364 -0.07640922 -0.03094465  0.06932982 -0.03214305  0.00176377
  0.08113223  0.07312439 -0.0653888  -0.0565298   0.07876733 -0.04326206
 -0.01984907  0.03210253 -0.01515027  0.07146199  0.03637669  0.00270285
 -0.005779    0.07532776  0.07406944 -0.00185335  0.04662476  0.07549261
  0.00257898  0.06106628 -0.00772426  0.0762689   0.05146406  0.01480239
 -0.07848143  0.04627438  0.04205698 -0.07095525 -0.06819816 -0.06450132
  0.01303011  0.04896087  0.05596629 -0.01902317 -0.02004514 -0.00363401
 -0.0435199  -0.00294409  0.01547552  0.05053833  0.07150366 -0.06784637
 -0.03954078  0.08334973 -0.01081588 -0.04485829 -0.0347352   0.01905145
  0.06210581 -0.03474871 -0.05139825  0.08620431  0.00515503  0.02596397
  0.01322767  0.0356981 ]

transformer_blocks.3.attention_layer_norm.weight — shape: torch.Size([128])
[1.0272158 1.025166  1.0132192 1.0089673 1.0080676 1.0308601 1.031505
 1.0329617 1.0195311 1.0257518 1.0234158 1.0285422 1.0295995 1.0243686
 1.0146674 1.0092081 1.0220543 1.0271875 1.024582  1.0341613 1.0257773
 1.0294341 1.0360849 1.0330656 1.0225025 1.010774  1.0336881 1.0335591
 1.0306665 1.0046593 1.0255125 1.0324972 1.0282891 1.0247307 1.0162973
 1.0300007 1.029651  1.0195775 1.0273811 1.0265067 1.0278343 1.0203664
 1.027169  1.0155127 1.0264442 1.0325392 1.0170164 1.0256722 1.011011
 1.0204513 1.0063337 1.0186235 1.021926  1.029341  1.0301393 1.029077
 1.0083288 1.0302422 1.0307513 1.0220925 1.0357506 1.028488  1.0217292
 1.019318  1.020312  1.0281035 1.025156  1.0254091 1.0207586 1.025622
 1.019188  1.0194865 1.025284  1.0175948 1.0263809 1.0276098 1.0169941
 1.0116522 1.0080451 1.0259976 1.0324761 1.0208578 1.033041  1.0131226
 1.019941  1.0355619 1.0122335 1.0261352 1.0134712 1.0211675 1.0283723
 1.0119036 1.0218915 1.0093939 1.0326847 1.0216535 1.0180957 1.0296909
 1.0296278 1.034265  1.0305372 1.0246716 1.0224631 1.0116216 1.0322386
 1.0164219 1.0193977 1.0192224 1.0283642 1.025906  1.0182449 1.034134
 1.0218029 1.0343926 1.0194002 1.0257727 1.0211637 1.0355806 1.0355656
 1.0301727 1.0210863 1.0262344 1.0116119 1.0252076 1.03354   1.0338159
 1.0325313 1.0289173]

transformer_blocks.3.attention_layer_norm.bias — shape: torch.Size([128])
[-1.35958451e-03 -7.81792973e-04  1.00802490e-02 -1.53683336e-03
 -4.99642547e-03  6.76141772e-03 -2.50665494e-03  4.14003013e-03
  2.53997697e-03 -2.85274372e-03 -6.31935662e-03 -1.04947984e-02
 -3.10332514e-03 -2.45714816e-03 -2.05136929e-03  1.23029388e-02
 -4.80961427e-03  4.59737331e-03 -1.71186887e-02  6.31138543e-03
 -2.97659589e-03  7.96733075e-04  3.60789942e-03 -4.13110107e-03
 -3.47036403e-04 -2.82935775e-03  4.66754136e-04 -1.10417865e-02
 -1.39123702e-03  3.13738384e-03 -4.26659500e-03  3.44235124e-03
 -2.91223056e-04  3.02581023e-03 -2.96372105e-03  3.41528771e-03
 -7.32994871e-04 -7.48605048e-03 -2.37105647e-03  5.65262977e-03
  2.24415516e-03 -1.72497600e-03 -6.40957337e-03 -1.12543534e-03
 -1.76203772e-02 -1.06934868e-02 -1.92500811e-05  7.81722274e-03
 -7.77457049e-03  4.01272485e-03  3.62716056e-03 -2.23847246e-03
 -9.15949233e-04 -8.77641491e-04 -2.26663565e-03 -2.54052947e-03
  3.45650944e-03 -8.28989781e-03 -2.59831874e-03 -5.16556017e-03
  3.74690891e-04  6.64966041e-03 -2.92461715e-03  1.10927904e-02
 -3.25620221e-03 -3.84283043e-03  8.00684281e-03 -5.83227165e-03
 -1.16418237e-02  3.52022704e-04 -6.30047265e-03  8.50594603e-03
  5.18003944e-03  1.11376739e-03 -1.70785713e-03  6.80932833e-04
  1.59208383e-03  8.42302616e-05 -6.19755872e-03 -3.05746240e-03
 -1.19558896e-03 -6.77415868e-03  7.76517857e-03  7.22569413e-03
  2.40648375e-03  1.09673350e-03  1.13010663e-03  2.16981652e-03
 -4.83918888e-03  5.45391953e-03 -3.57854617e-04 -2.28157919e-03
  7.33262906e-03 -1.69080123e-03  6.63515273e-03  7.25981593e-03
 -1.10862672e-03 -1.04313679e-02 -5.39641827e-03 -1.74837455e-03
 -3.13184084e-03  6.18254906e-03  4.67964681e-03  2.81655393e-03
 -1.23230033e-02 -1.10416543e-02  1.57381094e-03 -6.20221952e-03
  1.14693027e-02 -1.89699873e-03  5.36311744e-03  2.12432467e-03
 -4.47632046e-03  2.59611313e-03  4.53526201e-03 -1.04470519e-05
  7.43788481e-03  4.84278845e-03  4.35943343e-03 -5.09742973e-03
  6.75818138e-03 -1.63378788e-03 -1.85982522e-03  2.96696625e-03
  2.64553563e-03 -1.00693237e-02 -4.97966679e-03 -2.62815040e-04]

transformer_blocks.3.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0300125 1.025844  1.0174477 1.0374779 1.0328721 1.015642  1.0151129
 1.0320163 1.0175279 1.0342871 1.0122789 1.0195377 1.0283438 1.0318544
 1.0244538 1.0275356 1.0274434 1.0108254 1.0326201 1.0295124 1.0224897
 1.0440979 1.0341811 1.038169  1.0219581 1.0316013 1.0319331 1.0086776
 1.00864   1.0069151 1.030106  1.0332991 1.0372518 1.0234743 1.0248783
 1.0218887 1.0207353 1.0322881 1.0235591 1.0297275 1.0234579 1.0296656
 1.0331124 1.025364  1.0286295 1.0304788 1.0250647 1.0290658 1.0302687
 1.0286746 1.0343285 1.0345912 1.0231762 1.0290468 1.0337929 1.0340407
 1.0312033 1.0200956 0.9870193 1.0310239 1.026909  1.0278349 1.0275692
 1.0303711 1.0264374 1.0320137 1.0299237 1.0292464 1.0180395 1.0302744
 1.0337617 1.0239929 1.0283835 1.0077518 1.0223721 1.024398  1.0398517
 1.0282515 1.037904  1.0331349 1.0288647 1.03219   1.0231613 1.0026119
 1.0158619 1.0313085 1.0288762 1.0200578 1.037571  1.0310057 1.027226
 1.029098  1.0192639 1.0269445 1.0308836 1.0315164 1.0167665 1.0291333
 1.027444  1.02478   1.0141034 1.0324295 1.0093691 1.021044  1.0295945
 1.0157194 1.0370716 1.0239328 0.9854203 1.0294842 1.0296222 1.0313109
 1.0295613 1.0188279 1.0307597 1.0235863 1.0358769 1.0325313 1.0349103
 1.0166227 1.0067252 1.0278242 1.015415  1.0276608 1.0307828 1.0249496
 1.028512  1.031484 ]

transformer_blocks.3.feed_forward_layer_norm.bias — shape: torch.Size([128])
[-0.01442028 -0.01018623 -0.01255029 -0.00237574  0.00968468 -0.0065322
 -0.00834869 -0.00759844 -0.00108165  0.01160338 -0.012712    0.02524888
 -0.0019007  -0.01299     0.00318738  0.01504701  0.00551143  0.00303883
  0.01983446  0.02711741  0.00346872 -0.00684094 -0.01052843  0.00948587
  0.02281328 -0.00094789  0.00547639 -0.00430844  0.0163496   0.00854546
  0.00344212  0.01901773  0.01122015  0.00216322  0.00963771  0.01008239
  0.01950365 -0.00724462 -0.01297058  0.01253946  0.02464113 -0.00456386
 -0.02187401 -0.00606717 -0.02052383 -0.02558856  0.01178165  0.01070366
  0.00298065  0.01154452  0.00131973  0.0263458   0.0128836   0.00241905
  0.01042127  0.00297193 -0.00392145 -0.01432979 -0.00336016 -0.00147276
 -0.01404869 -0.00724732  0.01528791  0.0120229  -0.01218194  0.02147057
  0.01023895  0.00840857  0.00694045 -0.00633856 -0.00985473 -0.02296601
 -0.01265272 -0.01685376 -0.00129828 -0.00264081  0.02343029 -0.01419218
 -0.00948874 -0.02626678  0.00066489  0.00279333 -0.00466643  0.01022597
 -0.01016368 -0.0115128   0.01320291 -0.01801281  0.00819205 -0.01420433
 -0.00957203  0.00218361 -0.00460143  0.01089349  0.00619319 -0.01230079
 -0.00314469  0.01596952  0.00546646 -0.00512317  0.0235284  -0.01251325
  0.0036909  -0.00217034 -0.01284403 -0.01553636  0.00854194 -0.01377612
 -0.00097327 -0.016174    0.01232317 -0.00324176 -0.00175574  0.00474907
 -0.01688575 -0.00356679  0.01348671  0.0150618  -0.02384517  0.01860695
  0.01343696  0.00702693  0.00811673 -0.00357996  0.00727494 -0.00742748
  0.00604986  0.02118235]

transformer_blocks.3.feed_forward.0.weight — shape: torch.Size([128, 128])
[[ 0.01525903  0.05198893 -0.08592095 ... -0.10011287  0.01780865
  -0.08539802]
 [ 0.08439489  0.00888631 -0.02178808 ... -0.02589952  0.11428175
   0.00556957]
 [-0.03948222  0.08940781 -0.07149218 ...  0.03958752  0.10476769
   0.05057847]
 ...
 [ 0.00220824  0.0892076   0.01174066 ... -0.0729834  -0.09100122
  -0.06870718]
 [-0.08419972 -0.0274495   0.01437852 ...  0.08372377 -0.03242938
  -0.05453649]
 [ 0.00738968 -0.07545517  0.01478916 ...  0.03391545 -0.05588107
  -0.055062  ]]

transformer_blocks.3.feed_forward.0.bias — shape: torch.Size([128])
[-1.03720287e-02 -1.18046990e-02 -5.78542612e-03  8.17435328e-03
  1.06682340e-02  6.94218650e-02  8.03929344e-02 -1.65221859e-02
  4.29886766e-02  6.74438551e-02 -6.90166205e-02 -4.89681680e-03
  3.21476944e-02 -3.25493924e-02  1.07569888e-01  7.34431744e-02
  3.84627618e-02 -8.70686471e-02  1.00898437e-01  2.78121103e-02
 -3.85199673e-02 -1.58126131e-02  1.67500367e-03  6.52493387e-02
  4.68850546e-02  2.52566803e-02 -4.48124744e-02  1.00198351e-02
  1.59154590e-02 -2.04406679e-02 -6.14416376e-02 -6.46798760e-02
  3.01332865e-02  1.75771695e-02  2.31319796e-02 -6.31898642e-02
 -3.07673234e-02  7.92897493e-02 -4.03395221e-02  8.33084732e-02
 -1.21623818e-02 -6.00478761e-02 -8.57024416e-02  1.03057101e-02
  6.62787482e-02  3.36417258e-02 -3.98874544e-02  1.02511458e-01
 -4.96136881e-02 -2.97462791e-02  6.92021921e-02  1.77689791e-02
 -2.55841594e-02  9.53407958e-03  3.69723476e-02 -1.05489716e-02
 -6.36819378e-02 -2.64761802e-02  1.21811545e-03 -2.40958873e-02
  5.98666780e-02  7.84322098e-02 -4.70209792e-02 -4.58444320e-02
 -1.50609622e-02 -5.10273203e-02  2.17331182e-02 -5.86233884e-02
  9.26914439e-02  5.41862324e-02 -4.53812331e-02 -7.29327649e-02
 -2.41144747e-03 -4.83616143e-02  7.61779621e-02  1.75291318e-02
 -6.07262477e-02  7.85917640e-02  9.22990143e-02  7.92857334e-02
  1.45358881e-02  1.74239557e-02  1.85546000e-02  1.04163565e-01
 -6.80986568e-02  9.64840949e-02 -9.63024236e-03 -3.76212411e-02
  7.28440508e-02 -3.50345671e-02  3.98781896e-02 -5.99206658e-03
 -8.06258544e-02  8.56706500e-03 -2.62749102e-02 -5.18895313e-02
 -1.08762505e-02 -2.75592227e-02  6.33457154e-02  3.75547707e-02
 -2.95596048e-02 -4.50475104e-02  3.88215557e-02 -2.85597276e-02
  3.55699323e-02  9.64693874e-02 -3.11743300e-02 -2.77403854e-02
 -1.92171484e-02  7.26066137e-05 -5.99999316e-02  7.52695352e-02
 -2.06952393e-02  2.60265321e-02  9.00189504e-02  1.07844854e-02
  7.25521296e-02  1.01754665e-02  4.50326353e-02 -7.47212470e-02
 -1.35269696e-02  2.62425933e-02 -2.85766856e-03 -5.96617647e-02
  6.38053641e-02  3.59998606e-02  3.51157039e-02  3.93359475e-02]

transformer_blocks.3.feed_forward.2.weight — shape: torch.Size([128, 128])
[[-0.08986737 -0.07273784 -0.1003328  ...  0.03430611 -0.00681386
  -0.08787168]
 [-0.09932518 -0.01310452 -0.06015256 ...  0.05494889 -0.07529651
   0.05632166]
 [ 0.05725263 -0.08179568  0.06096844 ... -0.04228514  0.01899264
   0.05149401]
 ...
 [-0.0278982  -0.05423895 -0.06134764 ... -0.02593339 -0.0857946
   0.05538014]
 [-0.00680241 -0.09177408 -0.08950895 ... -0.00537202  0.08587292
   0.01732368]
 [-0.00822239 -0.02710967 -0.07143318 ... -0.09650396  0.01408144
  -0.09099847]]

transformer_blocks.3.feed_forward.2.bias — shape: torch.Size([128])
[-0.05571076  0.03546934 -0.0046062  -0.06479433 -0.04156089  0.09052849
  0.01073914  0.01532438 -0.02093336  0.07095172  0.02999871 -0.0013672
 -0.01964052  0.06426679 -0.01895724 -0.04715462 -0.0415557   0.02488235
 -0.07140154 -0.05961325  0.01291087 -0.03171729  0.03364766 -0.04933843
 -0.08813477 -0.0398512  -0.01684142 -0.07280339  0.04912817  0.06298257
  0.05055765 -0.04779422 -0.00443776 -0.06142821  0.0640508   0.02000217
 -0.03840886 -0.03271912  0.00698107  0.02112103 -0.08237099  0.0801485
  0.06310693 -0.00236275 -0.07113305  0.03996921  0.03766999 -0.05502896
  0.03467058  0.05800162 -0.06634943  0.00857129 -0.0047724   0.01776072
  0.00192899 -0.03166386 -0.03245464  0.01303052  0.00384856 -0.03742666
  0.08339474 -0.01511086  0.06707964 -0.00743943 -0.06685108 -0.01338698
 -0.0038393  -0.08474851  0.01142862 -0.07676918  0.06267405 -0.01703462
  0.04345125 -0.03819059  0.0185258   0.01581167 -0.07612482  0.06927311
 -0.03714586  0.02810194  0.01548631 -0.00150618  0.04887073 -0.07581589
  0.04981415  0.05406245  0.06730139 -0.04930959 -0.08763252 -0.0313327
  0.03239028 -0.04444835 -0.01560369 -0.05872828  0.06999757 -0.04777445
 -0.03064754  0.05614387 -0.03345929 -0.06046316 -0.00596619  0.05608216
  0.05960453 -0.01942131  0.05205416 -0.06260292  0.02253852 -0.00593681
 -0.07982398  0.03296876  0.08286334  0.01606479 -0.01066005  0.07799868
 -0.01223816  0.02051198 -0.03868919  0.07115361 -0.08424793  0.05808663
 -0.01382989  0.03440937 -0.01796774  0.03193153 -0.00621837 -0.01457676
 -0.00711193 -0.08513054]

transformer_blocks.4.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.09321766 -0.03890441  0.02295243 ...  0.02342395 -0.04235233
  -0.03675907]
 [-0.04724919  0.04924116 -0.04333859 ... -0.07109938 -0.05772338
  -0.08123232]
 [ 0.05530692 -0.08031601 -0.053324   ...  0.04957623 -0.00745037
   0.03981379]
 ...
 [ 0.04287665 -0.03366023 -0.03690168 ... -0.0694717  -0.02068662
   0.0783862 ]
 [ 0.08232498 -0.06823414  0.02108715 ... -0.0005247   0.08679144
   0.06368334]
 [-0.03256177  0.00562393 -0.07957766 ... -0.04664209  0.07516364
   0.09460337]]

transformer_blocks.4.attention.query_key_value.bias — shape: torch.Size([384])
[ 9.73155499e-02 -2.65092365e-02  5.36494404e-02 -6.82755411e-02
  3.07934172e-02  5.28292209e-02  1.19230160e-02 -6.00369610e-02
 -2.48385891e-02 -1.01626769e-01  8.78607761e-03 -2.04527918e-02
 -2.54920889e-02 -2.14197561e-02 -7.15650544e-02 -4.95968871e-02
  7.01650828e-02  8.59072246e-03 -7.45689198e-02 -2.14677081e-02
 -8.10902789e-02  3.42130917e-03 -1.44117037e-02  4.81426865e-02
 -3.80853266e-02  3.70087940e-03 -8.64075795e-02  3.02007850e-02
  1.64509024e-02  9.99692921e-03  6.22348748e-02  7.79844522e-02
  6.76129013e-02  1.23550575e-02  1.98129360e-02  2.07821950e-02
 -3.32763642e-02  3.81948836e-02 -6.44481853e-02 -4.73485403e-02
  6.40498400e-02  6.33257814e-03  5.47277508e-04  5.83193563e-02
 -1.29888495e-02 -1.71180274e-02  8.29098299e-02  4.05018777e-02
  6.63339272e-02  5.82837909e-02 -5.94080091e-02  5.51703162e-02
 -4.44173925e-02 -6.34761236e-05  3.65045220e-02  6.06056303e-02
 -4.89568450e-02  2.87092049e-02  4.59044985e-02  8.69236216e-02
 -4.31851335e-02 -4.78996597e-02 -1.61176454e-02 -1.39124237e-03
  7.85351023e-02  7.93905109e-02 -4.95544039e-02  2.89682895e-02
 -7.07637891e-02 -4.42014113e-02  3.06062168e-03  5.88379391e-02
  1.02158584e-01  2.48017013e-02  7.57180154e-02 -8.11365172e-02
 -9.23383385e-02  7.75192901e-02 -3.06311455e-02 -9.95398499e-03
 -6.50402308e-02 -5.14542609e-02 -4.44708504e-02 -2.48391870e-02
 -5.49802883e-03 -8.91380534e-02  2.15341095e-02 -3.17676216e-02
 -5.09803779e-02  4.78841551e-02 -5.61244972e-02  7.95550719e-02
  1.13189570e-03  1.95854474e-02  8.01647678e-02  1.29254404e-02
  3.82225923e-02  4.44705114e-02  5.00222016e-03 -2.42217910e-02
  6.01837188e-02 -1.05784123e-03 -1.99750718e-02  2.39249486e-02
 -1.66683514e-02 -3.50008793e-02 -3.10866199e-02 -6.33831099e-02
  2.33697090e-02 -1.43777058e-02  8.40422884e-03  3.06730792e-02
 -8.56399909e-02 -5.79773672e-02 -5.90943582e-02 -2.76560597e-02
 -1.80638041e-02 -4.83630039e-02  6.08761199e-02  1.73446164e-02
  4.96035069e-03 -8.05831105e-02 -7.88567811e-02  7.67393634e-02
  7.86235109e-02 -1.89514980e-02  6.90142736e-02  4.10284176e-02
 -7.55994767e-02  8.36287662e-02  8.39452073e-02  2.61282418e-02
  2.13373508e-02 -5.03984801e-02 -3.18941176e-02 -5.86624146e-02
 -7.50436215e-03 -3.78687382e-02  8.42526630e-02 -1.97418649e-02
 -3.52263451e-02  6.69109523e-02  5.42184385e-03 -2.54570656e-02
  7.37891272e-02  3.36282450e-04  2.44296547e-02 -1.69357732e-02
 -1.02501372e-02 -6.68721274e-02 -7.63180628e-02  3.96115445e-02
 -1.73184071e-02 -2.23490279e-02 -6.86955964e-03  5.97558320e-02
  1.03496909e-02 -4.56099249e-02 -5.63151725e-02  8.41927379e-02
  4.24792878e-02 -7.26683065e-02 -3.52453366e-02 -3.65182315e-03
 -3.39399315e-02 -2.49725245e-02 -1.56227043e-02  1.16525625e-03
  3.42691988e-02 -2.39399425e-03 -5.25270663e-02  1.28926532e-02
  5.76894507e-02 -8.83167312e-02 -2.24326309e-02 -4.56844419e-02
  3.43311578e-02 -4.00548540e-02  5.87243820e-03 -1.07936934e-02
  6.59787878e-02  6.04404919e-02 -3.03082187e-02 -7.96729848e-02
 -6.27751499e-02 -2.37589087e-02  5.26960790e-02 -3.26914228e-02
  4.05151434e-02  1.97999142e-02  3.20318416e-02  3.18476856e-02
  4.29780073e-02  1.48010030e-02 -5.01374938e-02 -8.59014392e-02
  7.29266033e-02 -3.69058438e-02  1.85688920e-02 -8.54125023e-02
  1.67060010e-02  1.96587341e-03 -6.57698363e-02  3.42819728e-02
 -3.22237127e-02  6.35725558e-02  6.01930767e-02 -4.93659861e-02
  2.78604794e-02 -1.68204997e-02  3.86796519e-02  1.06835021e-02
 -5.86398691e-02 -2.74120569e-02  3.49456221e-02  1.02501130e-02
 -7.44204149e-02  1.05542568e-02 -6.63324222e-02 -5.66099584e-02
 -4.62210961e-02 -7.89065957e-02 -6.74759373e-02  4.74013872e-02
  2.00162381e-02 -1.03132501e-02 -5.85795892e-03  7.41735175e-02
 -7.00676441e-02  8.75509828e-02 -7.44289756e-02 -6.89044967e-02
 -3.59649435e-02  4.11027893e-02  5.05861901e-02 -3.22599858e-02
 -4.31326255e-02  6.85269982e-02 -9.64414887e-03  3.43095660e-02
  2.32424345e-02  1.46848829e-02  4.05125208e-02 -1.67767946e-02
 -5.62822856e-02 -4.86769825e-02  7.10379481e-02  1.49052320e-02
 -5.45225218e-02 -3.94465663e-02  5.35489433e-02  5.30027645e-03
  6.69476464e-02  4.82092761e-02 -4.65937704e-03  7.97356665e-02
 -9.30482463e-04  8.11848640e-02 -3.82284783e-02  7.80111970e-03
 -4.15708497e-02  7.68384263e-02 -2.11818027e-03 -7.75706992e-02
  1.32369790e-02  1.07080247e-02 -5.02235107e-02  2.62631532e-02
  2.49909097e-03  3.77642252e-02  7.35232700e-03 -8.68600830e-02
 -8.21403489e-02 -5.85432872e-02 -1.57167688e-02 -1.30179562e-02
  2.22709216e-02 -4.82625924e-02  3.36409546e-02 -5.24825975e-02
 -4.35949527e-02 -6.98255301e-02  4.83997108e-04 -6.57160301e-03
  8.66137594e-02  6.97147697e-02  6.40092269e-02  3.74693759e-02
  5.95679618e-02  3.97039205e-02 -4.72993068e-02  2.80122422e-02
 -1.06829712e-02  2.06738175e-03 -2.71315649e-02 -2.48398669e-02
 -3.45212966e-02 -7.00567439e-02 -1.41321896e-02  1.90093964e-02
  7.75769576e-02  6.25118688e-02  1.39027899e-02 -7.12139457e-02
  8.29442665e-02  7.62715712e-02 -6.15992472e-02 -4.71256264e-02
 -8.44331682e-02  6.27424940e-02 -8.45876930e-04  6.27582371e-02
  4.07009125e-02 -4.46155667e-02  8.73218626e-02  4.61491719e-02
  8.89045093e-03 -8.87131095e-02 -3.67840827e-02  8.03994294e-03
  8.78037810e-02 -3.20154913e-02 -1.78924073e-02 -2.23719664e-02
 -3.84657122e-02  1.32944100e-02 -7.50260055e-02 -8.32471922e-02
  5.80417775e-02  2.30862461e-02  8.41931906e-03  6.53911307e-02
  8.22922811e-02  5.92787452e-02 -3.54351066e-02 -5.01093268e-02
  6.05498403e-02  5.27480133e-02 -9.25028324e-03 -1.47124184e-02
 -2.83209793e-02 -3.73606645e-02  4.20506708e-02 -2.24332195e-02
  1.84453279e-02 -3.20366537e-03  5.15375435e-02 -1.78093538e-02
  3.16631757e-02  1.38807353e-02  7.78873935e-02 -1.85895786e-02
 -4.12956346e-03  6.51880130e-02 -2.07471419e-02 -4.00113575e-02
 -6.22125678e-02 -4.45899665e-02 -3.81552689e-02 -5.51585406e-02
  3.93226072e-02  3.60267460e-02 -4.26823758e-02  2.77823955e-02
  3.36559042e-02  6.63383752e-02  5.58902323e-02 -6.77584782e-02
  1.07478015e-02 -2.25559380e-02  6.34015724e-02  4.61610444e-02
  1.12526417e-02 -1.88116767e-02 -2.95941979e-02  6.36359975e-02
 -5.72420890e-03  3.41346301e-03 -6.60407031e-03  6.89373016e-02
  4.73029092e-02  9.70589183e-03 -8.14589038e-02 -1.40432622e-02]

transformer_blocks.4.attention.output_projection.weight — shape: torch.Size([128, 128])
[[-0.04858092 -0.0431532  -0.05396629 ... -0.00916457  0.04570701
  -0.02579124]
 [-0.07885682 -0.01695135 -0.05649217 ...  0.0001865  -0.05726996
  -0.06886484]
 [ 0.05843947  0.04493426 -0.07055872 ... -0.01973809  0.08139268
   0.03015178]
 ...
 [ 0.11789257  0.02355799 -0.06922385 ... -0.03560827  0.10512662
   0.05989254]
 [ 0.05154331  0.07434715  0.01989863 ...  0.05552887 -0.08309883
  -0.07792879]
 [ 0.03253212  0.08240088 -0.06380277 ... -0.07023122  0.01331749
  -0.01384373]]

transformer_blocks.4.attention.output_projection.bias — shape: torch.Size([128])
[-0.00433461 -0.08410855 -0.0645077  -0.05795326 -0.05237758 -0.08628237
 -0.00292984 -0.00373208  0.02171295  0.07220847 -0.05220071 -0.05066992
  0.04634843 -0.01698991 -0.00628702  0.02762211  0.04551748 -0.05558178
 -0.01008129 -0.00623173  0.00996079 -0.00976294  0.02418188  0.07674736
  0.06562813 -0.01645197 -0.07560886  0.0413151  -0.06709366  0.02516459
  0.0303467  -0.07526226 -0.08805053 -0.07386754 -0.03489971  0.02541983
 -0.01294313 -0.02861205 -0.04825208 -0.02295299  0.06170556  0.01527202
 -0.01006158  0.02207355 -0.06007169 -0.018027    0.00827936 -0.07995292
  0.02701048 -0.0356166   0.05821086 -0.07001343 -0.0557074  -0.03202655
  0.07484894 -0.08267443  0.03469112 -0.0872798   0.05437826  0.05280228
 -0.02859346 -0.08661103  0.01940647 -0.01673033  0.07091762 -0.01206335
  0.04152318 -0.0709997  -0.0356118  -0.03108255  0.05264439 -0.03213114
  0.08701387  0.07212197  0.02247865 -0.03605151  0.00499766 -0.00515897
 -0.01378864 -0.06810933 -0.05040005  0.06644806  0.01438307  0.03670419
 -0.05312058  0.06198051  0.06667308  0.04343574  0.02779559 -0.01593537
 -0.06665008 -0.0282689  -0.06606491  0.03728255 -0.01740531  0.0800667
  0.03312539 -0.08632797  0.03993359  0.0055269  -0.07308877 -0.05013983
  0.02833383  0.0061753   0.07963562 -0.05074353 -0.06093723 -0.03237561
 -0.07727188  0.0819684   0.02847454 -0.0569846   0.00226489  0.05251465
 -0.04666378  0.01915977  0.00677981 -0.06897183 -0.07772367  0.02983564
  0.01700718 -0.0723733   0.04580975  0.04585103  0.00861357  0.08112201
  0.00068509  0.00097929]

transformer_blocks.4.attention_layer_norm.weight — shape: torch.Size([128])
[1.0318576  1.016818   1.0168661  1.0302022  1.0223541  1.0347885
 1.0174912  1.0070959  1.0358338  1.0281821  1.021716   1.0168828
 1.0320954  1.0279559  1.0144495  1.0237079  1.018381   1.032035
 1.0286678  1.0208433  1.0267131  1.0208951  1.0066794  1.0321379
 1.0118488  1.0250887  1.0181482  1.0191791  0.99794704 1.0197406
 1.0027292  1.0011861  1.0169715  1.027811   1.0227123  1.0252573
 1.0264094  1.0270045  1.0157263  1.0264441  1.0343391  1.0209464
 1.024332   1.0160544  1.0211459  1.0282835  1.0170128  1.0228944
 1.0238805  1.0347021  1.0000551  1.0370989  1.0167238  1.0294449
 1.0235215  1.0253357  1.0039511  1.0288966  1.0316987  1.0133929
 1.0215589  1.0315291  1.0191827  1.0129735  1.0306789  1.0310541
 1.0316322  1.019768   1.0278828  1.0147403  1.0251644  1.02008
 1.0275728  1.0311718  1.016288   1.0356952  1.0332427  0.992239
 1.0092924  1.0209274  1.0239186  1.0412198  1.0215063  1.0111091
 1.0240847  1.0217391  1.0319284  1.0272638  1.0011171  1.0307704
 1.027858   1.0215603  1.0309803  1.008923   1.0195161  1.015283
 1.027296   1.024175   1.0342971  1.0268134  1.0120014  1.0354344
 1.0354309  1.023684   1.0244625  1.0264214  1.0249258  1.0338229
 1.0312673  1.0264173  1.029895   1.0376621  1.0082631  1.0209445
 1.0251224  1.0289176  1.0251461  1.0292125  1.0261447  1.0300308
 1.0286812  1.0120895  1.0243893  1.0308936  1.0374166  1.0184505
 1.0266008  1.0229187 ]

transformer_blocks.4.attention_layer_norm.bias — shape: torch.Size([128])
[ 0.00352269  0.01245313  0.00822042 -0.00395051 -0.0022996   0.00994946
  0.00476517  0.00594867 -0.00181719  0.0019351  -0.00278277 -0.00038096
 -0.00285502 -0.01325557 -0.00134245  0.00864021 -0.0091338   0.00302733
  0.00413676  0.00474498 -0.00896451 -0.00292956 -0.00611461 -0.00358828
  0.01170463 -0.00482177  0.00069665  0.000834   -0.00186504 -0.00055531
 -0.00039502  0.00793215  0.00272014 -0.00554358  0.01638833 -0.00757068
 -0.0021364  -0.00438137  0.0028304   0.00343299 -0.01058658  0.0008522
 -0.00873721  0.00877211 -0.00768137 -0.002017   -0.00564818 -0.00201655
 -0.01101029 -0.00666006  0.00427095 -0.00253302  0.00663023 -0.00268597
 -0.00315913  0.00235022  0.00174036  0.00230566 -0.00391364 -0.00999504
  0.0026867  -0.00265297 -0.01115391  0.00866042  0.00550997  0.0073646
 -0.00092921  0.0017337  -0.00327045  0.00400209  0.00208268  0.0046154
 -0.00254442  0.00146372  0.00319651  0.00807347  0.00115995 -0.00913262
  0.00183531 -0.00213269  0.00015646 -0.00603954 -0.00260248  0.00685189
  0.0018699  -0.00987172  0.00170399 -0.00758452 -0.00136077 -0.00073884
  0.00157259 -0.00437391  0.01875798  0.00266115  0.00044442  0.00189479
 -0.00384135 -0.00078903  0.00073263 -0.001827   -0.00138776 -0.00171491
  0.00709847  0.00157667  0.00187871 -0.00378638  0.0011851  -0.01393984
  0.01358589 -0.0020946   0.0087853  -0.00674675 -0.00808934  0.00057201
 -0.00106324 -0.00168063  0.00131549  0.01008507 -0.00488897  0.00218509
  0.01111813  0.00049711 -0.00759641 -0.0026781   0.00228447 -0.00174726
  0.00196964  0.0013795 ]

transformer_blocks.4.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0122195  1.024448   1.008357   1.0276912  1.0305914  1.0381544
 1.0267267  1.0304083  1.0179166  1.0229151  1.0098872  1.0299824
 1.0298325  1.0372052  1.0138358  1.0302771  1.0139737  1.0248698
 1.0289689  1.036352   1.0387852  1.0212046  1.0249933  1.0200634
 1.0257068  1.0106641  1.0324036  1.0252597  1.0286419  1.0351735
 1.0165759  1.0174811  1.0159775  1.0283114  1.0024725  1.028119
 1.0212147  1.035507   1.0242143  1.031995   1.0274698  1.0221263
 1.0330641  1.0253023  1.0256859  1.0373966  1.0215583  1.0274395
 1.0215589  1.0345947  1.0326576  1.026717   1.0147322  1.0280863
 1.0207622  1.0303022  1.0301421  1.0275096  1.0159932  1.0298228
 1.0330769  0.99010456 1.0248623  1.026051   1.0265255  1.0206648
 1.0240825  1.0296451  0.99998206 1.0303253  1.0380167  1.032141
 1.0129646  1.0234861  1.0320679  1.0183977  1.0292085  1.0291512
 1.0318835  1.0368892  1.0332862  1.03       1.0315363  1.034984
 1.0307032  1.0255727  1.0304264  1.0144845  1.0230781  1.008644
 1.0315614  1.0169218  1.0267619  1.0170834  1.0185629  1.0361676
 1.0346946  0.990073   1.0386147  1.0296164  1.0271478  1.0325724
 1.0396694  1.0381182  1.0310125  1.0035163  0.9917592  0.98186535
 1.0113856  1.0216544  1.0295223  1.0363477  1.022834   1.0201234
 1.0294245  1.021454   1.0302548  1.0298839  1.0281346  1.0327948
 1.0282103  1.0259821  1.0301254  1.0352858  1.0298418  1.0049481
 1.0301243  1.0245148 ]

transformer_blocks.4.feed_forward_layer_norm.bias — shape: torch.Size([128])
[ 6.94646558e-04  1.28996484e-02 -1.49232466e-02  2.23698411e-02
  7.99485296e-03  7.53364293e-03  1.07946498e-02  6.81027304e-03
 -8.00053403e-03 -1.23586822e-02 -6.49744365e-03 -1.59744099e-02
 -8.24476406e-03  1.04513578e-02  6.91563683e-03 -2.01366544e-02
 -1.25364373e-02  4.10490250e-03 -4.85673640e-03 -1.72722470e-02
 -1.16606606e-02  1.65827796e-02  7.23072980e-03  7.31808040e-03
  1.92456543e-02  2.09851172e-02 -6.84741046e-03 -2.15181969e-02
 -2.91396491e-03 -6.39785919e-03 -7.99828861e-03 -1.10095777e-02
  2.33629296e-04 -1.09136384e-02 -3.09039559e-03 -4.33319621e-03
  2.33438890e-02  1.64202955e-02  1.11247832e-02 -2.97819264e-03
  1.60318259e-02  1.58971082e-02 -2.27037192e-04  1.89165911e-03
 -1.96692050e-02  6.64283568e-03  3.50477453e-03 -1.20127881e-02
  1.35368239e-02  8.70565604e-03  7.23280478e-03  4.64586401e-03
  1.88657595e-03  8.66073300e-04  1.16549395e-02 -5.47222979e-03
 -5.48033044e-03 -8.64511076e-03 -4.45905328e-03  8.56336765e-03
  1.62292626e-02 -5.81309339e-03 -1.37121817e-02 -8.22399091e-03
 -3.53964488e-03 -6.07140642e-03  1.44647306e-03  6.65775035e-03
  2.17824448e-02 -1.03519252e-02 -1.55035756e-03 -2.62868195e-03
 -1.21328635e-02 -9.82065126e-03 -1.14907688e-02 -2.05257125e-02
 -6.26390381e-03 -1.27359172e-02  6.11861190e-03 -2.46481411e-02
 -5.16955275e-03 -2.10055964e-06 -1.58392414e-02  3.33511573e-03
 -6.24164706e-03 -1.22048836e-02 -1.76447518e-02 -5.01352653e-04
 -1.22990776e-02  1.80588104e-02 -3.28028388e-03  1.42344190e-02
  1.24637941e-02 -6.91168895e-03  2.19389498e-02 -4.64855833e-03
  1.22951344e-02 -2.48371647e-03  5.29521331e-03 -7.22585898e-03
 -8.15890823e-03  4.74361284e-03 -5.08939265e-04 -5.05828008e-04
  1.71820614e-02  1.81342626e-03  3.70334275e-03 -2.44084504e-02
  4.55581024e-03  4.98046959e-03  8.17913655e-03 -5.45061007e-03
  2.34016727e-04  1.50212068e-02  2.93279905e-03 -1.87921580e-02
 -1.02431746e-02 -4.01334232e-03 -1.28300386e-02  1.53707024e-02
  9.10460670e-03 -1.54637983e-02  2.03487016e-02  1.09359082e-02
  1.43900011e-02  7.44426972e-04  8.43464769e-03 -7.52885418e-04]

transformer_blocks.4.feed_forward.0.weight — shape: torch.Size([128, 128])
[[ 0.05995478 -0.05538218  0.10172661 ... -0.02409791  0.04177678
   0.02543122]
 [ 0.08315421  0.04763616  0.09933463 ...  0.04702844 -0.03884206
  -0.10956498]
 [-0.07388711  0.03886566 -0.04983725 ...  0.02108051 -0.03196485
   0.08940201]
 ...
 [ 0.08987793 -0.00489948 -0.02314954 ... -0.02815269  0.0660143
   0.07750267]
 [-0.0172005   0.0249178  -0.05831191 ...  0.00734791  0.03662432
   0.0278885 ]
 [-0.04084155 -0.0123554   0.05206758 ... -0.00076257 -0.02291646
  -0.09580725]]

transformer_blocks.4.feed_forward.0.bias — shape: torch.Size([128])
[ 0.04270104 -0.0751066   0.08886611 -0.02144372  0.02085928 -0.07063993
  0.05107095  0.06389761  0.01132487  0.08024321  0.0575343   0.09759326
 -0.06723301  0.01421372 -0.00943649 -0.00666724  0.05390143 -0.0776862
  0.0826257   0.05621047 -0.06340671  0.03535154  0.04559328  0.03910004
 -0.05202024 -0.0476042  -0.03624864  0.01170857 -0.0127024  -0.03724476
 -0.00549971  0.02084476 -0.05185095 -0.08489754  0.06133796  0.03772831
  0.07697553  0.02152182  0.08034956 -0.00857978  0.05311753 -0.03716095
  0.09095719  0.0947631  -0.0134681   0.02864809 -0.02248964  0.0640533
  0.05513136 -0.04933263  0.04915543  0.02325739  0.00057846  0.06900655
 -0.01599241  0.07104213 -0.09125972  0.04284389 -0.00171561 -0.03904615
 -0.00703454 -0.02671374 -0.06632385  0.06873386 -0.05504627  0.08417425
 -0.0314702   0.01681357 -0.04159305  0.00731939 -0.07453254 -0.06137803
  0.02517324  0.04612282 -0.01202329 -0.03927661  0.06645357  0.0663261
  0.0154467  -0.06417848 -0.05018745  0.06194957 -0.08762503  0.10490703
 -0.00549491 -0.00882034  0.0788809   0.05228959 -0.02996808  0.00908689
 -0.05032576  0.01875246  0.03005748  0.08861677  0.07519997  0.01783716
 -0.05977891 -0.00518783  0.04412926  0.01786923  0.04181524 -0.00859312
  0.07121132  0.04688593  0.00281897 -0.04208928 -0.07795668  0.06731838
  0.09916756  0.02948626 -0.00256916 -0.07450259 -0.08670456  0.03844736
 -0.02045895 -0.0125456   0.04462276 -0.02806486 -0.04816899  0.08117741
  0.09873919  0.08965962 -0.05744797 -0.08258684 -0.0366866  -0.05197268
  0.01556404  0.07457776]

transformer_blocks.4.feed_forward.2.weight — shape: torch.Size([128, 128])
[[-0.07650641  0.05410288 -0.05040172 ... -0.03702185  0.01121194
   0.00310694]
 [ 0.08982698  0.00355897 -0.05780191 ... -0.04012001  0.08641014
  -0.07049537]
 [-0.00361875  0.0586105   0.06488039 ...  0.03238652  0.0680526
  -0.02977302]
 ...
 [ 0.04494031 -0.06342637  0.0123014  ...  0.04931193  0.05549767
  -0.02104806]
 [-0.09603746 -0.10158152 -0.02152012 ...  0.09945913 -0.03946364
   0.00949161]
 [-0.03731097  0.1080351  -0.02560901 ... -0.02854824 -0.03929531
  -0.03918578]]

transformer_blocks.4.feed_forward.2.bias — shape: torch.Size([128])
[-0.06980343  0.04221861  0.02577574 -0.00453315 -0.07874712 -0.08312433
 -0.07124501  0.04071257 -0.07580979  0.00522105 -0.03822825  0.02756362
  0.00368814  0.01964539  0.07641525  0.01291092 -0.03630123 -0.09062108
 -0.04516136 -0.02519026  0.08556492 -0.01552656 -0.08892344 -0.0079444
 -0.07092233  0.00554744  0.01212705  0.04854412  0.08484498  0.00593829
 -0.04683004  0.07888182  0.00425711  0.06080941  0.00040376 -0.07576904
 -0.03141335 -0.02406229  0.00584403  0.0439511  -0.08237277 -0.03355674
 -0.06890495  0.06556704 -0.03781815  0.04202649  0.0112881   0.06297255
  0.07575651  0.02763711  0.06034782 -0.0614785   0.02314717 -0.01341051
  0.05334317 -0.08624076  0.0154263  -0.04315865  0.00936519 -0.04989637
  0.01622358 -0.0169404   0.02107029 -0.02879812 -0.0713466  -0.0398081
 -0.02340417  0.01351347  0.02034911  0.00481359  0.00843646 -0.03813481
 -0.01860842  0.07974938  0.00750866  0.06086431 -0.02172507  0.05536919
  0.01470404  0.01661788 -0.01008248 -0.007991   -0.02287724 -0.03717124
 -0.04705628  0.08651549  0.04143165  0.04278004  0.02243589  0.00734404
  0.07842653  0.0252443   0.03663232 -0.0111696  -0.00186769  0.0512661
  0.05583331 -0.03443323 -0.020169   -0.03789612 -0.05719034  0.00024943
 -0.05424825 -0.0197449  -0.06729516 -0.02354694  0.02501475  0.02802291
  0.03534691  0.06302803 -0.05693816  0.06211193 -0.02840518  0.05194328
 -0.01994677  0.0374836  -0.08465968  0.02331538  0.00665599  0.05765376
 -0.07314089 -0.03821965  0.06969555  0.07683577 -0.07015058  0.07472901
 -0.03067766  0.04326336]

transformer_blocks.5.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[-0.00580721  0.0138829  -0.09344523 ...  0.08385285 -0.00332589
  -0.0696377 ]
 [-0.0508523   0.01204837 -0.07055313 ... -0.0284429  -0.06857283
   0.01755543]
 [ 0.01822253 -0.02569464 -0.01259644 ... -0.01684324  0.09611352
  -0.10692872]
 ...
 [ 0.07862045  0.06093903 -0.08696815 ... -0.07573798 -0.01459242
  -0.03458708]
 [-0.02583494 -0.03603956  0.0366229  ... -0.03657655  0.02333094
   0.00743376]
 [-0.00429753  0.08054901 -0.00473235 ... -0.05876739  0.08768551
   0.05341218]]

transformer_blocks.5.attention.query_key_value.bias — shape: torch.Size([384])
[ 0.02976256 -0.03892102  0.09476655 -0.07476021 -0.03462722 -0.03416045
 -0.08438923 -0.00322582 -0.07473075  0.05015627 -0.00926039  0.02505555
  0.00677339  0.07441221 -0.06335404 -0.03591144 -0.07518186 -0.04247602
 -0.03709081  0.08161902 -0.0614391   0.05172003 -0.06328367 -0.07123765
 -0.04066259  0.02461734 -0.01050097 -0.06045973 -0.01679662 -0.07741611
 -0.01964904 -0.0069438   0.07792722 -0.02539661 -0.05897104  0.02285149
  0.0885822  -0.01551011 -0.09208658 -0.08022568  0.05031155 -0.01710288
  0.01344748  0.04642765 -0.07767919 -0.04333642 -0.02296662 -0.0107501
  0.04114196  0.05374943  0.04951128 -0.02303217  0.02469588 -0.05438681
  0.05869709  0.04887849  0.02292491  0.00149045  0.02333803  0.01596357
  0.02066854  0.05040809 -0.04543372 -0.03936398  0.00365042 -0.05007463
  0.08300067 -0.02114039  0.01380873  0.03630067  0.07913349 -0.08927583
 -0.03200028  0.05924684 -0.07153776 -0.08201466 -0.0766113   0.05637122
 -0.09225739 -0.01299439  0.00828176 -0.0118296  -0.03794487  0.05861742
  0.04656584  0.04503877 -0.04027678  0.06514557 -0.04287864 -0.05542887
  0.02478545 -0.08141942  0.05260619  0.06210656  0.07883043  0.00294846
  0.01852052 -0.03662571  0.03508456 -0.07137848  0.00078768 -0.0433945
  0.09577904 -0.02403721 -0.06187253  0.05884933 -0.01412839 -0.04310169
  0.00613018  0.03960125 -0.0016742   0.09067871 -0.10656007 -0.06093318
 -0.01155813  0.02675476 -0.0433469   0.05772825 -0.0542321  -0.08032724
  0.06963781 -0.08513193 -0.07189646  0.07698019 -0.02014841 -0.07711576
 -0.02337366 -0.02631488 -0.03814409  0.0760242   0.07768755  0.0627454
 -0.08529662 -0.01062042  0.08063868 -0.05379685  0.0528738  -0.00755282
  0.06751476  0.00226797 -0.01522638  0.0529545  -0.05246609 -0.05306026
  0.05403001  0.03319838 -0.02488552 -0.08273463 -0.00162955 -0.03077053
  0.01496447  0.00770096 -0.0517731   0.04675103 -0.08124933  0.08699163
 -0.07478465 -0.02211414  0.00847084  0.06117057  0.07066593  0.03800415
  0.07123382  0.0501362   0.0305112   0.06120197  0.04664903 -0.07723679
  0.02012371  0.04307142 -0.00758292  0.02931494  0.02607017 -0.01271525
 -0.00709456  0.05680134  0.07228983 -0.00230364  0.06942803 -0.01398239
  0.08721725  0.05652599 -0.03720852  0.08633175  0.03997349 -0.01052437
  0.07400736  0.02124417 -0.06626176 -0.02736471 -0.06695537  0.07745197
 -0.05917057 -0.03272969  0.01903609 -0.08193992 -0.06807639 -0.06377345
  0.02404879 -0.08472667  0.01975898  0.0596397  -0.02861375  0.03882402
 -0.0769885   0.08312792 -0.0356107  -0.00844982 -0.06539359  0.00467506
 -0.03288565 -0.02869799 -0.07360151  0.01041163 -0.08729243  0.07551953
 -0.07442819  0.07926275  0.02594748 -0.03123337  0.0755529  -0.02953517
  0.04243007  0.0789668  -0.02495849  0.06579093  0.00388754 -0.04975798
 -0.08230286 -0.06052988 -0.03835647  0.07591555  0.07541269 -0.02846059
 -0.00444084 -0.03952071 -0.08055539 -0.00897317 -0.04538248 -0.03533462
  0.08238725  0.08717779 -0.04291015 -0.07322916  0.01921633  0.07380203
 -0.03531789  0.06144455 -0.00534716 -0.03350543 -0.08673358  0.07060312
 -0.02304226 -0.05112425  0.08748143 -0.00121104  0.08053215  0.05032112
 -0.0150443  -0.0894961  -0.01001597  0.02997534  0.05241863  0.06451838
 -0.07979064  0.03420012  0.04277232  0.04376233  0.00174662  0.04289195
 -0.0695431  -0.00022109  0.09076407  0.07969695  0.06047968  0.09159926
 -0.05113564  0.00448945  0.01643364 -0.01657425 -0.03948875 -0.06031258
 -0.04652659 -0.02325328  0.08764919 -0.0595801  -0.0384329   0.02458699
  0.0737462  -0.0140181  -0.01005808 -0.07595874 -0.0158367   0.02590706
  0.04959556 -0.08330377 -0.02996632 -0.03578114  0.00861019 -0.0625507
  0.04237585 -0.01022677  0.07289787  0.07935132 -0.05392944  0.00566312
 -0.07326602  0.05578871 -0.08619084 -0.04527006 -0.01211918  0.04935008
 -0.05613175 -0.06614439  0.01571175 -0.06661336  0.06164022 -0.01829769
 -0.07232366 -0.01532107 -0.00443846  0.00612834 -0.01913124 -0.05383302
  0.04909422 -0.03581023  0.06250389  0.06934972  0.0621119  -0.02299036
 -0.0246762   0.06585354 -0.02188588  0.03858309 -0.08586679  0.06337794
  0.01891932  0.03518508 -0.08258582  0.0111465  -0.08113655  0.00134236
  0.02022863 -0.0134195  -0.03331955 -0.04166625  0.03878113 -0.01390709
  0.07772116  0.06328975  0.07571054 -0.06944181  0.01600788  0.05605625
 -0.07631005  0.0526128  -0.01455908 -0.04966484  0.01880959 -0.03253451
  0.0757102   0.06359676 -0.07841878  0.03462077 -0.00982582  0.0801964
 -0.01985624 -0.01744083  0.05478981  0.04347867  0.04942971 -0.05391631
  0.00451369  0.00462966 -0.00184763 -0.0503661  -0.03206294 -0.04349528
 -0.08324897  0.04042038  0.01132999 -0.01841833 -0.08431652 -0.02817412]

transformer_blocks.5.attention.output_projection.weight — shape: torch.Size([128, 128])
[[-0.00344245 -0.06108839  0.04883201 ...  0.05861647  0.02098888
  -0.04278579]
 [ 0.06014378 -0.08304177  0.04938189 ...  0.01969588  0.03214018
   0.04277655]
 [ 0.07893637 -0.03861818 -0.02360701 ...  0.03948542  0.08328398
   0.02948277]
 ...
 [ 0.04692261  0.01088709  0.02908906 ... -0.00795954 -0.03551157
  -0.08921523]
 [-0.01166463  0.06340013 -0.08192198 ... -0.00440471 -0.02831222
  -0.05414839]
 [ 0.0674087  -0.07374565  0.01649707 ...  0.07760666 -0.06643767
  -0.0540336 ]]

transformer_blocks.5.attention.output_projection.bias — shape: torch.Size([128])
[ 0.0592177   0.02185558 -0.06449708 -0.07047275 -0.02943619  0.04271592
  0.05551593 -0.0649823   0.02591215 -0.01486716 -0.03351155  0.03521263
 -0.04924417  0.05504753 -0.0767768   0.00658195 -0.00142716 -0.00194426
 -0.01005242  0.06826676  0.07126717  0.03275127  0.08306745  0.00516031
 -0.02833262  0.05292841  0.09371112 -0.08158783  0.02962093 -0.00771668
  0.01983414  0.02902859 -0.05223525 -0.00083406  0.05946227  0.05887243
 -0.07397533  0.0472185   0.00940387  0.00699864  0.00792444  0.06889866
  0.02762245 -0.05082737  0.0115785   0.04911862  0.0802907  -0.0275124
 -0.02057106  0.01423526 -0.02744278  0.05361961 -0.0414007   0.00694661
  0.01383694  0.09038538 -0.01334504  0.05269945 -0.08339589  0.00561147
 -0.05974806  0.08699902  0.00277879  0.03747321  0.00611656  0.04867309
 -0.01615727 -0.01994224  0.00337376 -0.04046309 -0.04081525  0.02079151
  0.00095227  0.01709121  0.06106972  0.02664559 -0.05344688 -0.01913113
 -0.00373007  0.04306444 -0.05208188  0.08126374  0.09028031  0.00178735
  0.04044208 -0.04827595  0.01544581  0.04375482  0.06094906  0.0229427
 -0.07015616  0.0760858   0.03346228  0.03212603 -0.01086767 -0.07122707
  0.05194939 -0.08022577  0.02582837  0.0727662  -0.04981092  0.0806292
  0.05675198  0.08134949  0.06082127 -0.04055917 -0.07901486  0.0033952
 -0.06117643 -0.05852006 -0.03308313  0.0366251  -0.05609407  0.07002871
  0.03339137  0.04155927  0.06293405  0.00260741  0.07016291  0.01116552
 -0.03315366 -0.06598362  0.02953701 -0.02380229 -0.05662055 -0.03989467
 -0.08438364 -0.07892708]

transformer_blocks.5.attention_layer_norm.weight — shape: torch.Size([128])
[1.0347415 1.0192025 1.0175798 1.028036  1.0342711 1.0291235 1.0099245
 1.0127323 1.0341363 1.0340917 1.0244348 1.0161214 1.0218426 1.020621
 1.0061756 1.0086478 1.0146844 1.02032   1.0394642 1.028489  1.0268857
 1.0311278 1.0067735 1.0325543 1.0143057 1.0125159 1.0270643 1.033247
 1.029445  1.0185461 1.0098026 1.0136133 1.0090333 1.0284226 1.0144845
 1.03048   1.0233643 1.0281788 1.0364145 1.0163879 1.0338657 1.0052145
 1.0332861 1.0204763 1.0139627 1.0177362 1.0207636 1.0304984 1.0210553
 1.0429943 1.0207849 1.0140713 1.0092808 1.0282811 1.0221819 1.0094727
 1.0119992 1.0235515 1.0237113 1.0148098 1.0306634 1.0171852 1.0185323
 1.0162001 1.0331337 1.0270104 1.0190307 1.0299826 1.0251838 1.0128236
 1.0181718 1.0382259 1.0224582 1.013388  1.0236372 1.0305753 1.0187534
 1.0114722 1.0142775 1.0276332 1.0308652 1.0343033 1.0183117 1.0207728
 1.0151643 1.0201828 1.0313839 1.0215662 1.0025781 1.0359728 1.0193518
 1.0244392 1.0311197 1.0282372 1.0294268 1.0381609 1.0135046 1.0244403
 1.0247695 1.0088959 1.0125246 1.0290775 1.0319839 1.0192537 1.0226803
 1.0096166 1.0141797 1.0246906 1.0227708 1.017974  1.0059717 1.0388758
 1.0196196 1.0277251 1.0220603 1.0255879 1.0145199 1.0116435 1.0306915
 1.0261759 1.0217158 1.0222046 1.028029  1.030993  1.0354611 1.0337338
 1.0276936 1.0176792]

transformer_blocks.5.attention_layer_norm.bias — shape: torch.Size([128])
[-5.27520990e-03  1.77827012e-03  8.82016309e-03 -3.96665093e-03
  6.23675669e-03  4.75945324e-03  5.05185162e-04  7.42605608e-03
 -1.98685890e-03 -4.88641998e-03  5.92028303e-03 -8.73310119e-03
  1.20101972e-02 -8.94968677e-03  8.59913998e-04  1.01880185e-04
 -4.54554660e-03  8.77397414e-03 -1.83773460e-03  1.08218263e-03
  3.67208454e-03  3.18192737e-03 -3.63787031e-03  5.99935791e-03
  3.70883988e-03 -5.46866655e-03 -4.44352627e-03  1.34682877e-03
  4.47967090e-03 -3.85383889e-03 -1.25048589e-02  1.90768414e-03
  6.38582918e-04 -1.15985237e-03  3.09118535e-03  7.09775835e-04
  1.11456530e-03  4.13269317e-03 -2.81350547e-03 -2.02028779e-03
  2.66076502e-04  3.01905838e-03 -1.78234428e-02 -1.47232076e-03
 -2.77682045e-03 -9.51346382e-03  1.47423579e-03 -2.65396899e-03
 -1.22067817e-02  4.52114921e-03  1.20247798e-02 -1.95799978e-03
  1.07425367e-02  1.48170646e-02 -1.04829109e-04 -2.04359437e-03
  1.79710006e-03 -2.05472996e-03 -5.82429441e-03  5.06411830e-04
  2.34884140e-03  1.47692603e-03 -5.65981586e-03  5.44426148e-04
  1.61911640e-02 -1.18754385e-02  5.94397495e-03 -8.80388485e-04
 -3.55007942e-04  3.04040383e-03  5.77739440e-04  1.73703898e-02
 -3.86090047e-04 -2.65602674e-03 -3.59506905e-03  1.06432093e-02
 -4.60114563e-03 -4.35203692e-05  2.66872789e-03  2.76190811e-04
  5.99838444e-04 -4.40730993e-03  7.83415232e-03  9.54434089e-03
  8.92616622e-03  1.01483322e-03  5.77429403e-03 -1.16661622e-03
  4.90822946e-04  9.66044434e-04  1.31616881e-02 -1.02873603e-02
  1.19166495e-02  4.78429766e-03 -7.11616885e-04 -1.92467670e-03
  1.24053552e-03 -9.05491970e-03 -1.15662245e-02  8.47683696e-04
 -1.52007223e-03 -6.21913048e-03 -2.42268303e-04  1.17339734e-02
  8.64811358e-04  1.08863635e-03  5.19600185e-03  1.50155189e-04
  1.19752123e-03 -6.38327189e-03  4.92793368e-03 -8.23226012e-03
 -2.95624137e-03 -3.03111924e-03 -1.21787051e-02 -6.75453851e-03
  5.29601891e-03  6.08169520e-03  5.76483924e-03  2.41602727e-04
  7.70487124e-03  4.32519056e-03 -6.82413112e-03 -9.15487180e-04
 -7.53379660e-03 -6.29776716e-03  8.65022745e-03  9.57542565e-03]

transformer_blocks.5.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.044948  1.036145  1.0289534 1.0296302 1.0416937 1.0323455 1.0330018
 1.0257932 1.0334888 1.0166397 1.0306246 1.031853  1.0288234 1.0330138
 1.024646  1.0309985 1.0241319 1.0329977 1.0294516 1.0149314 1.0301132
 1.0265067 1.0199391 1.02754   1.0258946 1.0345837 1.0285157 1.0344247
 1.0334363 1.0329192 1.0045979 1.0215275 1.0302873 1.0265542 1.0341299
 1.0326909 1.0365472 1.0187522 1.0080584 1.0156294 1.0304686 1.0094817
 1.0184444 1.0202827 1.0185406 1.0369389 1.0289319 1.0323244 1.035687
 1.033317  1.0327374 1.0338401 1.0131825 1.0315346 1.0341161 1.0304655
 1.0241072 1.0224319 1.0310905 1.0243694 1.0319177 1.0222964 1.0291123
 1.0300145 1.0243531 1.0214504 1.0311229 1.0272335 1.0356405 1.0224125
 1.0272057 1.016541  1.0267851 1.0317374 1.0186822 1.0198011 1.0254029
 1.028304  1.0489877 1.0301608 1.0332693 1.0298961 1.0360926 1.0195621
 1.0138403 1.0384203 1.0250462 1.0194134 1.006351  1.0255792 1.023267
 1.0368067 1.0112295 1.0235876 1.0181279 1.0391079 1.0347857 1.0264162
 1.0250175 1.0326782 1.023957  1.0322247 1.0221217 1.0302887 1.0353469
 1.0324179 1.0287516 1.0393001 1.0259148 1.0200961 1.0266008 1.0246782
 1.0241307 1.0331217 1.0263267 1.0367492 1.0271583 1.0330147 1.0147148
 1.029429  1.0302169 1.03179   1.032156  1.0286213 1.0355569 1.0174587
 1.0044973 1.0161288]

transformer_blocks.5.feed_forward_layer_norm.bias — shape: torch.Size([128])
[ 1.37059912e-02 -1.69701464e-02 -6.34146202e-03  7.63512077e-03
  4.37614834e-03  1.06198275e-02 -1.07920598e-02  1.99229512e-02
 -1.72685031e-02 -5.50735416e-03 -9.13555082e-03 -3.34935402e-03
 -1.84254814e-02 -9.96427704e-03  1.31313959e-02 -1.66286044e-02
  8.66771489e-03  2.24552467e-03  2.42758775e-03  7.92235229e-03
  1.43373357e-02  1.24136191e-02  3.10661979e-02  1.55771989e-02
 -4.29913355e-03  5.16517879e-03  1.85378522e-04  1.99154243e-02
 -1.60088874e-02  1.05449418e-03 -1.01374257e-02 -3.83178587e-03
  2.93343398e-03 -1.29181435e-02 -9.87746846e-03  1.06922463e-02
 -1.84224488e-03  5.54943224e-03  4.09145560e-03  1.32773025e-02
  1.03668552e-02 -1.26735251e-02 -2.62001948e-03 -1.48210321e-02
 -6.69391872e-03  1.44540621e-02  9.34186950e-03  8.85992683e-03
  7.02304626e-03  8.39683600e-03  6.14214456e-03  9.90250055e-03
 -2.83341529e-03 -2.27157269e-02 -1.11615295e-02 -2.03562640e-02
 -9.41629056e-03  2.67070788e-03 -8.84012040e-03  2.28868257e-02
 -8.88625346e-03 -8.53926037e-03  9.53438971e-03 -1.04333851e-02
 -1.13851940e-02  6.36679493e-03 -8.10074096e-04  7.91072194e-03
 -2.79704086e-03  1.09513383e-02 -8.51043779e-03  2.39960346e-02
 -2.56344816e-03 -1.48650799e-02 -2.01974772e-02 -6.68326532e-03
 -2.63280496e-02 -6.11351477e-03 -1.15355533e-02 -2.22000480e-02
  1.53659768e-02 -9.35539417e-03  2.58051436e-02  4.78717615e-04
  1.76396351e-02  8.25559534e-03  1.87428053e-02  1.30953928e-02
  7.13936752e-05  1.07665854e-02 -1.73837766e-02  1.74375046e-02
 -1.41501371e-02 -1.44380713e-02 -2.23932564e-02  3.84313916e-03
  1.05346432e-02 -9.07577015e-03 -1.42444167e-02 -7.20036030e-03
  2.06386447e-02 -1.56712662e-02  7.49011803e-03 -7.30652874e-03
  2.60234047e-02 -2.54505072e-02 -6.00260310e-03  1.06715271e-02
  8.36225133e-03 -1.86036415e-02 -1.92321725e-02  2.60014571e-02
  1.78605448e-02 -1.88616868e-02  1.27187464e-02  1.68715660e-02
  1.85393747e-02  8.94006714e-03 -8.54340009e-03 -1.18595141e-03
 -2.09444556e-02  8.94987676e-03  1.46566667e-02 -2.82497760e-02
  1.09808752e-02  1.99462827e-02 -2.23428849e-02 -9.52117424e-03]

transformer_blocks.5.feed_forward.0.weight — shape: torch.Size([128, 128])
[[-0.12200452 -0.04538786 -0.09329981 ...  0.0144079   0.00611339
  -0.08291604]
 [-0.00307129 -0.04032151 -0.00833606 ...  0.10679832  0.03543568
  -0.06125524]
 [-0.01898875  0.04424315 -0.00091732 ...  0.00672047 -0.05906326
  -0.07524479]
 ...
 [-0.02592878  0.0599343  -0.06754844 ... -0.02332721 -0.00012381
  -0.03871382]
 [-0.04765641 -0.01975608  0.04054006 ... -0.00839449  0.07899818
  -0.01591073]
 [-0.03012737 -0.04763905  0.05060092 ...  0.07307906 -0.00604771
   0.05382159]]

transformer_blocks.5.feed_forward.0.bias — shape: torch.Size([128])
[-0.0680337   0.04971245  0.06668936  0.08432454 -0.05690042  0.09586794
 -0.0433691   0.06458404 -0.05846885 -0.02629628 -0.0062105   0.09525734
  0.04140718 -0.0555187   0.04289136 -0.04690525 -0.02977072 -0.06881638
 -0.04460062  0.08359371  0.01858369  0.02296722 -0.01904253 -0.04200366
 -0.02209842 -0.04858941 -0.03405173 -0.02314615  0.06170155 -0.05632619
 -0.02555979 -0.06132072 -0.06719807  0.01694883  0.04699808  0.01339145
 -0.04995044 -0.03247067 -0.02599309 -0.04999271  0.01827208  0.03438299
  0.08900356  0.08463433 -0.01642071  0.07499455 -0.0877971  -0.03615764
 -0.01926419 -0.01193177  0.0251885   0.01973098 -0.05329545 -0.01657095
 -0.01404124  0.0693052  -0.06730592  0.06928711  0.08127713 -0.00403451
 -0.05042405  0.07244945  0.06625438 -0.05238158  0.02814049  0.10214484
  0.04745013 -0.00298545 -0.03047279  0.00326534  0.00688034 -0.04652932
 -0.01913846  0.0330162   0.02306194 -0.05172998 -0.04500753  0.06536516
  0.07176943 -0.08311009  0.0813556  -0.07790138 -0.05906057  0.04486033
 -0.03126477  0.06304194 -0.0009838  -0.02111036  0.04800477  0.06145691
 -0.06280958  0.01129229  0.05323742 -0.03779125  0.02672748  0.08433247
  0.01018215  0.03080271  0.01295763  0.02236171  0.00463134 -0.00779859
  0.0136662   0.07500459  0.05596635  0.08804268 -0.0141636  -0.06446309
  0.0551336  -0.07726006 -0.013466    0.02391401  0.05671452  0.09698914
 -0.04496611  0.0669326  -0.03238292 -0.06599828 -0.05165355  0.02251275
  0.05979709 -0.07320585  0.03348083  0.08573025 -0.00801167  0.05076092
  0.08623841 -0.04331813]

transformer_blocks.5.feed_forward.2.weight — shape: torch.Size([128, 128])
[[ 0.0986313   0.01689554 -0.04105539 ... -0.04269538  0.07528932
   0.06052331]
 [ 0.03731039  0.08943849  0.0073996  ... -0.07603587  0.071074
  -0.06541077]
 [-0.04142535 -0.07552617 -0.01222375 ...  0.09460292  0.01916573
   0.04688028]
 ...
 [-0.02791882 -0.05596673  0.05113697 ... -0.00071367 -0.05036827
   0.09020231]
 [-0.09386319 -0.06697576  0.03845159 ... -0.08028669  0.09158029
  -0.01315575]
 [-0.01379682 -0.00160334  0.04048863 ... -0.06038823  0.01589064
  -0.09499384]]

transformer_blocks.5.feed_forward.2.bias — shape: torch.Size([128])
[ 0.00412316  0.02254532 -0.08354688  0.00511944 -0.02601409  0.07607043
 -0.02226705 -0.02513997  0.02988918 -0.07100607 -0.0878568   0.03253643
 -0.03394992  0.0808871  -0.06337866  0.05744676 -0.03526627 -0.00770387
 -0.0691779  -0.06366351  0.05924624  0.04974568 -0.02700844 -0.00785381
  0.02077983  0.04916687  0.06892658  0.03983655  0.05588849  0.03534542
 -0.03839037 -0.02942889  0.03012285 -0.05181441 -0.00498284 -0.07405446
  0.03417093  0.02092196 -0.05120388  0.06594538 -0.02303617 -0.03994428
  0.02547098 -0.03292065 -0.00414547 -0.09507369  0.0189647   0.07106537
 -0.00906659 -0.06943338 -0.00427015  0.07479368  0.06098222  0.0541615
  0.02825554 -0.00854372  0.0647519  -0.02289302  0.00961702 -0.03134958
  0.01647807 -0.01660739 -0.01896073 -0.06609151 -0.04205282  0.06115878
  0.08736087  0.04515707  0.07037704  0.02803083 -0.01248957 -0.07046735
  0.04781169 -0.01498814  0.04681195 -0.07615691  0.05507796  0.00211698
  0.07791425  0.04699313 -0.05831879 -0.01911366  0.00699558  0.00613936
 -0.07195359 -0.00879913 -0.01897872 -0.00494534 -0.06385804  0.00131135
  0.08510323 -0.07187156 -0.03169739 -0.08775116  0.08249056 -0.03971584
 -0.01884322  0.02079476  0.00787053 -0.01222618  0.00522869  0.09205378
 -0.08744093  0.00612039 -0.07023941 -0.04618939  0.01101894  0.06469855
  0.00979121  0.03204869 -0.01998255 -0.04731664 -0.02490478 -0.05794298
  0.0730349   0.01183012  0.07086182 -0.01940849 -0.04867867 -0.05403261
 -0.03979006  0.03410055  0.07371467 -0.04525275 -0.04850421  0.0131496
 -0.06310905 -0.08151792]

transformer_blocks.6.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.08484244  0.0204384  -0.02150409 ... -0.04557963 -0.00460303
  -0.04391313]
 [-0.06437264  0.08277415 -0.01574888 ...  0.05013712  0.02299622
  -0.06971959]
 [ 0.00948394  0.02348101 -0.09036978 ...  0.06051826  0.04933321
  -0.01623268]
 ...
 [ 0.00963702 -0.06261638  0.01915238 ...  0.09940375 -0.08401159
  -0.02800164]
 [-0.02027606  0.04471363  0.04030658 ...  0.06248062 -0.02923626
  -0.08690918]
 [-0.03160486 -0.03794695  0.07047282 ...  0.07633469 -0.09113877
  -0.04350771]]

transformer_blocks.6.attention.query_key_value.bias — shape: torch.Size([384])
[-8.54650438e-02  2.45056339e-02 -5.61894178e-02  6.05579168e-02
  3.53846550e-02 -8.92353989e-03 -6.81065023e-02 -7.94116706e-02
 -6.15262575e-02 -2.35884935e-02  8.34649950e-02 -3.46795423e-03
 -2.89048422e-02  5.07533662e-02  3.75146009e-02  2.86854263e-02
  3.24788988e-02  4.78134677e-02 -4.78194170e-02  7.80877024e-02
  6.59636781e-02 -2.68869307e-02  3.21815051e-02  4.51544970e-02
 -5.54558784e-02 -3.56481620e-03 -3.60400490e-02 -6.48607537e-02
 -2.01495327e-02 -1.31350197e-03  9.47717018e-03 -5.54627962e-02
 -5.25789559e-02  7.50987008e-02  4.61628176e-02 -7.03366101e-02
 -5.42889014e-02 -2.83925589e-02 -1.36211375e-02 -7.97505826e-02
  6.06062971e-02 -4.17146794e-02 -5.25487214e-02  4.15748432e-02
 -1.22332363e-03  6.72363862e-02 -7.37699261e-03 -9.82097983e-02
  9.97495204e-02  7.64121041e-02  6.48649456e-03 -7.96351507e-02
  7.28042796e-02  9.90457013e-02 -1.72361266e-02  5.53532504e-02
 -2.30513643e-02 -2.63305046e-02  1.31031564e-02  9.00075957e-02
  3.04821301e-02 -2.86418293e-02 -3.04088984e-02 -6.71632513e-02
  6.68604076e-02 -1.88677236e-02 -3.46800499e-02 -2.27921037e-03
  2.30166130e-02 -5.42513058e-02  8.34208541e-03  8.47529694e-02
 -4.58014347e-02 -2.88341772e-02 -4.39994503e-03  5.47177084e-02
 -4.45110053e-02 -3.76266963e-03 -4.66940403e-02 -6.24547042e-02
 -7.07646534e-02 -2.32340172e-02  8.38807672e-02  2.55060103e-02
 -3.29249986e-02 -7.36620873e-02  6.55960813e-02 -1.05508119e-02
  8.53515323e-03 -4.61843498e-02 -3.97815043e-03 -3.46533470e-02
  7.41371065e-02 -7.08717993e-03 -2.39991378e-02  4.83768843e-02
  1.16004683e-02  3.02965678e-02  3.56232822e-02  4.68311645e-02
 -8.05085003e-02 -7.99932703e-03  7.21352696e-02 -8.05682391e-02
 -3.21063325e-02  5.37948646e-02 -2.94713620e-02  7.01155812e-02
 -4.20098491e-02 -1.84584707e-02 -5.90799972e-02  5.73259145e-02
  5.73981181e-02  1.61270034e-02 -5.77860959e-02  5.82092740e-02
  7.09331632e-02 -1.10300675e-01  3.82855572e-02 -6.91323280e-02
 -7.47340992e-02  7.88601115e-02  3.30948941e-02  7.01156482e-02
  6.88131154e-02  5.46942912e-02  5.29445447e-02 -4.24587466e-02
  3.92783545e-02  2.15306878e-02 -3.36888037e-03  3.93433608e-02
  2.88761477e-03 -2.56356187e-02  5.21698967e-02  2.36699925e-05
  7.80869871e-02 -6.00731075e-02 -4.75717857e-02 -1.15288328e-02
 -8.45987699e-04 -1.22122345e-02  2.88199615e-02 -4.71108481e-02
 -1.79124475e-02  2.47758068e-02 -7.07458332e-02  5.67411408e-02
  4.41479869e-02 -7.23766088e-02 -3.84932272e-02  5.76904556e-03
 -1.62152648e-02 -8.06077272e-02  2.71828622e-02 -6.76861554e-02
  7.35448226e-02  5.29253893e-02 -2.86892857e-02 -7.55669549e-02
 -1.99921150e-02  6.84077572e-03  7.23111331e-02  5.46274520e-02
 -2.54856106e-02 -4.56169434e-02 -8.68349522e-02 -4.78727482e-02
  8.20613652e-03 -1.68244019e-02  3.63736860e-02  6.51183203e-02
  6.06722161e-02 -3.36354822e-02  8.16146508e-02  7.22698346e-02
  3.08274385e-02 -4.95116636e-02  6.86526373e-02  3.59977856e-02
  3.72785665e-02 -7.82552455e-03  2.90765762e-02 -7.83015341e-02
 -1.63649209e-02 -5.91005124e-02  6.00915067e-02 -7.90281780e-03
 -3.11913025e-02 -1.77993029e-02 -2.84895804e-02  3.07833962e-03
  1.37233073e-02 -8.47085640e-02  5.00599816e-02 -4.06734683e-02
  4.92379293e-02 -8.56626257e-02 -9.43955779e-03 -5.46998978e-02
  6.80912808e-02 -8.51440653e-02 -2.89704967e-02 -8.37666914e-02
  2.53685359e-02  4.63960692e-02  2.91928854e-02 -6.44612834e-02
  7.03752565e-04 -4.25144471e-02 -4.18101959e-02 -6.54283017e-02
  7.15828389e-02 -4.18452471e-02 -5.67652844e-02 -8.13993812e-02
  2.22068187e-02  4.18222398e-02 -4.57741022e-02 -8.57785158e-03
 -3.21130157e-02  2.13065352e-02  2.59690341e-02  7.30356053e-02
  3.94594893e-02  3.49844508e-02  3.11907567e-02  4.50421823e-03
  1.29914042e-04  2.10576244e-02 -3.19235064e-02  8.61111656e-02
  1.79702919e-02 -4.52869609e-02  1.35228632e-03 -4.41004857e-02
  8.49567205e-02  2.23224461e-02  8.24320465e-02 -6.03626929e-02
 -1.84413549e-02 -1.32713765e-02 -1.59426350e-02 -3.20063382e-02
  7.68961385e-03  8.81426334e-02 -1.33034224e-02  6.41288683e-02
 -4.92489897e-02 -1.03810392e-02 -2.71478686e-02  2.41636094e-02
  4.39934917e-02  8.76978412e-02  1.77582353e-03  3.18246707e-02
 -6.55481219e-02  3.14762145e-02 -8.27898271e-03  8.10518339e-02
 -5.01524955e-02  3.24050337e-02  7.98245817e-02  9.46742222e-02
  7.62323439e-02 -4.90628481e-02  2.23864634e-02  2.59465016e-02
  1.00240475e-02 -3.94741452e-04 -4.02636454e-02 -7.11120889e-02
 -6.51783422e-02  5.87012582e-02 -7.17338249e-02  8.08210298e-02
  2.28935555e-02 -6.35430515e-02 -1.04645090e-02 -6.25470793e-03
  7.35549480e-02  1.70501955e-02  5.77054173e-02  6.97069690e-02
  3.79135907e-02 -3.95172648e-02  1.14801945e-02  4.83741798e-02
 -1.47968018e-02 -3.37913744e-02 -4.01961766e-02  7.71586224e-02
 -5.42736687e-02  3.94504443e-02 -7.12218285e-02  8.31432194e-02
  5.59158809e-02  6.75196201e-02 -8.77328739e-02  3.72412056e-02
 -4.16814908e-02  9.18564498e-02  1.61506161e-02 -1.64556061e-03
  1.63009968e-02 -2.48240046e-02 -4.73328978e-02 -4.35273424e-02
 -2.61088018e-03 -7.29665756e-02  3.68347764e-02  4.55057509e-02
 -9.74476337e-04 -5.32786995e-02  6.73557818e-03  3.86873307e-03
 -9.46540199e-03  4.30999324e-02 -1.79463755e-02 -8.52389038e-02
  4.82469276e-02  6.84888214e-02  4.00635302e-02  7.82049298e-02
 -5.76171018e-02 -4.22744714e-02  2.46645771e-02  4.94408887e-03
 -9.60383378e-03 -4.01238836e-02 -5.77854514e-02  3.86998267e-03
 -1.01042110e-02 -2.03630123e-02 -4.60862443e-02 -8.21559057e-02
  8.32874030e-02 -8.39291811e-02 -2.57548932e-02  2.20814124e-02
 -4.14149985e-02 -6.93654716e-02 -2.78617926e-02  2.68903878e-02
  3.68570574e-02  3.73025262e-03  4.93636876e-02  6.15613908e-02
  3.51602621e-02 -7.94928223e-02 -1.10550979e-02 -6.12129979e-02
 -2.44818889e-02 -6.44128323e-02  7.68338293e-02 -2.51555759e-02
  6.21615350e-02 -4.25728671e-02 -8.35117847e-02  4.20703702e-02
  6.50986210e-02  5.24342097e-02 -4.67973948e-02  1.04874577e-02
  4.04077210e-02 -7.90854618e-02  5.78319840e-02 -3.21853906e-02
  6.79318234e-02  8.83595571e-02  6.65143579e-02  3.22067998e-02
  4.81502190e-02 -1.79755781e-02 -7.14682490e-02 -1.05187362e-02
  5.87655157e-02  5.66755235e-02 -2.48112762e-03  2.63585486e-02
 -2.86529716e-02  3.77502739e-02 -2.15582643e-02 -7.04206377e-02]

transformer_blocks.6.attention.output_projection.weight — shape: torch.Size([128, 128])
[[ 0.00386987  0.00933169 -0.01346872 ...  0.04062519 -0.04525231
   0.05869821]
 [-0.04778892  0.03138386 -0.01714491 ...  0.0441079  -0.06016086
  -0.0788019 ]
 [-0.08053509 -0.11086038 -0.01133319 ...  0.04369321  0.00234919
   0.06251194]
 ...
 [-0.05280641 -0.0031059   0.05241659 ... -0.03606725 -0.04864005
  -0.07507248]
 [-0.07239052  0.0443884   0.03027448 ...  0.04528534  0.05721877
  -0.04046896]
 [ 0.02467555  0.06332656 -0.05283798 ... -0.08320423 -0.03395425
   0.0383194 ]]

transformer_blocks.6.attention.output_projection.bias — shape: torch.Size([128])
[ 0.04274333  0.02754207  0.00825766 -0.06246516 -0.04596596  0.02509686
 -0.02049713 -0.05069366  0.0238868   0.07941459 -0.04004694 -0.00343858
  0.02858801  0.00141314  0.02026582  0.01648644 -0.00422568 -0.00326483
 -0.02241793  0.04947108 -0.06561715 -0.07091793  0.06815352 -0.04375502
  0.02022989  0.06829031  0.08850575 -0.02201424  0.07578859  0.02103119
 -0.05589706  0.06711986  0.03098219  0.03235809 -0.06298678 -0.07416865
 -0.05132822  0.02054507 -0.0808625  -0.00370729  0.06782712  0.029564
 -0.0768585   0.0893811   0.04342308  0.03063142  0.07898021 -0.07933144
 -0.04442373  0.08032204  0.08430416  0.02072871 -0.02246493  0.03124051
 -0.03452436  0.02581939  0.07784475 -0.02238516  0.01913011 -0.00657894
  0.04004572  0.05055938 -0.07853679  0.04142244 -0.02034225  0.08091039
 -0.03353291 -0.04064092 -0.08520231  0.04708497  0.09487202 -0.02062267
 -0.05844956 -0.00858316  0.04398375  0.09407845  0.04426588  0.01530083
  0.03301683 -0.08394315 -0.01353445 -0.04330095 -0.01792904  0.00356183
 -0.08222305 -0.01738475 -0.07921469  0.06237614  0.02738234 -0.02258033
  0.01772388  0.04625685  0.0319187   0.07030357  0.05253988 -0.00133437
  0.03163523  0.00415744  0.03500161 -0.04337139  0.07970706  0.06170736
 -0.03792505 -0.01182588 -0.06761305  0.03196253  0.05961865 -0.05715084
 -0.05807051 -0.05248607 -0.03069962  0.00096494 -0.07600008 -0.00881321
  0.01034691  0.0313665  -0.07969444  0.0539009  -0.08743215  0.03836971
 -0.07004325  0.0464295  -0.07702712 -0.04686845  0.05506823 -0.07894734
  0.01772358 -0.07500885]

transformer_blocks.6.attention_layer_norm.weight — shape: torch.Size([128])
[1.037605   1.0192195  1.0290992  1.0435096  1.0262096  1.0269322
 1.0355247  1.0403281  1.0256469  1.0224781  1.027024   1.0169777
 1.0508463  1.03343    1.0177102  1.0317037  1.015835   1.0307211
 1.0375077  1.035623   1.0345126  1.030973   1.0095606  1.0288855
 1.0133834  1.0221035  1.0350944  1.030986   1.0277056  1.0028089
 0.997909   1.0221125  1.0278395  1.0226616  1.010755   1.0315801
 1.0158093  1.0153166  1.0239553  1.0268056  1.034512   1.0006634
 1.032241   0.99260706 1.0197088  1.0303158  1.0094405  1.0271022
 1.0214788  1.0395882  1.0160247  1.0298078  1.0060073  1.0371909
 1.0286543  1.0244657  1.0200235  1.0317935  1.0273148  1.0065659
 1.0309515  1.0317287  1.016645   1.0269649  1.0370034  1.0237707
 1.0339378  1.0144832  1.0290855  1.0228202  1.0328745  1.0136756
 1.0223429  1.0210184  1.0251346  1.0294119  1.0284249  1.0168877
 1.0324243  1.0336727  1.0144738  1.0366876  1.026804   1.0260984
 1.0159754  1.0344019  1.0177364  1.0165992  1.0025984  1.0305119
 1.0282352  1.0294056  1.0238134  1.0316815  1.0314839  1.0239105
 1.0251164  1.0319551  1.025632   1.0201877  1.0131046  1.022094
 1.0284225  1.0383097  1.0364637  1.0135866  1.0314234  1.0356307
 1.0144984  1.030443   1.0163411  1.0359756  1.0356157  1.0162497
 1.0339978  1.0289166  1.0228292  1.0142912  1.0197009  1.0220207
 1.0334324  1.02566    1.0181975  1.0353686  1.0327784  1.0349631
 1.0174054  1.0169823 ]

transformer_blocks.6.attention_layer_norm.bias — shape: torch.Size([128])
[ 3.10637592e-03  8.04058928e-03 -4.83678794e-03 -6.80346682e-04
  3.49609996e-03  2.55102711e-03 -6.70482463e-04 -2.99282838e-03
 -4.72191209e-03  6.33237371e-03 -4.83972905e-03 -1.34728877e-02
 -1.07316265e-03  3.21328477e-03  3.67118837e-03  1.06382705e-02
  5.98321902e-04 -6.20902283e-03 -6.16460806e-03 -8.63165129e-03
  1.63387172e-02 -1.13919296e-03 -1.59093575e-03  1.42028171e-03
 -1.06390333e-03 -6.46358868e-03 -3.33583564e-03 -5.42053767e-03
  5.36697404e-03  1.17178643e-02 -6.05088705e-03 -1.70472835e-03
  8.23644176e-03  2.33956333e-03  3.56199616e-03  4.41390183e-03
 -5.06365148e-04 -5.43594314e-03  2.72793183e-03 -2.52129068e-03
  5.39748278e-03  1.50778287e-04  6.36691041e-03  5.71659952e-03
  1.76519563e-03  1.45555823e-03 -1.08552352e-03 -1.28478673e-03
 -1.01764016e-02  2.44684052e-03  4.94424393e-03 -1.07807275e-02
 -4.34431597e-04  5.20482985e-03 -3.05444957e-03  2.78517930e-03
  1.02963950e-02 -5.66266896e-03 -4.90748277e-03 -6.30247523e-04
  1.00106541e-02  4.29145899e-03 -4.14045202e-03  1.29449423e-02
  9.55915079e-03 -8.11092276e-03  4.36581066e-03  3.88669694e-04
  9.55871679e-03  4.03418562e-05 -9.42230516e-04 -1.64465397e-03
  2.58519989e-03 -2.93232733e-03  3.88549617e-03  7.13838078e-03
  5.06037613e-03  9.77046043e-03 -8.63324478e-03  2.34217895e-03
  1.06622158e-02 -9.38152708e-03  1.67546328e-03  1.33095747e-02
  3.32931383e-03 -1.69393588e-02 -2.43779592e-04  1.07261054e-04
 -4.92173526e-03 -3.06250039e-03  6.21505314e-04 -7.63166638e-04
 -7.78886350e-03  5.82367927e-03  2.39504385e-03  1.72559284e-02
  1.44957262e-03 -3.12705780e-03 -6.78776763e-03 -4.56057198e-04
 -3.15195229e-03 -6.86445786e-03 -7.48276617e-03 -5.13913110e-03
  3.95647250e-03  7.63841846e-04  2.16261018e-03  2.26308941e-03
  4.78058879e-04 -2.51565641e-03  3.11717507e-03  3.28769744e-03
 -3.16797942e-03 -1.43338495e-03 -8.19013570e-04  1.46556215e-03
  7.10363593e-03  3.43673211e-03 -2.38250522e-03  1.94916327e-04
  8.42907839e-03  6.95423689e-03  1.79557188e-03  1.84900025e-04
  4.04327875e-03  1.22658082e-03  4.38353512e-03  9.09012090e-03]

transformer_blocks.6.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0349672  1.0240941  1.0271417  1.0389826  1.027062   1.040827
 1.0358021  1.0341457  1.0260664  1.0273426  1.0152514  1.034364
 1.0311304  1.028708   1.0278256  1.0351841  1.0326275  1.0379844
 1.0434275  1.0334921  1.0288671  1.0395807  1.026815   1.0293797
 1.0245342  1.0296284  1.0279595  1.010331   1.0363115  1.0082052
 1.0338968  1.0312791  1.0273857  1.025256   1.0284793  1.0156872
 1.0342724  1.0148547  1.0319877  1.0337849  1.031532   1.0322529
 1.0229211  1.0264989  1.0236319  1.0340513  1.0295665  1.0276601
 1.029161   1.0344582  0.99404716 1.0345459  1.0292608  1.005652
 1.0276128  1.0335401  1.0127853  1.0304623  1.0351774  1.0309126
 1.0323335  1.0239819  1.0208472  1.0293094  1.028367   1.0317969
 1.0302234  1.0324862  1.0353554  1.0260246  1.0322601  1.0318383
 1.0292231  1.034135   1.0062348  1.0320601  1.0277346  1.0256096
 1.0329561  1.0390096  1.0324818  1.0353688  1.030534   1.0262907
 1.0300794  1.0225207  1.0317477  1.0316545  1.0328196  1.0101012
 1.029351   1.0280815  1.0342132  1.0235658  1.0284892  1.0298619
 1.0142119  1.0273042  1.0214406  1.0272484  1.0400417  1.0375533
 1.0345738  1.0329026  1.032646   1.0224394  1.023999   1.0298737
 1.0263486  1.0093584  1.0313764  1.0272365  1.0272633  1.0332872
 1.0055001  1.0256501  1.0175256  1.0277051  1.0195762  1.0315944
 1.001439   1.0303546  1.0248618  1.0331259  1.0325143  1.0269243
 1.025509   1.0450433 ]

transformer_blocks.6.feed_forward_layer_norm.bias — shape: torch.Size([128])
[ 1.74019057e-02 -1.07293511e-02 -1.09985964e-02  1.29566910e-02
 -4.40640142e-03  1.31302448e-02  1.34322736e-02 -1.34306792e-02
  1.00960992e-02  2.61840201e-03 -1.25073623e-02  2.67197266e-02
 -1.66597404e-02  1.08216992e-02  2.34044604e-02 -7.16194231e-03
  1.15868822e-02  1.00240679e-02  9.85279959e-03  4.22475440e-03
 -1.10184802e-02  1.24870334e-03 -1.96246486e-02 -7.61287846e-03
 -2.01213341e-02  2.49517467e-02  1.46261817e-02  1.69237237e-02
  1.17208939e-02 -7.43950112e-03  8.83092918e-03  3.51338786e-05
 -1.65355708e-02 -1.30354213e-02  1.68198776e-02 -1.18143940e-02
  1.45562757e-02  2.07812283e-02  1.43038398e-02 -1.84469279e-02
  1.92305744e-02 -7.43760401e-03 -7.91823491e-03 -3.15823057e-03
  8.70580226e-03  4.12766170e-03  1.03374207e-02  5.26641915e-03
  1.45356078e-02 -1.86617691e-02  6.40614145e-03  2.55882964e-02
 -1.46752372e-02  2.29452793e-02 -1.75436363e-02 -1.69295855e-02
  2.85486523e-02  5.07260812e-03 -1.37671968e-02  1.65480357e-02
 -1.95786566e-03  1.10770697e-02  3.99560761e-03  2.05117352e-02
 -9.22958273e-03 -1.12039316e-02  7.39853922e-03 -7.34558702e-03
  1.21668335e-02 -2.49135923e-02  3.06285825e-02  1.07643763e-02
 -7.63171585e-03 -2.75621936e-02 -1.69712491e-02 -1.20991617e-02
  1.18003068e-02 -9.65484139e-03  9.98763600e-04 -5.29670739e-04
  1.62837058e-02  8.43574014e-03 -5.96962927e-04 -1.22623248e-02
  1.06684407e-02 -1.70086995e-02 -2.02423874e-02 -1.20604122e-02
  1.51930833e-02 -1.44352214e-02  1.87679213e-02  1.39092756e-02
 -2.38657761e-02  1.51281967e-03  9.51825362e-03 -1.30108753e-02
 -2.44606137e-02 -1.86150800e-02 -1.66014768e-02 -1.61926840e-02
 -2.13593561e-02  2.12317053e-02 -9.93211567e-03  1.93613768e-02
 -1.98715273e-02 -6.63140137e-03  1.49020916e-02  2.19662096e-02
 -1.68915074e-02 -3.46705987e-04 -9.29490291e-03 -1.55924186e-02
  1.41140847e-02  3.38499877e-03  1.40469742e-03  2.22195908e-02
  9.40603018e-03 -2.33015474e-02 -1.96331330e-02  8.40329682e-04
 -2.01879703e-02  1.33408997e-02 -4.15600324e-03 -8.05015955e-03
  1.12066073e-02  1.82165522e-02  2.56188009e-02  2.03106496e-02]

transformer_blocks.6.feed_forward.0.weight — shape: torch.Size([128, 128])
[[-0.08303493 -0.08222405  0.01266263 ... -0.03788807  0.06283697
  -0.06755576]
 [-0.06397742 -0.02164335 -0.04180641 ... -0.04817221  0.02987244
   0.055762  ]
 [-0.00310838  0.05786882  0.06139668 ... -0.02312895 -0.03594984
  -0.0599975 ]
 ...
 [ 0.06869038 -0.03646989  0.11412858 ...  0.01023213  0.06697498
  -0.05007789]
 [ 0.0170054   0.00334023 -0.00354734 ...  0.06676129  0.06071227
   0.02737336]
 [-0.03656771 -0.0694082   0.09998544 ...  0.06383739 -0.03765084
   0.00347016]]

transformer_blocks.6.feed_forward.0.bias — shape: torch.Size([128])
[ 0.01080079 -0.060045    0.10550749 -0.04178414 -0.06816008  0.04434811
 -0.08522646 -0.03575009  0.00761573  0.07109271 -0.00861701 -0.01501967
 -0.05151398 -0.02577494  0.08334954  0.06851012 -0.06513792  0.073477
 -0.04518056  0.04092178 -0.01650348  0.05892095  0.07258193  0.00475489
 -0.03557659  0.02632877  0.08224168 -0.02096046  0.0209085   0.05090175
 -0.03173468 -0.06545079 -0.01230666 -0.0119221   0.07987047  0.04896792
  0.01681108  0.0319709   0.02275191 -0.03556823 -0.02408226  0.09352956
  0.09526697 -0.06807301  0.06753235 -0.09265438  0.09449459 -0.07468791
 -0.04904712  0.08772962 -0.06311189 -0.00481943  0.04813278 -0.01283697
 -0.03446877 -0.06840997 -0.00969596  0.04015839  0.05260886 -0.04183001
 -0.00216129  0.00851624  0.09777134  0.02274549  0.0178262   0.01435475
  0.0383445   0.00808211 -0.03411807  0.08511346  0.09023291 -0.06287617
  0.10843588 -0.02140996  0.08991232 -0.04750922 -0.06336367  0.09179903
  0.07719412  0.0523529   0.09331509 -0.03159206  0.02205012  0.00571055
 -0.00881636 -0.04742033 -0.01595826 -0.02584784  0.10609563 -0.03878667
  0.04402451 -0.00513532 -0.00433702 -0.06634212 -0.00381769 -0.02912238
  0.03400622 -0.02535533  0.02081337 -0.06827122  0.09383225  0.06765436
  0.0809582  -0.07248898 -0.02646484  0.0142935  -0.05607665 -0.0584005
  0.01361435  0.06414703 -0.00388388  0.00167222  0.00425435  0.09236223
  0.02209943  0.07312267  0.09394347  0.05789706 -0.04898734  0.02965181
  0.00659868 -0.02790136  0.02115154  0.03316481 -0.03211533 -0.03454994
 -0.01002291  0.07338488]

transformer_blocks.6.feed_forward.2.weight — shape: torch.Size([128, 128])
[[ 0.03216662 -0.01071995 -0.04562597 ... -0.04336771 -0.04119779
  -0.01822265]
 [-0.0650559   0.03007607 -0.09958471 ...  0.00596184 -0.0041125
   0.07345434]
 [ 0.06051715 -0.0504347  -0.06616046 ... -0.01298615  0.01582991
  -0.0691896 ]
 ...
 [ 0.02995452  0.01159092  0.08861676 ... -0.04328777 -0.11094772
   0.05843065]
 [ 0.11535915 -0.04843916  0.04331183 ...  0.03818523  0.07180347
   0.06152391]
 [-0.02160982  0.093812   -0.07439878 ... -0.02731886 -0.0848026
   0.00706284]]

transformer_blocks.6.feed_forward.2.bias — shape: torch.Size([128])
[ 1.9577099e-02  5.1068548e-02  1.5098915e-02  3.7622981e-02
 -3.8044889e-02  4.5467161e-02 -3.4967255e-02 -2.8755195e-02
 -7.4316815e-02 -4.2272747e-02  3.0639848e-02  6.4853728e-02
 -5.0575074e-02  6.5667957e-02  5.6867041e-02 -8.6638421e-02
  5.7519125e-03 -1.5312227e-02  2.4012448e-03 -3.7799098e-02
  5.4112125e-02 -3.2653429e-03  1.1025228e-02 -2.7760852e-02
 -1.8335210e-02  1.1396007e-02  7.4388199e-02  4.0641531e-02
 -7.5689100e-02 -5.2817818e-02  8.1988603e-02 -4.9695797e-02
  5.7527650e-02 -4.1751955e-02 -7.7700153e-02 -8.3578089e-03
  8.3995741e-03 -2.3020210e-02 -4.1271795e-02 -7.3524311e-02
  2.3878723e-02 -1.7673511e-02 -4.5436502e-02 -2.3942843e-02
 -3.2410792e-03  2.2861799e-02  4.4680438e-03 -6.9807254e-02
 -2.1001717e-02  4.7017109e-02  5.5559095e-02 -4.8253043e-03
 -9.0535671e-02  6.0572825e-02 -8.7352313e-02  9.6607700e-02
  6.8555541e-02 -5.3998914e-02  4.1674294e-02  4.5362417e-02
 -4.0903527e-02 -3.8532589e-02  8.8791050e-02  6.7542039e-02
  3.8359225e-02 -7.4265823e-02  5.9912917e-03 -6.8984374e-02
 -3.9508089e-02 -3.8571026e-02 -5.7839971e-02  5.9195314e-02
 -8.3522440e-04  6.9017507e-02 -1.5882298e-02  2.1902004e-02
 -5.4615382e-02  7.1198955e-02 -2.8951367e-02  7.6657444e-02
 -3.9399371e-02 -4.0765606e-02  5.4894671e-02  4.5189358e-02
  8.1734680e-02 -7.5690940e-02  7.1063854e-02 -4.5952160e-02
 -4.5027889e-02  8.2637191e-02 -1.1841248e-02  6.4899214e-02
  7.8333847e-02  5.4305874e-02 -8.8313678e-03 -6.2351465e-02
  1.2801766e-02  8.2586080e-02  6.9511253e-03 -3.5482369e-02
 -3.4752331e-04 -6.5641627e-02  3.5498038e-02  3.4526769e-02
 -4.6700463e-02  4.1462861e-02  8.0625102e-02 -3.6132220e-02
  6.4269319e-02  6.4328678e-02  7.7296456e-05 -9.3126655e-02
  5.8740377e-02  4.4559121e-02  7.5243791e-03 -5.4814242e-02
 -6.4160131e-02  5.6662090e-02 -5.5374827e-02  2.5422670e-02
  3.0344417e-02  1.6795456e-02  4.5126703e-02 -1.0165869e-02
 -7.2852291e-02  3.4713697e-02  4.2496677e-02 -7.1968935e-02]

transformer_blocks.7.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.00384269  0.09515916  0.07167035 ...  0.09127317 -0.00795893
  -0.09261268]
 [ 0.12644392  0.01759878 -0.01027233 ... -0.03506027 -0.01853492
  -0.0071929 ]
 [ 0.04235435 -0.00939495  0.07287363 ...  0.08390075 -0.02794114
  -0.01274375]
 ...
 [-0.04511225 -0.04672326  0.06452916 ... -0.06060353  0.01984409
  -0.04942459]
 [ 0.05037649 -0.02894079  0.00630159 ... -0.01081121 -0.06985423
  -0.01202097]
 [ 0.05826239 -0.03948369  0.06079319 ... -0.00177313 -0.03146692
   0.0517141 ]]

transformer_blocks.7.attention.query_key_value.bias — shape: torch.Size([384])
[-6.30708560e-02  4.70125638e-02  6.39908910e-02 -8.04374814e-02
 -1.83260068e-02  9.07940412e-05  8.22585449e-02 -5.07482551e-02
 -7.19998702e-02 -3.55622396e-02  6.21973984e-02 -1.34779103e-02
  1.10333096e-02  6.54439926e-02 -8.37604180e-02 -8.25943500e-02
  1.27081051e-02 -7.84862787e-02  6.51631281e-02 -1.02114733e-02
  7.67859221e-02  8.42913464e-02 -1.41996089e-02  6.02546446e-02
  4.19573225e-02 -4.70666289e-02 -5.34753948e-02 -6.32781982e-02
  8.68486017e-02 -5.64185344e-02 -6.32416978e-02 -6.62139580e-02
  7.73232877e-02  3.93149741e-02  7.24277124e-02 -6.04757369e-02
  6.29397333e-02  1.55900151e-03  6.48600573e-04 -5.82093261e-02
  9.62401368e-03  6.42024055e-02 -1.05123356e-01 -2.91956794e-02
  5.52876480e-03 -1.95185412e-02  5.94585901e-03  3.12248096e-02
 -6.39204285e-04 -5.79908257e-03  2.64236685e-02  4.25184518e-02
  6.87516332e-02 -4.74533960e-02 -9.00861248e-02  1.62329804e-03
  8.46956298e-02  3.97739001e-03 -4.54780972e-03  1.36924572e-02
 -9.19044465e-02  3.28557007e-02 -7.53404275e-02  6.97709769e-02
  8.86367410e-02 -3.26848067e-02  3.09939403e-02  8.78453776e-02
 -3.69718969e-02  7.41407797e-02 -9.86647159e-02 -6.33297414e-02
  6.07473776e-02 -9.33288410e-02 -4.25168360e-03  5.84223680e-02
  8.71445984e-02  1.91461910e-02  3.91554721e-02  4.37912680e-02
  4.96848784e-02  1.43674398e-02  9.27506539e-04  8.10317099e-02
 -6.20630048e-02  6.29727170e-02 -5.60629182e-02 -8.97990689e-02
  2.78060287e-02 -3.80428657e-02  5.84750287e-02 -1.11091509e-02
  2.18014996e-02  6.31182501e-03  7.25422874e-02  8.14005509e-02
 -9.03078169e-03 -3.26858535e-02 -4.17553633e-02  5.57375560e-03
  9.87062138e-03  6.98139295e-02  9.63987336e-02 -6.50543794e-02
 -1.47406450e-02 -9.45523940e-03  3.09858285e-03 -1.48422932e-02
  5.75773045e-02  2.85621211e-02 -2.87437011e-02 -4.63709757e-02
  9.20010358e-02 -5.02137430e-02  4.34809700e-02  1.29064685e-02
 -6.72140941e-02 -1.76475495e-02  5.04377596e-02  4.45926078e-02
  7.90796801e-02 -3.64729427e-02  2.65275943e-03 -1.19689992e-02
 -1.03667788e-02 -7.17176544e-03 -8.87182206e-02  5.80441356e-02
 -6.23449236e-02 -8.56473818e-02  5.22640757e-02  5.21637276e-02
  7.38410056e-02  3.54879387e-02  8.53464156e-02 -7.59912208e-02
  9.38834995e-03 -4.46905941e-02  4.28018309e-02  5.98488282e-03
 -2.82949433e-02 -3.64649668e-02  5.96643686e-02 -6.08253554e-02
  7.56152421e-02  6.18930906e-02  1.01986546e-02  4.59131263e-02
 -6.56421408e-02 -8.28325152e-02 -4.70763668e-02 -2.42244583e-02
 -1.98129080e-02  4.91859131e-02 -4.93473485e-02  1.90113261e-02
 -6.51302561e-02  1.45748835e-02 -1.64636988e-02 -5.11349067e-02
  1.15122730e-02  5.59563413e-02  6.03894703e-02  6.58030286e-02
 -7.52488822e-02  5.94210252e-02 -7.74827302e-02  4.45712581e-02
 -6.31990954e-02  3.32113728e-02 -6.51927665e-02  5.67372255e-02
 -6.20325431e-02 -7.84473270e-02 -1.26874633e-02  3.78481410e-02
  4.39234450e-02  8.46866816e-02 -8.43979940e-02 -7.03846067e-02
  5.11446632e-02 -2.21269038e-02 -4.37066928e-02  7.53932595e-02
  5.66022098e-02 -5.31984903e-02 -2.19051838e-02 -3.12504172e-02
 -2.86797546e-02  4.71266992e-02 -7.41073042e-02  7.53248855e-02
 -5.78013025e-02  7.18426704e-02  6.45413548e-02  5.89439226e-03
  4.33862694e-02 -6.94253668e-02  6.08109571e-02  2.16648858e-02
  3.30039524e-02  7.12455809e-02  7.13699237e-02  6.62588254e-02
 -3.47127840e-02  4.19925563e-02 -5.05836457e-02 -4.40010354e-02
  5.39436415e-02  7.55454674e-02  3.87219712e-02  6.79959953e-02
  8.01028237e-02  6.61492124e-02  1.62744873e-05 -6.77193552e-02
 -4.78072912e-02  1.87765323e-02  1.14576025e-02 -6.72105849e-02
 -1.12188691e-02 -8.58463049e-02  8.75968412e-02 -3.01015247e-02
  6.00091890e-02 -5.42990044e-02  5.75747108e-03 -2.93765981e-02
  2.17978819e-03 -2.77915411e-02 -3.94975431e-02  2.40595471e-02
  7.38387778e-02  3.36642824e-02 -3.81676815e-02  3.93615924e-02
  3.23354192e-02 -5.53072207e-02  4.10607606e-02  3.57023738e-02
 -2.78568603e-02 -7.21377954e-02  5.92490546e-02 -8.42910334e-02
  2.25357693e-02  6.75719753e-02 -4.41468135e-02  6.14534132e-03
 -8.22292194e-02 -5.65663315e-02 -6.31724373e-02 -5.45688495e-02
  4.80249971e-02 -1.90949701e-02  5.32721616e-02 -5.15333079e-02
  1.89797729e-02  7.48980567e-02 -2.07573771e-02 -8.20049495e-02
 -6.71297014e-02  5.83575368e-02 -7.63599016e-03 -3.11443936e-02
  3.43299359e-02 -3.41959000e-02 -6.28223643e-02 -5.37771881e-02
  2.62546120e-03 -7.69943669e-02  1.47691155e-02 -8.79719481e-03
  1.77882309e-03  3.38667631e-02  1.47402706e-02 -8.35823938e-02
  3.11605223e-02  6.92700297e-02  2.77609006e-02 -1.08332222e-03
  6.24368414e-02  9.07076299e-02 -6.92001507e-02  5.79076121e-03
  1.65797286e-02 -7.89689124e-02  5.83105162e-03  5.73634431e-02
  3.23980190e-02 -5.40820993e-02  6.11276180e-02  1.88084263e-02
 -1.71728581e-02  2.95659285e-02 -8.53462145e-02 -4.75463979e-02
  2.40434799e-02  5.20583466e-02  8.29898864e-02 -5.53182792e-03
 -6.90739676e-02 -7.97296241e-02 -8.13925192e-02  4.93584611e-02
 -4.81524020e-02 -2.19111275e-02 -9.05271620e-03 -5.25986217e-02
 -1.52288852e-02 -3.78767662e-02  2.67247055e-02  7.22850338e-02
  8.30114558e-02 -2.34797336e-02 -8.07145685e-02 -7.56315812e-02
  5.18854633e-02  3.84918265e-02  8.34286660e-02  5.31632863e-02
  4.02987041e-02  3.02232150e-02  9.06063244e-02 -5.13700582e-02
 -5.58207259e-02 -5.11743277e-02  3.62397395e-02 -7.13541210e-02
  8.22189599e-02  2.59486623e-02 -4.78124022e-02 -4.66234572e-02
  2.19447855e-02  6.06480278e-02 -5.68077490e-02 -5.02038077e-02
  4.08279896e-02 -4.24573347e-02 -4.99623977e-02  3.22724245e-02
 -1.21970503e-02 -3.25002819e-02 -1.28254825e-02 -5.33256531e-02
  2.87509449e-02 -5.19358888e-02 -5.04584648e-02  7.83845335e-02
  6.70889467e-02  3.72161977e-02  7.63543844e-02 -1.41623830e-02
 -7.38947466e-02  6.47890046e-02  3.79098725e-04 -7.34712705e-02
  8.16212818e-02 -2.94471532e-02  1.13092046e-02 -3.97512652e-02
 -7.92921409e-02 -8.09618309e-02 -3.96947842e-03  6.75640255e-02
  4.74317791e-03  1.12817129e-02  7.57315829e-02  4.23184931e-02
  5.57722077e-02 -5.43722138e-02 -2.24443544e-02 -2.63583399e-02
  9.05512050e-02  2.44136341e-02 -4.30263281e-02 -5.41934930e-02
 -7.91401044e-02  8.61241147e-02 -1.52182216e-02  2.19992157e-02
  7.13445246e-02 -6.43045604e-02 -4.66233417e-02  5.48473932e-02]

transformer_blocks.7.attention.output_projection.weight — shape: torch.Size([128, 128])
[[-0.09018199 -0.06392898 -0.01012031 ...  0.09845261  0.07997035
  -0.00180449]
 [ 0.08688863 -0.0181843   0.00069064 ...  0.01565567  0.03427297
   0.00932503]
 [-0.07974965  0.01107391 -0.07663239 ... -0.04648982  0.05075296
  -0.05130658]
 ...
 [ 0.03240073 -0.06827288  0.05133617 ...  0.01133275 -0.00771353
   0.0739581 ]
 [ 0.0790159  -0.01839754 -0.02571671 ... -0.024112   -0.00701452
  -0.01142013]
 [-0.05081572 -0.04645934  0.03096742 ... -0.03048914  0.04844498
   0.05385223]]

transformer_blocks.7.attention.output_projection.bias — shape: torch.Size([128])
[ 0.0189283   0.02669013  0.07542573 -0.00574533  0.05710413 -0.05031937
  0.03827658  0.08302925 -0.03265026  0.05897581 -0.03945143  0.07551798
 -0.08065961  0.04428047  0.08609469  0.0615678  -0.0856223  -0.06470329
 -0.06009709  0.07315304 -0.0951283  -0.01445191  0.01979336 -0.00932517
  0.05867954 -0.02851082  0.08115126  0.07059988  0.05056224 -0.02107172
 -0.05158684  0.03242796  0.07473677 -0.06663015 -0.07880107 -0.06569528
  0.02698561  0.02926605  0.01365562 -0.04633999  0.03718409 -0.06166215
  0.04939034 -0.05702152 -0.03794498  0.06254005  0.02051372 -0.0385685
 -0.07755414  0.04040429 -0.05701408  0.05789977  0.05235407  0.0070896
 -0.03728293  0.02685676 -0.0028127   0.01956162  0.08921238 -0.01589594
  0.02365212 -0.01732604  0.04101185  0.00661481 -0.06704171  0.00507562
 -0.02454442  0.08382073 -0.0821757   0.01323219 -0.01835953 -0.03355785
  0.03565147 -0.00711454 -0.06373955 -0.07378375  0.01257675 -0.03763572
  0.07863409 -0.05220857 -0.02865601  0.00701182  0.04267884  0.01606235
 -0.01437481 -0.06524353  0.03721947 -0.0242846   0.00295097 -0.06014952
  0.0439066   0.03723128  0.07581723  0.05933319  0.04615635  0.06584128
 -0.0506052   0.03613605  0.05294392  0.00681231 -0.07109845 -0.05875446
  0.04730427 -0.07765924 -0.02289948  0.09314632 -0.00287629  0.03457405
 -0.00761956  0.0260432   0.01882654 -0.02831734  0.00508973 -0.04493282
  0.03872555 -0.07200887  0.04213059 -0.04172924  0.0017175  -0.07983211
  0.0519398  -0.00196572 -0.00568324  0.08171029 -0.02451323  0.05903032
 -0.05346805 -0.06818659]

transformer_blocks.7.attention_layer_norm.weight — shape: torch.Size([128])
[1.036629  1.0043353 1.0330913 1.0246189 1.0275145 1.019609  1.0167032
 1.0018541 1.0486805 1.0333717 1.0372592 1.0242252 1.024671  1.033414
 1.000696  1.0137268 1.0197552 1.0320686 1.038689  1.0251731 1.0258205
 1.0396478 1.0155083 1.0284941 1.005059  1.0043309 1.0284127 1.0149151
 1.0261594 1.0059335 1.0093497 1.018977  1.0181215 1.0381778 1.0252448
 1.0174057 1.0020019 1.0236843 1.024104  1.0133536 1.0338722 1.0091811
 1.0259161 1.0301833 1.0030413 1.0127949 1.0052131 1.0265583 1.0227425
 1.0192312 1.0035709 1.0211331 1.0283185 1.0339534 1.0238168 1.0331161
 1.0221405 1.0238488 1.0287566 1.014274  1.0278734 1.0202512 1.0208867
 1.0385835 1.0217203 1.0358272 1.0291759 1.000761  1.0138196 1.0382767
 1.0353279 1.0308176 1.024754  1.0353997 1.0382198 1.034121  1.0398766
 1.0221679 1.0324029 1.0197246 1.0262942 1.0244802 1.0389581 1.012169
 1.0257869 1.0255271 1.021059  1.0256073 1.0040239 1.0192063 1.0301496
 1.0128981 1.012729  1.0244086 1.02932   1.0236069 1.025025  1.0353816
 1.0214068 1.0250126 1.00322   1.0329674 1.0197595 1.0308831 1.0187413
 1.0093193 1.017964  1.0266744 1.0205162 1.0274088 1.012727  1.0163959
 1.0157492 1.0268016 1.0139617 1.0342436 1.0167251 1.0231103 1.0156306
 1.0244938 1.0226897 1.0281259 1.0177041 1.0315938 1.0355165 1.0283618
 1.0191123 1.0152564]

transformer_blocks.7.attention_layer_norm.bias — shape: torch.Size([128])
[ 1.5769662e-02 -1.7702127e-03  4.8631201e-03 -4.3175118e-03
 -3.1658961e-04 -5.6125904e-03 -1.2334000e-03  6.9142049e-03
  1.1149492e-02  2.4189474e-03  1.6776947e-03 -6.6175251e-03
 -2.3374429e-03  4.0649693e-04 -7.1393522e-03 -4.4494169e-03
 -4.2221406e-03 -3.7737135e-03  1.5079531e-04 -4.3239947e-03
  5.6225718e-03 -6.1410456e-03  5.0621983e-03 -4.1814675e-03
  6.3704676e-03  9.6454116e-04  5.2854652e-03 -5.2563283e-03
 -4.9133496e-03  9.2772879e-03 -1.2435361e-02 -2.0050944e-03
 -4.1775228e-03  4.0102364e-03  3.1963536e-03  3.6587536e-03
  3.1219928e-03 -7.6722289e-03  2.6518118e-03  7.6375069e-04
 -6.6916780e-03  3.4704448e-03  7.0145903e-03  8.0514615e-03
  1.5555388e-04 -8.6626150e-03 -2.2341947e-04  1.5665231e-03
  2.0669177e-03 -5.5358736e-03  9.9058291e-03 -2.0999743e-02
 -4.7975648e-03 -4.7612935e-03 -7.8521273e-04 -3.1781658e-03
  5.7608420e-03  6.3068676e-03 -6.2872132e-04  3.6802168e-05
  8.8704610e-03 -5.8031292e-03  1.0343337e-03 -7.7489968e-03
  5.0222129e-03  6.5641622e-03 -3.4533457e-03  6.1659422e-03
  1.9369164e-02  1.1493662e-02 -5.5216118e-03  8.1377784e-03
  2.7379342e-03  4.1268169e-04  4.0827771e-03  4.8810951e-03
 -1.4220241e-03  1.1872732e-03 -5.4767015e-03  9.2083234e-03
 -2.5725630e-03 -4.7634058e-03 -4.4904770e-03  1.0194472e-02
  1.1159327e-02 -6.5654336e-04 -1.6845342e-03  5.9746923e-03
 -4.1389265e-03  9.0774531e-03  7.8381728e-03 -1.2367877e-02
 -7.4943332e-03  2.7615207e-04  1.3704684e-03  4.8914552e-03
  7.2182091e-03  4.3864185e-03 -1.3646260e-03  1.0166388e-03
  2.3034455e-03  3.1245709e-03 -5.1960251e-03  9.1962155e-04
  3.2293177e-03 -9.5431721e-03  2.0893109e-03  5.3167129e-03
 -2.6471706e-03 -9.5036440e-04  4.4364342e-03 -4.3156208e-03
 -9.6494937e-03 -4.1186372e-03 -7.1067992e-03 -2.8610488e-03
 -2.9110485e-03  3.8385803e-03 -4.1747233e-03 -1.0515014e-02
 -1.5453219e-04  6.1056782e-03 -1.6016454e-03  7.9720989e-03
 -3.7469051e-03 -1.8867364e-03  5.8708538e-04  3.9111236e-03]

transformer_blocks.7.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0281539  1.0383084  1.0223329  1.0308567  1.0369644  1.0160953
 1.0336521  1.0243039  1.0323741  1.0370136  1.0301061  1.032851
 1.028848   1.0329572  1.025159   1.0296358  1.0325565  1.0437512
 1.0310119  1.007608   1.0346031  1.0287136  1.0331748  1.030015
 1.0327986  1.0366721  1.0262234  1.0316839  1.0297065  1.030996
 1.0285432  1.0293416  1.0214075  1.0269787  1.0317261  1.0231211
 1.0350103  1.0329179  1.0307593  1.0333099  1.0124873  1.0277153
 1.0302365  1.0346605  1.0390885  1.0300747  1.0180287  1.0328721
 1.014372   1.0234523  1.0301012  1.0240508  1.0342526  1.029391
 1.0317116  1.0327555  1.0290374  1.0186592  1.0314921  1.0229533
 1.0303088  1.021274   1.0365484  1.0255567  0.9959881  1.0374769
 1.0308596  1.0328336  1.0255742  1.0305271  1.0337262  1.0024343
 1.0302967  1.0093977  0.99376553 1.0246406  1.0107296  1.0345418
 1.0226816  1.0369495  1.0329103  1.0389429  1.0354884  1.029771
 1.0399432  1.0375347  1.0362957  1.019351   1.0335318  1.0353196
 1.0229537  1.0158159  1.021613   1.0337249  1.0382484  1.0269917
 1.0336268  1.0203254  1.0236368  1.0295165  1.0297246  1.0233763
 1.0112778  1.0101482  1.0315548  1.0369239  1.0268773  1.0186994
 1.0318964  1.0295033  1.0320902  1.022268   1.0300852  1.0241554
 1.029987   1.025287   1.027295   1.0324681  1.0209115  1.0350628
 1.0267762  1.0323739  1.0355896  1.0327853  1.0262538  1.0328329
 1.0130008  1.0273099 ]

transformer_blocks.7.feed_forward_layer_norm.bias — shape: torch.Size([128])
[-6.64787227e-03 -1.08586047e-02 -2.10649390e-02 -2.38390863e-02
 -3.09241540e-03  8.45750887e-03 -9.30386595e-03 -1.08100462e-03
  1.04537699e-02  1.03316503e-02 -5.75750554e-03  4.48673498e-03
 -1.55901555e-02 -1.84954121e-03  2.48592012e-02 -2.70425435e-02
  5.58747398e-03  7.53211696e-03  9.21735133e-04 -7.53064314e-03
 -7.58227543e-04 -9.38078761e-03  6.91672508e-03  4.19772975e-03
 -2.47836839e-02  2.13460717e-02  8.87205731e-03 -1.67411491e-02
  2.17513461e-02 -2.07454357e-02 -2.21498143e-02 -1.38960760e-02
 -1.76218599e-02  2.76946314e-02  3.98939010e-03 -2.42494121e-02
  2.70797592e-02 -5.75668737e-03  6.00523141e-04  4.69850237e-03
 -2.24792808e-02 -4.31954628e-03 -1.53437769e-02  5.07314631e-04
  2.86882948e-02 -1.80619396e-02  2.33004633e-02 -1.95841491e-02
 -1.18801482e-02 -9.85787995e-03 -2.48082876e-02  2.18456909e-02
 -1.18194940e-02  7.43348757e-03 -1.47255631e-02 -6.89993473e-03
 -2.33622417e-02 -7.41424877e-03  2.06246432e-02  1.59381423e-02
  1.27419764e-02 -4.82767588e-03  2.38735583e-02 -9.15816147e-03
 -6.64182100e-03  1.37429936e-02  1.91665143e-02  1.60489548e-02
 -1.20748291e-02 -2.08163634e-02  8.23416933e-03  6.81819720e-03
 -3.06353858e-03 -4.60478524e-03  2.54259445e-02  1.64912350e-03
  1.06426971e-02 -1.35236345e-02  1.12869814e-02 -9.49999876e-03
 -1.41055379e-02  2.00432036e-02  7.88746169e-04  4.56283474e-03
 -2.69174943e-05 -2.14357693e-02  1.89002920e-02 -1.49046630e-02
  1.93578936e-02 -2.84336694e-03 -1.42480303e-02  2.37327069e-02
  1.66553315e-02 -6.53444370e-03  1.53990090e-02 -1.91974603e-02
 -2.38445471e-03 -1.01277968e-02  2.07110066e-02 -2.07830826e-03
  1.37772206e-02 -1.73214078e-02  1.85145549e-02 -5.47385949e-04
  6.67941570e-03 -1.35008413e-02 -2.53332928e-02  1.70401409e-02
 -1.80731509e-02 -1.55533748e-02  1.32593615e-02 -2.04451904e-02
 -7.18506239e-03  1.72798838e-02  2.33615860e-02 -1.46228224e-02
 -7.27152266e-03 -3.54148424e-03  1.46357259e-02 -1.09732905e-02
  1.58962309e-02 -1.65158156e-02 -5.78171993e-03  1.04870163e-02
 -1.34187145e-02  2.14005578e-02 -1.58023764e-03  5.29214414e-03]

transformer_blocks.7.feed_forward.0.weight — shape: torch.Size([128, 128])
[[-0.08497328 -0.080583    0.00834282 ... -0.07855459 -0.03776636
   0.04845463]
 [ 0.03110219  0.05577969 -0.02017308 ... -0.11395888 -0.03077274
   0.07660446]
 [ 0.05415417 -0.01883249 -0.02325203 ...  0.00949727  0.04919473
  -0.0702032 ]
 ...
 [-0.01667241 -0.04732463  0.00139698 ... -0.00894186 -0.02753886
   0.01316402]
 [ 0.01131819  0.00870138 -0.06095733 ...  0.05937046 -0.04094233
  -0.10322949]
 [ 0.06741278 -0.03310332 -0.05862436 ...  0.01527356 -0.00468233
   0.06374484]]

transformer_blocks.7.feed_forward.0.bias — shape: torch.Size([128])
[-0.02452569 -0.00924786  0.09605674 -0.03743688 -0.03913058  0.09084065
 -0.03502005  0.06501162 -0.05739094  0.11927707  0.0964927   0.00393901
 -0.06333373  0.05577663  0.01354014 -0.08526062  0.01267443 -0.06850905
  0.06136721  0.00583988  0.08089702  0.03044746 -0.02267609 -0.04174543
  0.03956218  0.084134   -0.04257331  0.03699852  0.07486814 -0.02209901
  0.03706244  0.09647769 -0.04230694 -0.03187277  0.02582404 -0.0191669
 -0.03452151  0.01600715  0.08039008 -0.05720021  0.07517964  0.07063133
 -0.00264623 -0.01329946 -0.05709263 -0.05298505  0.07463502 -0.04691495
  0.00068321  0.05072087  0.07625444 -0.05154822  0.05423082  0.00882131
  0.04604216  0.06315821  0.05788439 -0.02429371  0.0255932   0.02462696
  0.04452232 -0.07459208 -0.04437111 -0.00230672 -0.03930604  0.02076591
 -0.04771946  0.03276107 -0.06175748 -0.07681531 -0.05554352  0.01207681
 -0.04230702  0.07532283 -0.00791325  0.02905892  0.06314591 -0.0333478
  0.00817987  0.07271066  0.0246403   0.08252211 -0.02012992 -0.01659994
 -0.05800535  0.06997045  0.05727176  0.02909857 -0.0539133  -0.05743019
 -0.01404808  0.03482661  0.03270065  0.03243281  0.03029406 -0.09828451
  0.05349434 -0.03880836 -0.04482636  0.04777443  0.02655875  0.00299103
 -0.03958595  0.08457022 -0.05150916 -0.05305165  0.02677626  0.08102932
 -0.0331879  -0.04098264  0.04189528  0.10908703 -0.02477692  0.06053745
  0.02294609  0.02159988  0.01874514  0.0647726   0.00378502  0.04676835
 -0.02194448 -0.00528766  0.02669751  0.01649385 -0.06013571 -0.05866124
  0.08408726 -0.03304429]

transformer_blocks.7.feed_forward.2.weight — shape: torch.Size([128, 128])
[[-0.04721029 -0.0196384  -0.06182065 ...  0.05841467 -0.05426048
  -0.05936878]
 [ 0.00963732  0.04909212  0.05418931 ... -0.04251815 -0.06581624
  -0.04877335]
 [ 0.01748137  0.02860363  0.06270556 ... -0.05346164  0.00960314
   0.06160887]
 ...
 [ 0.06064265  0.1066073  -0.05293972 ...  0.03374195 -0.0596646
   0.05878342]
 [ 0.07130731 -0.0519706  -0.06034343 ... -0.02179535 -0.06467346
  -0.01749057]
 [ 0.04890256  0.00822126 -0.02067849 ...  0.09741269  0.01529886
  -0.09510535]]

transformer_blocks.7.feed_forward.2.bias — shape: torch.Size([128])
[-0.05867837 -0.00146651 -0.06751548 -0.01658519  0.08762442  0.01308656
  0.03097058 -0.00519603  0.0622785   0.042267    0.00335622 -0.00951283
 -0.0742239   0.05065503  0.09244345 -0.04114722  0.04818987  0.06880395
 -0.02855071 -0.02516091  0.00159723  0.00275887 -0.015004    0.06347392
  0.02218203  0.06765333 -0.08416972  0.05527659 -0.05119876  0.00329328
 -0.06657216  0.0572551  -0.03015151  0.00187702 -0.02583775  0.07075818
 -0.04314484  0.05864451 -0.07608844 -0.05885283 -0.06836676 -0.03417924
 -0.00489946 -0.03259314 -0.02026599  0.03873035  0.03654145 -0.07007374
  0.02009172  0.04532187  0.00019992 -0.05180517 -0.09055303  0.05938543
 -0.00391661 -0.01199514 -0.06960444  0.00086543 -0.04260601  0.0350373
 -0.03318975 -0.06383613  0.01861008 -0.05132888 -0.07056664 -0.04094292
 -0.06570856  0.08114119 -0.0764467  -0.07205387 -0.06368526 -0.04779204
  0.06380339 -0.07194804 -0.05570768 -0.00738066 -0.06068944 -0.01754495
 -0.00544149 -0.0542732  -0.0036392   0.01049839 -0.07183881 -0.08897599
 -0.01024339 -0.07203355  0.00641381  0.07470652  0.00811048  0.0652307
 -0.07502081  0.01877173  0.07130601 -0.04673484  0.0871802   0.00260914
  0.03673222  0.00466179  0.06553931 -0.01689608  0.04990822  0.02437698
  0.01273756  0.02570934 -0.01132662  0.05054891  0.01943591 -0.07515435
 -0.07007891  0.04282077  0.07909496 -0.00569715 -0.04866575  0.02192991
 -0.07715631  0.05732482 -0.05270627 -0.0070957   0.05783628  0.06084119
 -0.09611834  0.08276621 -0.02574175 -0.06562401 -0.01760983 -0.06706695
  0.04212496  0.04051268]

transformer_blocks.8.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.0424932  -0.02795301 -0.05070151 ... -0.03960583 -0.03135397
   0.04383493]
 [ 0.07843695 -0.05451722  0.01544067 ...  0.04657147 -0.09132729
  -0.02711261]
 [-0.08444074  0.09628434  0.02055349 ...  0.07107569  0.08895041
  -0.08713074]
 ...
 [-0.0421891   0.08909318  0.07799923 ...  0.0542861   0.04153034
  -0.0320742 ]
 [ 0.01778514 -0.0573196   0.05608261 ... -0.09650514 -0.06393518
  -0.08616407]
 [-0.00738002 -0.02500455  0.03161682 ... -0.0625075  -0.07743832
   0.06435562]]

transformer_blocks.8.attention.query_key_value.bias — shape: torch.Size([384])
[ 2.1481141e-02  8.1385836e-02  3.7556604e-02 -7.0199214e-02
 -9.5832914e-02 -7.9326592e-02 -3.9933965e-02 -4.1361410e-02
  3.3727512e-02  2.6738355e-02  1.1818550e-02  9.2166541e-03
  5.2267864e-02 -3.4230232e-02  1.9457683e-02 -6.8913490e-02
 -1.2105778e-02 -5.5049319e-02 -4.7873884e-02 -4.0497330e-06
  4.7520742e-02  1.5053642e-02  9.1409177e-02  5.8613218e-02
  7.3279135e-02 -6.8969965e-02 -6.1634220e-02  8.4695555e-02
 -4.5476589e-02  8.4828280e-02 -3.1384122e-02 -4.7496077e-02
  5.9508327e-03 -4.2128440e-02  5.9832297e-02 -5.5731822e-02
  8.2878835e-02  1.7550189e-02  5.0443865e-02  4.3940116e-03
  5.0510429e-03 -6.8821074e-03  3.1719349e-02 -8.9124396e-02
 -7.6256081e-02 -4.2911195e-03  5.2828595e-02  1.4884817e-02
  2.5829215e-02 -7.7895194e-02  6.6373624e-02  3.4651104e-02
  8.3266059e-03 -8.7321796e-02  1.1536259e-01 -9.1790617e-02
 -8.2280405e-02  3.8266789e-02  7.5866587e-02  3.6057751e-03
  7.0436098e-02  6.5090150e-02  4.8293434e-02  1.8376986e-02
  4.5326866e-02  3.5402495e-02 -2.8637329e-02  2.8357655e-02
  5.4612197e-02 -8.2238898e-02 -7.2299875e-02 -4.6587791e-02
 -5.7501428e-02 -5.0577879e-02  5.7829775e-02 -6.1415777e-02
  3.5295032e-02 -6.3980237e-02 -3.5703436e-02  7.0211358e-02
 -5.6542840e-02 -9.7664773e-02  9.2908628e-02 -1.7654490e-02
 -5.8858637e-02  3.0539384e-02  5.8649659e-02 -4.9484629e-02
  8.2848787e-02  5.1184613e-02 -1.0817247e-02  1.9610284e-03
  3.7924789e-02 -2.9006384e-03  3.9839506e-02 -7.9268008e-02
 -5.9746383e-03 -8.7296888e-02  6.0885046e-02 -5.0031144e-02
 -2.9282860e-02 -6.0263544e-02  3.0303774e-02 -7.4731402e-02
  1.8445907e-02  1.9048445e-02  5.5504879e-03  1.9137483e-02
 -1.0244747e-02 -3.7558738e-03 -6.8313368e-02 -4.8689995e-02
  4.7939893e-02  5.1259629e-02 -4.8792269e-02  6.4355195e-03
  8.7383119e-03  1.4582914e-02 -4.4820337e-03  2.9781409e-02
  6.1911102e-02 -5.2543540e-02  6.3653782e-02 -4.1084625e-02
 -3.4184143e-02 -1.3998352e-02  2.7703932e-03 -9.3373381e-02
  1.0995942e-02  5.1693548e-02 -6.5813415e-02 -7.6271761e-03
  6.8857618e-02  1.8224824e-02  4.3253221e-02 -2.0809719e-02
  5.0196080e-03 -9.5690349e-03  2.9909668e-02  1.0834815e-02
  6.7639150e-02 -8.1921645e-02  7.8645289e-02  4.6589825e-02
 -6.1320011e-02  5.8873653e-02 -2.6114184e-02  5.2619342e-02
  1.6145950e-02  5.3488999e-03  1.6745180e-02 -6.4441696e-02
 -6.1777096e-02  3.8103476e-02 -1.5993198e-02 -1.0741363e-02
  7.5572707e-02 -2.0514982e-02 -1.3559435e-03  7.5040832e-02
 -3.8458309e-03 -2.1791933e-02  6.7203172e-02  1.3193805e-04
 -7.0341133e-02 -1.8637963e-02 -1.2571126e-02  7.6455705e-02
 -8.7505825e-02  4.7554150e-02  2.7485918e-02  2.2082112e-03
 -1.2852368e-02 -1.8342394e-02  7.7279203e-04 -4.9701184e-02
  5.4255869e-02 -2.5546074e-02 -8.6909741e-02  4.0983073e-02
 -2.7688446e-02  3.2310523e-02 -2.2581767e-02 -4.7730855e-03
 -7.5971574e-02  2.1844253e-02  4.4783767e-02 -8.5508160e-02
 -5.7379846e-02 -3.6195818e-02  1.3362366e-02 -7.4675530e-02
 -3.3018675e-02  1.6548164e-02 -4.5912415e-02 -1.9282144e-02
  2.2129638e-02 -4.8154122e-03 -7.1803838e-02  3.6485810e-02
  2.2916615e-02  6.2615566e-02 -3.4147032e-02 -1.5433195e-02
 -4.2003024e-02  6.8766892e-02 -5.0943993e-02  3.8419822e-03
 -1.9136381e-03 -8.5782677e-02  1.0171979e-02  5.9868619e-02
  6.4969495e-02  7.2009638e-02  1.3350740e-02  7.6574825e-02
  3.3448271e-02 -5.0453398e-02  5.4138277e-02 -1.0290392e-02
  7.8800553e-03  3.2883666e-02  5.5740811e-03 -1.8361332e-02
  7.0232332e-02  2.1547349e-02  3.5085902e-03  1.7703593e-02
 -7.6158419e-02 -5.6251860e-04  8.4509343e-02  7.6919191e-02
  9.9631036e-03 -7.9833001e-02  7.9469346e-02  5.5084053e-02
 -7.2051123e-02  6.4710349e-02 -7.7932388e-02  2.0355841e-02
  4.2327803e-02  7.2774403e-02  4.4249874e-02 -4.6810511e-02
  5.5653244e-02 -6.3667737e-02  7.5267196e-02  6.9142386e-02
  2.6647590e-02 -7.5169350e-03 -1.6958922e-02  2.2155061e-02
 -7.6981209e-02  4.8612602e-02  7.6677725e-02  8.6625338e-02
  3.4439174e-04  8.6336970e-02  2.8350322e-02 -6.2263098e-02
  8.7674445e-05 -4.7157917e-02 -1.6921913e-02 -2.6886123e-03
 -7.1908114e-03 -2.7626916e-03  2.3351714e-02 -3.0889787e-02
 -3.0984493e-02  6.2881028e-03 -7.2610721e-02  6.1577406e-02
 -8.0266669e-02  7.8354865e-02 -4.7096042e-03 -3.2423593e-02
  7.2101325e-02  7.4399719e-03 -1.9538533e-02 -3.2637652e-03
  7.6062486e-02 -9.1949724e-02  5.6730691e-02 -6.2783889e-02
  1.2758748e-02 -7.0477828e-02  3.4039319e-02 -5.5546388e-02
 -2.3346601e-02  7.0249617e-02 -9.8110018e-03 -7.6304600e-02
  3.6526162e-02 -5.6859232e-03 -1.5574048e-02  2.6972294e-02
  8.5041001e-02 -8.4662266e-02  1.2891764e-02  2.9025333e-02
  3.9351508e-02 -2.5600392e-02 -3.3241935e-02  2.4743421e-02
  6.6139273e-02  9.0939045e-02  6.4206511e-02  6.3111730e-02
  7.7077150e-02 -8.0647588e-02 -5.1053390e-02 -6.9850959e-02
  8.9958340e-02  4.0580966e-02 -2.0591412e-03  4.2278878e-02
 -6.1741360e-02  8.1728786e-02  8.8199591e-03 -7.9252720e-02
  5.6894466e-02 -3.7102958e-03  3.7363429e-02 -7.6716430e-02
  6.4846203e-02 -1.0328661e-02 -3.7896290e-02 -6.0249921e-02
  7.3884912e-02  6.7584999e-02  7.9253599e-02 -8.2189590e-02
  1.7455291e-02 -1.9829771e-02 -7.3921680e-02  8.9393675e-02
 -3.0915115e-02 -2.8893767e-02  9.0455133e-03  8.8296093e-02
  6.7401178e-02 -1.2217273e-02 -8.4708735e-02 -9.5986985e-03
  2.4707591e-02 -2.6067588e-02 -3.6122590e-02 -7.8762054e-02
  8.0505773e-02  5.5380613e-02  8.4226178e-03  7.2484717e-02
 -3.6834035e-02  2.5469644e-02  2.7498446e-02  4.0093150e-02
 -6.5518968e-02 -1.0536506e-02  1.6491868e-02 -6.0724061e-02
 -3.6599390e-02  9.3576938e-02 -9.0186976e-02  7.1802072e-02
 -3.6187258e-02  1.3493767e-02  2.4782076e-02 -8.0628864e-02
 -2.7982514e-02 -8.2059801e-02  7.2647728e-02  3.4674358e-02
  6.9178522e-02 -2.8163258e-02  1.7685844e-02 -5.5899508e-02
  1.2117922e-02 -2.0656807e-03 -7.9106174e-02  5.8484111e-02
  3.4164056e-02  7.2583035e-02  6.7733377e-02  8.8761039e-02]

transformer_blocks.8.attention.output_projection.weight — shape: torch.Size([128, 128])
[[-0.02944462  0.00298478 -0.0042878  ... -0.07872818  0.07029696
   0.04608089]
 [-0.06918017  0.06561424  0.05167627 ... -0.00390286 -0.07740851
  -0.03218817]
 [-0.05964484  0.07332773  0.03319722 ... -0.00032409 -0.01888732
   0.03465846]
 ...
 [-0.07267643 -0.0122479  -0.08982675 ... -0.02866068  0.08975668
  -0.04233573]
 [ 0.08093341  0.03447231 -0.01944974 ...  0.02370044 -0.05957367
   0.08610576]
 [-0.03503657  0.02018068 -0.06783133 ...  0.03507429 -0.00682677
  -0.07210316]]

transformer_blocks.8.attention.output_projection.bias — shape: torch.Size([128])
[-0.06778885 -0.00719887 -0.06201137  0.05574406  0.03219914 -0.06360585
  0.06778728 -0.0549134  -0.00499716 -0.0013875  -0.05195776  0.07431301
  0.02344996  0.00113616  0.03006378 -0.08119964  0.07616892  0.05498354
  0.03806    -0.05565093 -0.02180811  0.07215494 -0.03651463  0.06586611
  0.04452322 -0.00897508  0.03740235 -0.00705379 -0.03799258 -0.02991951
  0.02673288  0.02699408 -0.04872683 -0.03071909  0.01728141  0.06584592
 -0.0597364  -0.05151688  0.04922548 -0.02697949 -0.07481827  0.05090526
  0.04555988  0.07456535  0.06123845 -0.05373315  0.08380029  0.01507057
  0.01840309 -0.00468139 -0.00550727  0.01345166  0.04034406  0.00534346
 -0.06033348 -0.03696771  0.01430943 -0.09667363  0.06919842  0.06975374
  0.03556984  0.03682377  0.07959241  0.04449635  0.03436697  0.04694299
 -0.08393295  0.00399646 -0.00425107 -0.0817794  -0.0704973   0.07032136
 -0.07710224  0.03141233  0.04009774  0.059765   -0.01387495  0.03241946
  0.00077274 -0.05578113  0.06180417  0.02906066 -0.04032924  0.03391948
  0.03388567 -0.0258      0.00693858  0.01164194 -0.07242283  0.04397526
  0.04650907 -0.05592511 -0.08293463  0.02751425 -0.01529698 -0.02060703
  0.0844484   0.0554032   0.07842119 -0.08508317 -0.02457133  0.04480113
 -0.00898197 -0.01021107  0.02216227  0.07147024  0.0397329  -0.04429584
 -0.02209448  0.05668788  0.05778672  0.00711081  0.06810009 -0.03781032
 -0.04927883 -0.03842485  0.0098691  -0.0666408  -0.0660244   0.03157662
 -0.03041814 -0.07771138  0.0089374   0.03102192 -0.00827427 -0.06461161
 -0.01129253  0.02865576]

transformer_blocks.8.attention_layer_norm.weight — shape: torch.Size([128])
[1.0359    1.0073513 1.022359  1.0255439 1.0314882 1.0206958 1.0068607
 1.0404638 1.0304264 1.0400753 1.0260161 1.0214797 1.0218897 1.0200669
 1.0019335 1.0154942 1.0268099 1.0309125 1.0379167 1.0183276 1.0320365
 1.0355431 1.0185237 1.0268877 1.0156078 1.0164788 1.0352681 1.0312027
 1.0195857 1.0079894 1.0124004 1.0159138 1.0155096 1.0415338 1.0153421
 1.0306915 1.0342603 1.0339756 1.0212826 1.019089  1.0507509 1.0053124
 1.0390245 1.0304227 1.014296  1.0079145 1.0121382 1.0268528 1.0152917
 1.0321385 1.0165012 1.0180331 1.0223361 1.0338386 1.0440977 1.0297933
 1.0376949 1.0348817 1.0410942 1.0227863 1.0342412 1.0148294 1.0232031
 1.0222772 1.0233219 1.0384752 1.0380383 1.0279145 1.0365525 1.0320373
 1.0177445 1.0308334 1.0169096 1.033797  1.023794  1.0352666 1.0252056
 1.0163122 1.0288785 1.0320747 1.0259447 1.0410464 1.034893  1.0249401
 1.001288  1.0268297 1.01859   1.0096692 1.0005844 1.0252002 1.0267003
 1.0049171 1.0225815 1.0036254 1.0257666 1.0209959 1.0178653 1.0354356
 1.0242898 1.0276322 1.0125282 1.0323944 1.025737  1.0161886 1.0146465
 1.0316613 1.0079185 1.0729332 1.0110146 1.0318933 1.0305948 1.0320231
 1.0259795 1.0348591 1.0293411 1.0266696 1.0145978 1.0280874 1.0332186
 1.0356238 1.0451277 1.0235234 1.0206796 1.0254059 1.0275323 1.0284083
 1.0166192 1.0252782]

transformer_blocks.8.attention_layer_norm.bias — shape: torch.Size([128])
[-0.00694085  0.00500445  0.01001764  0.00531284 -0.00705686 -0.00417827
  0.00270425  0.00553697 -0.00560586  0.01132474 -0.00412032 -0.00081967
  0.00788406  0.00229931  0.00454019  0.00560343 -0.00936823 -0.01238957
  0.00651325 -0.00529422  0.0068404  -0.00337754  0.00166586 -0.00136283
  0.01166239  0.00321565  0.00329261  0.00544638 -0.00072349  0.00287871
  0.0043373  -0.00116934 -0.00763943 -0.00392479  0.00760519  0.00823158
 -0.00659374  0.01032006  0.00074324 -0.00238639  0.00537799  0.00367583
 -0.00613898 -0.00476558  0.00521678 -0.00453449 -0.00725599  0.00707126
  0.00093507  0.00328123 -0.00949392  0.00161569  0.01266151  0.00275854
  0.02252429 -0.01031636  0.0018582   0.00686779 -0.00117217 -0.00421166
  0.00137256 -0.00611814 -0.00064439 -0.00179915  0.02100001 -0.00479525
 -0.00560204 -0.00340626  0.00155314 -0.00333611  0.00373803  0.00260949
 -0.0052586  -0.00401905  0.00311859 -0.00871848  0.00801498 -0.00040139
 -0.01447801  0.00475561 -0.00202681  0.00558086 -0.00048028  0.00489018
  0.0120432   0.004461    0.00685208  0.00058402 -0.00816604  0.00562814
 -0.00813873 -0.0087085   0.0049279  -0.00221133 -0.00818203  0.00738238
 -0.00414802  0.00157281 -0.0017871  -0.00668857  0.00149286  0.00496624
 -0.00381134 -0.00657115  0.01040159 -0.0109234   0.00052519  0.00019828
 -0.00199087  0.00444069 -0.00406158  0.00161965 -0.00198878 -0.00362174
 -0.00775698 -0.00086424 -0.00263806 -0.00235229 -0.00593446  0.0049121
  0.0032837   0.00903997 -0.01007605 -0.00697287  0.00203549 -0.002699
  0.00672399 -0.00348834]

transformer_blocks.8.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0321858  1.032918   1.0316412  1.0334468  1.0258559  1.0369201
 1.0390445  1.041354   1.0262138  1.0329111  1.033995   1.0345049
 1.0307502  1.0280778  1.030794   1.035014   1.0290279  1.0088975
 1.0303745  1.0323025  1.0190294  1.0347131  1.0325264  1.0279797
 1.0280528  1.0334563  1.0271637  1.0313343  1.0293053  1.027606
 1.0281276  1.0269738  1.0356497  1.033785   1.0431983  1.0342646
 1.0311915  1.031097   1.0352161  1.0385906  1.0357901  1.0292577
 1.032216   1.0118812  1.0365994  1.0271269  1.0171397  1.0306845
 1.032865   1.017712   1.0330024  1.0314791  1.0328603  1.031804
 1.0351391  1.0111547  1.0180575  1.0306678  1.0396994  1.0179832
 1.0284853  1.0226094  1.03676    1.0330149  1.0321862  1.0436335
 1.0245688  1.0273961  1.0319091  1.0333879  1.029116   1.0219871
 1.0258826  1.0397776  1.0143158  1.0357713  1.0372998  1.0240172
 1.03485    1.0324273  1.0284009  1.0301098  1.0238798  1.0307096
 1.0316869  1.0312126  1.0343543  1.0350157  0.99429274 1.0261852
 1.0253826  1.0308918  1.0289484  1.0024334  1.0309688  1.0256813
 1.0320231  1.0272477  0.99901545 1.0350938  1.0238109  1.041393
 1.0473006  1.0354868  1.036562   1.0276399  1.0208131  1.0370275
 1.0298605  1.030359   1.0427902  1.0116059  1.0295098  1.0118376
 1.0423298  1.033776   1.0048544  1.0286286  1.0193511  1.0334415
 1.0367366  1.0299411  1.0349985  1.0341767  1.0228425  1.0312524
 1.0299119  1.0302286 ]

transformer_blocks.8.feed_forward_layer_norm.bias — shape: torch.Size([128])
[-0.00995946 -0.00205291 -0.00365034 -0.00469262 -0.00023182  0.00824772
 -0.01302867  0.005894   -0.0132528  -0.00566848  0.01940794  0.01377268
 -0.02279385  0.0044305   0.02825422 -0.00993838  0.01845455  0.00486858
 -0.01861525  0.01271847 -0.021323   -0.02026955  0.02344253  0.02587705
 -0.00847752 -0.01435056  0.02246119 -0.02472144 -0.02029405 -0.01169648
  0.02419667  0.01643607 -0.02073139  0.00799007 -0.01579014 -0.01685554
 -0.00344708 -0.02278526 -0.00555387 -0.00694542  0.00927542  0.00904156
 -0.01889064  0.02846115 -0.02204276 -0.00387311  0.00304063 -0.01654529
  0.02211109 -0.02744057 -0.02967106  0.01037655 -0.01382909  0.00724766
  0.02471758 -0.00610251  0.0261148  -0.02139239  0.00133093 -0.01191484
 -0.00335477 -0.0064936   0.01631519 -0.00641537  0.00734086 -0.01719723
 -0.01272841  0.02310545 -0.02259439 -0.01707783 -0.01807054  0.00320423
  0.02260374  0.01443824  0.02059784  0.00919482 -0.0208689   0.00969352
 -0.00524676  0.00824733 -0.01833686 -0.01502236 -0.02068425 -0.00330864
 -0.00896356  0.02385798  0.01833326 -0.02634094  0.00153632  0.01271566
  0.02179307  0.01631946 -0.02486434 -0.00326706  0.02374494  0.02369491
  0.01350979  0.02830637 -0.01533554 -0.02230485  0.02930004 -0.02633623
 -0.01412063 -0.01169839  0.02059958 -0.00217845  0.00951109  0.00462404
  0.01590922  0.01859802 -0.01489506  0.02706298  0.01240629 -0.0273814
  0.00703462  0.00444523 -0.02392889  0.01412569 -0.01467922  0.02638965
 -0.01935026  0.00577484  0.0259969  -0.01058017 -0.02502287 -0.0138922
 -0.0139312   0.00224508]

transformer_blocks.8.feed_forward.0.weight — shape: torch.Size([128, 128])
[[ 7.60200247e-02  6.52272254e-02 -2.77043916e-02 ... -7.62554258e-02
  -4.65137400e-02 -8.71455595e-02]
 [-2.10987832e-02  2.82497015e-02 -2.63913930e-03 ...  1.09970801e-01
  -5.46460301e-02 -6.93975985e-02]
 [-9.14415568e-02  2.45915502e-02 -1.03661083e-01 ...  7.61677474e-02
   1.93992201e-02  1.00429937e-01]
 ...
 [-5.19907176e-02 -7.31357858e-02  7.09125623e-02 ...  1.04385696e-01
  -1.06609225e-01  1.18329446e-03]
 [ 8.98719281e-02 -7.10314810e-02 -5.71424104e-02 ... -5.54785207e-02
   5.36790043e-02 -1.02987044e-01]
 [-8.06810334e-02 -6.71001570e-03 -3.20766829e-02 ...  5.34088940e-05
  -1.99621916e-02  2.25121360e-02]]

transformer_blocks.8.feed_forward.0.bias — shape: torch.Size([128])
[ 0.00418476 -0.02354007 -0.01679582  0.09413413  0.09559044 -0.00695071
  0.01803419 -0.00509837 -0.03173022 -0.01906162  0.04149061  0.09077729
  0.00148451  0.07198785 -0.00582162  0.08182846  0.04367966  0.065155
 -0.06110338  0.09290518 -0.06536212  0.03497224  0.02675746 -0.05941181
  0.02624775  0.05966532 -0.08585386  0.01231385 -0.05761599  0.08141946
  0.04627313  0.03792628 -0.01679223 -0.06760132  0.01076323 -0.0031248
 -0.04978935 -0.04850793  0.03973927 -0.00985166  0.04324795 -0.03156833
 -0.01266299 -0.03536548  0.06214153  0.05681334  0.04815605  0.03149268
  0.09080632 -0.06171018  0.07436366  0.04941861  0.07712097  0.06536438
  0.05882614 -0.06521161  0.0430812   0.01685387  0.05751752 -0.01510733
 -0.05508436  0.02212233 -0.05518996 -0.03370506 -0.00716766  0.00303988
  0.10321797 -0.00264353 -0.02702822  0.08263341  0.00444691  0.00599773
 -0.08169769  0.02645066 -0.05099268 -0.02928702 -0.05711585  0.00784235
  0.09848372  0.03798958  0.00930874 -0.04343728 -0.01189934 -0.01774775
  0.10887603  0.02351305  0.01208265  0.06676199  0.11309084 -0.03633678
  0.02526361 -0.02863286  0.01001523  0.08603065  0.1115028  -0.06381766
  0.06207027  0.01538185  0.06503145 -0.02193918  0.0296111   0.05796988
 -0.05242971  0.00390002  0.10304148 -0.01779257 -0.01684084  0.08783293
 -0.03897516 -0.00927326  0.02682001  0.064919    0.06414353  0.06391916
  0.07934637  0.06710231 -0.03488098 -0.03098903  0.09257104 -0.04035072
  0.09090963  0.0308651  -0.01868992  0.07462193  0.06771743 -0.07363315
  0.10418844  0.07071143]

transformer_blocks.8.feed_forward.2.weight — shape: torch.Size([128, 128])
[[ 0.05399245  0.0826481  -0.10794192 ...  0.03786912 -0.03181177
   0.02118281]
 [ 0.02437573  0.06899386 -0.0882847  ...  0.00414079  0.10738252
  -0.02900897]
 [ 0.0527481   0.01692175  0.05599859 ... -0.08137634 -0.09357948
   0.06057063]
 ...
 [-0.01310575  0.01292829  0.05737734 ... -0.0149764   0.0779187
  -0.08095672]
 [ 0.07055964 -0.09958921  0.02160892 ...  0.07498274  0.05060399
  -0.04120408]
 [-0.10789796  0.10999214 -0.08965211 ...  0.0750003  -0.05062991
  -0.03468692]]

transformer_blocks.8.feed_forward.2.bias — shape: torch.Size([128])
[ 0.04122404  0.00694287 -0.01758352  0.05703343  0.0468297   0.06338961
  0.0072178   0.03573218  0.03078112  0.01601383 -0.07851855 -0.02842174
  0.00128303  0.06271853  0.00315524 -0.07404967  0.04828839  0.0337066
  0.0691096  -0.00168436 -0.07166988  0.06377902  0.02159028  0.0116863
  0.04370455  0.0617414   0.02499845 -0.03950718  0.00373292  0.02545671
 -0.05101876 -0.05414892 -0.00517855 -0.04653519  0.00047086  0.01001177
 -0.01831967 -0.00718732  0.07809994  0.0834099  -0.01113582  0.02488564
 -0.01164911 -0.02526993 -0.00114518 -0.030873   -0.05696582  0.07826881
 -0.00263933 -0.02801062 -0.02210151  0.01118028 -0.04483731 -0.04100608
 -0.03763331  0.09146628 -0.07025309  0.01928576 -0.0284675   0.00449318
  0.03403111 -0.02989225  0.02719446 -0.08318617  0.02513683  0.05457082
  0.04585994 -0.02363698  0.0094464  -0.02647077 -0.00840829  0.08154517
 -0.06121276 -0.00560641  0.05244523 -0.05346848 -0.08361399 -0.08064857
  0.09167245  0.05748139  0.0339404   0.05120449  0.00739428  0.05794563
  0.00686467  0.0097996  -0.04971799 -0.0254247   0.03614809  0.02895281
 -0.07699363 -0.04637438 -0.06761623  0.02823914  0.06725142  0.01672325
  0.00307916 -0.05967678  0.02150061 -0.08149458 -0.06810486 -0.05063446
 -0.00150108  0.06955197 -0.03682203  0.07685627 -0.02001934 -0.04947643
 -0.06969729 -0.02728829 -0.05232617  0.01235493 -0.09145813 -0.07734225
  0.04541508 -0.09388443 -0.06116884 -0.02735652  0.01971475  0.07160152
  0.02127416 -0.08760464 -0.0591248   0.0190387   0.00733683  0.0284753
  0.01365684  0.00886294]

transformer_blocks.9.attention.query_key_value.weight — shape: torch.Size([384, 128])
[[ 0.00324438 -0.01042039 -0.00140282 ...  0.01771284 -0.03506659
   0.07296365]
 [ 0.05242839 -0.02360471 -0.0765169  ...  0.06676177  0.03680484
  -0.01207883]
 [-0.10994426 -0.00325353 -0.06586463 ...  0.01855439  0.01818929
  -0.04381671]
 ...
 [ 0.09062932  0.04365556 -0.09829264 ...  0.04446968 -0.06626423
   0.05198858]
 [-0.03578088  0.05289006  0.0712304  ... -0.05311788  0.01686658
  -0.04129223]
 [ 0.00222921  0.00604269  0.05582644 ... -0.07342029  0.03575064
  -0.02303444]]

transformer_blocks.9.attention.query_key_value.bias — shape: torch.Size([384])
[-0.09932771  0.00625326 -0.04195049 -0.05074989 -0.07028421  0.10168359
  0.04340578  0.08233117  0.03433794  0.06628422 -0.08512523 -0.02711551
 -0.09043619  0.01678829 -0.05072174 -0.05291724 -0.01437671 -0.04768655
  0.0206536   0.005485    0.00473454 -0.01057607  0.08969916  0.02376118
 -0.01340914  0.0619696   0.03346419 -0.01387278 -0.07573065  0.04131603
 -0.02583396 -0.07056472 -0.05135691  0.05666889  0.08430572 -0.07772559
 -0.00039064  0.01794087  0.0451081  -0.02741781  0.06852797 -0.03668822
  0.01168244  0.01194065 -0.09980595  0.02644959  0.03141459 -0.0062875
 -0.05250185 -0.06335033 -0.00501146 -0.01425777 -0.08903471  0.02088553
 -0.06684195  0.07002726  0.05763699 -0.03728795  0.04660755  0.05940867
  0.05420257  0.04657916 -0.03575489 -0.05622463 -0.02844874 -0.09955443
  0.00122694 -0.0572532  -0.03710109 -0.06059311 -0.09150591  0.00991995
  0.01966271 -0.05986161 -0.06493393 -0.00260819 -0.05709928 -0.04021682
 -0.05130066 -0.09620021 -0.07215729 -0.08156683 -0.09071561  0.0443741
 -0.06202184 -0.06879665 -0.07648537  0.02211147  0.08310998 -0.00459037
 -0.02088041 -0.0795763   0.00456009  0.01244223  0.04282938  0.09302166
 -0.03486959  0.00965993  0.01867062 -0.098016    0.02216926  0.08959545
  0.08018751  0.01320184  0.06086152  0.02440786 -0.07113887 -0.06642406
  0.03244665  0.01632417 -0.04139003  0.07768066 -0.05974215 -0.01865464
  0.05033102 -0.09819467 -0.06184858  0.06355119 -0.03468783 -0.028733
 -0.03634992 -0.03492118  0.06339677 -0.09263484 -0.07671221 -0.02783194
 -0.0253495  -0.05846624 -0.05996964  0.0212124  -0.00436021 -0.02542477
  0.04717828  0.04929921 -0.05212468  0.05420481 -0.0784431  -0.01106754
 -0.05950692  0.07720315  0.02953337 -0.0227124  -0.07229992 -0.02795889
  0.04753702 -0.05632116 -0.06377784 -0.07635102 -0.00657533  0.08090571
 -0.03020669 -0.05903693 -0.08371192  0.01357364  0.02818944 -0.02022943
 -0.02959486 -0.08540554  0.02732608 -0.07061725  0.05857052  0.01676028
  0.08134426  0.00457341 -0.02803746 -0.03976272 -0.0147907   0.01837368
 -0.03664863  0.00085819  0.05057425  0.04879658 -0.05502201  0.00029683
  0.02923645 -0.01169533  0.06854096  0.08191818 -0.00651607 -0.07475895
  0.06400835 -0.02056647  0.00123731 -0.02479236  0.01278374  0.06408224
  0.00346134  0.04110613 -0.02419464  0.05594842 -0.07169258 -0.01093043
  0.05318201 -0.04734021 -0.08502658  0.0735538  -0.04019322 -0.0607294
  0.0753523   0.05822155  0.06732102 -0.0386807   0.04565081 -0.04278117
 -0.00632192 -0.06496799 -0.01386083 -0.04146041 -0.07369266 -0.01020956
  0.06661598 -0.04031486  0.0484844   0.0840782   0.08523672  0.07687069
 -0.06809261  0.04452553  0.07216123  0.0844682   0.02998295  0.0681672
 -0.01437102 -0.02878874 -0.0453749  -0.05714478 -0.06886338  0.07367656
  0.01791939  0.00870231  0.05689103 -0.04628414  0.02730772 -0.05285626
 -0.01424723 -0.03609046 -0.00508415 -0.07838651 -0.01499457  0.02710125
 -0.04770334  0.03619425  0.00616522  0.06295676 -0.01029622 -0.07541153
 -0.03631927 -0.04307565 -0.06648619  0.06610316 -0.06669453 -0.01476552
 -0.08258561  0.0107192   0.03116942 -0.08079371  0.05819823  0.03175817
 -0.0175313  -0.03034361  0.00613122  0.06395283  0.001279    0.02626383
 -0.07442743 -0.01891253  0.05099706 -0.07509996 -0.008932    0.02108223
  0.02820158  0.00806908  0.00297229 -0.08418418 -0.05009797 -0.07183811
 -0.04033731  0.00301803  0.00930748 -0.05194325  0.01569131  0.02512853
  0.00130782 -0.00358017  0.06283022  0.01235566  0.07621188 -0.01369238
 -0.07793235  0.02672929  0.01354458  0.06119392 -0.03684902  0.05948784
 -0.04508582 -0.03691231 -0.07402939 -0.03056279  0.05596276  0.0375334
  0.06424684  0.02236714 -0.05245588 -0.07484405  0.02700306  0.08898713
 -0.05691307 -0.00565403 -0.07395151 -0.05343997  0.05592339 -0.05628895
 -0.00744207  0.08551786 -0.06946705 -0.05825504 -0.04187407 -0.0531857
  0.00789841 -0.07262859 -0.07101282 -0.03563525  0.05447068 -0.0551106
  0.06662022  0.04338023  0.07785977 -0.08384868 -0.04778748 -0.02278025
 -0.07523077 -0.04195017 -0.02976161  0.0537138  -0.06477232 -0.03675226
  0.04242958 -0.07462949  0.0122028   0.06302638 -0.04154251  0.03566328
 -0.03350406 -0.00994773  0.06971446  0.03721776  0.03351557 -0.03821282
 -0.02663469 -0.09155737 -0.08136646  0.05395798 -0.07543642 -0.08582263
 -0.01400353  0.00366535 -0.07123608  0.03430696  0.02455096  0.07620773
  0.07430093  0.00505664 -0.0606689  -0.07188939  0.012045   -0.05777768
  0.02851715 -0.06426921  0.03207913 -0.0297421  -0.00860286 -0.00340512
  0.01312196  0.02054053 -0.00142627  0.02641575  0.02825307  0.03519458
  0.02073247  0.0519225  -0.00160629 -0.05461656 -0.02923054  0.05495284]

transformer_blocks.9.attention.output_projection.weight — shape: torch.Size([128, 128])
[[-0.10393366  0.08816823 -0.07685576 ...  0.08124851 -0.02361032
   0.02437744]
 [-0.01024675 -0.0087255  -0.01046916 ... -0.01590047  0.03931793
   0.07049779]
 [ 0.02978374  0.06598766 -0.00478733 ... -0.03898042 -0.0484116
   0.04135304]
 ...
 [ 0.01236816 -0.04706368 -0.04859145 ...  0.07531052  0.08553484
   0.05736318]
 [-0.03521884  0.08278707 -0.02032351 ...  0.02803475  0.03235898
  -0.06088132]
 [ 0.04684616 -0.04508208 -0.07837274 ... -0.03637886 -0.0670943
   0.03595879]]

transformer_blocks.9.attention.output_projection.bias — shape: torch.Size([128])
[-0.06889943 -0.08492534  0.06575552 -0.00203778  0.02365374 -0.03530855
 -0.07406536 -0.02910153  0.0226218  -0.0645846   0.03072557  0.03250475
  0.03054366 -0.04414808  0.0638994  -0.06786763 -0.07974189  0.07622256
 -0.00631102  0.02561072 -0.07357041  0.05881497 -0.03965095 -0.00253537
  0.05577998  0.05810958 -0.07005872  0.01207804 -0.0830742  -0.02753742
  0.09840125 -0.06043901  0.02203718  0.07021053 -0.08541033 -0.04494475
 -0.03271706 -0.03088037 -0.05928666  0.04781634  0.05577549  0.00672935
  0.05219444  0.06771383 -0.06327043 -0.00602475 -0.0293722  -0.04497432
 -0.00461948 -0.04576109 -0.03446239  0.01141895  0.04165637 -0.01269255
 -0.04286851  0.09825085  0.0650771  -0.0147552   0.03337561  0.06680445
  0.03542374  0.03371342  0.06676122  0.03860353  0.06951207 -0.01185399
 -0.04405698  0.03566337  0.0378354   0.00879284  0.05267151 -0.02582442
 -0.07693923 -0.07033492 -0.06220173  0.05436555  0.01165465  0.00418373
 -0.0017893   0.04104073 -0.00926199 -0.05897779 -0.07052086  0.01058397
 -0.06096476 -0.06625308 -0.05113086 -0.08948619 -0.02758321  0.04582787
 -0.0245529   0.04928022 -0.00285099 -0.04437752  0.06402796  0.02114807
  0.02812418  0.09024008 -0.01312692 -0.08268092  0.06268752  0.00023991
  0.05706549 -0.06044969 -0.056801   -0.02621709  0.05551641  0.02032485
 -0.03054499  0.03902565  0.03900955 -0.07177383 -0.0030449   0.02678869
 -0.00214217 -0.06612682 -0.04657439  0.033594    0.06877681 -0.02784795
  0.07756026  0.06265449  0.05234559 -0.08357111 -0.02963522 -0.02939651
  0.05975781  0.06478596]

transformer_blocks.9.attention_layer_norm.weight — shape: torch.Size([128])
[1.0310311  1.001846   1.027628   1.0322032  1.0232102  1.0280714
 1.041868   1.037565   1.0408572  1.0370443  1.0524426  1.0249923
 1.0264606  1.0148679  1.0169247  1.0315232  1.0165316  1.0370625
 1.0372483  1.0315039  1.0309615  1.0313005  1.0071725  1.0248208
 1.029342   1.0210435  1.0336872  1.0329695  1.0129294  1.0180637
 1.0288975  1.0270596  1.0179228  1.0343742  1.0154942  1.0361532
 1.0379086  1.0341294  1.0252453  1.0277483  1.0239305  1.0030781
 1.048737   1.0341564  1.0267458  1.0337175  1.0173093  1.0297303
 1.04188    1.021393   1.0156522  1.0205222  1.0252512  1.0241191
 1.0035068  1.0395666  1.0396274  1.0315183  1.0305165  1.036153
 1.0286754  1.0262797  1.0181347  1.029767   1.0294715  1.0422118
 1.0342419  1.0318028  1.0302829  1.0231118  1.0308734  1.0299035
 1.0143751  1.0313712  1.0301347  1.0303942  1.0363449  1.0059777
 1.0178964  1.0205071  1.0373158  1.0401917  1.023796   1.0311315
 1.0204414  1.0138786  1.0285263  1.0293877  1.0150028  1.0310969
 1.032812   1.0136194  1.0235661  1.0191978  1.029996   1.0163778
 1.0084274  1.02148    1.0238168  1.0301623  1.0320747  1.0250046
 1.013928   1.0272532  1.0330468  0.99799293 1.0296086  1.0275936
 1.0137962  1.0357425  1.0454258  1.0417868  1.0215474  1.031716
 1.0329269  1.024229   1.0152446  1.0252287  1.0230489  1.0229325
 1.0369276  1.0287861  1.0261551  1.0291524  1.0165453  1.0314933
 1.0183301  1.0164378 ]

transformer_blocks.9.attention_layer_norm.bias — shape: torch.Size([128])
[ 6.67106267e-03  1.22581879e-02  5.88173862e-04  7.84483831e-03
  4.27788356e-03  5.70555124e-03 -8.96348245e-03 -1.09457839e-02
 -3.59483738e-03  9.60166100e-03 -9.52993869e-04  7.44165806e-03
 -2.77228700e-03 -6.39466522e-03 -7.58121954e-04  2.66917213e-03
 -1.73725944e-03  1.02482121e-02 -8.20522942e-03 -7.53236702e-04
  7.79589731e-03 -1.15312813e-02  1.80225167e-03  3.81370587e-03
 -7.32762553e-03  5.84879890e-04 -7.37482682e-03  2.79580336e-03
  8.59864336e-03  7.77025707e-03 -1.27431992e-02 -5.56659838e-03
  6.97316788e-03 -3.23624327e-03  5.87358605e-03 -1.01926329e-03
 -5.85315702e-03  1.80647138e-03 -1.31849013e-02 -3.84742394e-03
  4.26690001e-03  6.61805505e-03  3.78951849e-03  5.38448710e-03
  1.03841815e-02  1.20877493e-02 -2.33974843e-03 -8.65223352e-03
 -8.16195179e-03  4.87715425e-03  1.58459379e-03  9.08602960e-03
  1.21402566e-03 -9.08332225e-03  1.25259636e-02 -5.98168746e-03
 -9.48291551e-03 -4.27914085e-03 -3.88376811e-03 -7.77103473e-03
 -1.74078559e-05  6.76726084e-03 -7.92071968e-03  6.20537065e-03
 -4.73812362e-03  9.81508009e-03  1.24578029e-02 -1.12885907e-02
 -4.50299494e-03  9.85641684e-03  1.00124469e-02 -6.51550852e-03
  7.42005324e-03  8.70758481e-03 -9.70056839e-03  6.07032329e-03
 -7.89922196e-03 -2.12336512e-04 -7.92501727e-04 -3.57762771e-03
  1.12976423e-02  1.81789435e-02 -4.41008760e-03  1.95453642e-03
 -8.69369972e-03 -7.88512174e-03  1.45920189e-02  4.67071775e-03
 -8.87109991e-03 -4.47660126e-03  1.01089361e-03 -1.02232527e-02
 -3.29243788e-03  3.34626739e-03  5.35893347e-03  1.24947145e-03
 -7.38206552e-03 -1.29709570e-02  5.38865454e-04 -7.70868082e-03
 -7.33126095e-03  1.02168024e-02 -1.02004101e-02 -1.03947576e-02
 -1.44791901e-02 -1.24404384e-02 -1.97220594e-03 -1.00478791e-02
  1.09988302e-02  1.51101043e-02 -4.46053967e-03  5.23984199e-03
 -7.77118374e-03  5.74698322e-04  4.35653329e-03 -7.05511449e-03
  7.18101067e-03  6.18214067e-03  6.24661101e-03 -8.61839578e-03
 -1.09948739e-02  5.64533612e-03 -5.19006699e-03 -4.94647771e-03
 -8.32725409e-03  2.10257387e-03  8.06041248e-03  5.74594876e-03]

transformer_blocks.9.feed_forward_layer_norm.weight — shape: torch.Size([128])
[1.0236977  1.0080969  1.0306292  1.0115653  1.0356768  1.042926
 1.0510005  1.0328797  1.04306    1.0344576  1.0302696  1.0320287
 1.0359714  1.0295945  1.0121717  1.0296829  1.0339228  1.0287302
 1.0326552  1.030042   1.0347936  1.0315437  1.0413717  1.0209699
 1.0068066  1.0338359  1.0251627  1.0351758  1.0247588  1.034556
 1.0265255  1.0254015  1.0329901  1.0312595  1.0276396  1.0294243
 1.0237352  1.0137595  1.0451994  1.0324361  1.0232863  1.0261141
 1.0140694  1.0169029  1.022739   1.0281475  1.0254921  1.0145061
 1.0078729  1.026143   0.98838073 1.0353956  1.0084022  1.0317749
 1.035891   1.0236475  1.0306191  1.0277636  1.0358863  1.0278735
 1.0301598  1.0281413  1.029972   1.0287004  1.0312142  1.0333773
 1.024451   1.022426   1.0298796  1.0293417  1.0304431  0.9958927
 1.0301979  1.0131458  1.0264734  1.0261612  1.0340627  1.01336
 1.0306668  1.022324   1.0328399  1.0203234  1.0307125  1.0352566
 1.0192204  1.0330746  1.0146816  1.0078421  1.0271728  1.0262262
 1.0388887  1.0061092  1.0291777  1.03748    1.032931   1.0239462
 1.0345849  1.0303422  1.0485517  1.028528   1.0348021  1.0322323
 1.02759    1.0283154  1.0328182  1.0313313  1.024857   1.0355976
 1.013142   1.007918   1.0250121  1.0344169  1.0299184  1.0351621
 1.0152458  1.033563   1.0318191  1.0164582  0.9965424  1.0365571
 1.0249403  1.006258   1.0234752  1.0335865  1.0412924  1.0262834
 1.0368488  1.016515  ]

transformer_blocks.9.feed_forward_layer_norm.bias — shape: torch.Size([128])
[-0.01814162 -0.0091496  -0.01878668  0.00369376 -0.01540086  0.00600531
  0.01069828  0.0095433   0.00790623 -0.01864466  0.01880322  0.02520024
  0.01154907 -0.01025552 -0.02111787 -0.02599875 -0.00210892 -0.02023951
  0.02936796 -0.02531353 -0.04025778  0.01328827 -0.00204111  0.01192044
  0.00564106  0.02750674 -0.00091446  0.00868069  0.00990349  0.00960315
  0.01897321 -0.01268323 -0.00832724 -0.00927806 -0.01444584 -0.01939987
 -0.01204747 -0.02713569 -0.00446966  0.02133263  0.01420075  0.03078265
 -0.01670919  0.0248605  -0.00349352 -0.00446166 -0.0167654   0.01348445
 -0.00362022 -0.02006823  0.0135217   0.01683272 -0.00380305 -0.01889939
 -0.01414173 -0.00162572 -0.00852941 -0.01303358 -0.00931782  0.02755993
 -0.0234362   0.01705036  0.02074756  0.01411991  0.0205735  -0.00039475
  0.01755748  0.01843817 -0.0086328  -0.01004651  0.01646897  0.01517263
  0.00470682  0.01376615 -0.00639266 -0.01185322  0.02092234  0.02293335
  0.01053652  0.01531663  0.01093595 -0.00593655  0.02402216 -0.0210282
 -0.01447812 -0.02822275  0.00112839 -0.01346823 -0.02052596 -0.01830748
 -0.02594025  0.02565561  0.00476606  0.00438542 -0.02147386 -0.02659514
  0.00271733  0.01074926 -0.01541891 -0.00800977  0.01352739 -0.02610985
 -0.02620031 -0.00847014 -0.02348759  0.01156468  0.01333386  0.00248722
  0.00623601  0.02444632  0.0057665   0.0020157  -0.02200357  0.00902069
  0.02094321 -0.02370851 -0.02250928  0.01027344  0.02531634 -0.00232836
  0.01209284  0.00243842 -0.00638951  0.0266623   0.01982904  0.00446607
  0.02465669 -0.02062234]

transformer_blocks.9.feed_forward.0.weight — shape: torch.Size([128, 128])
[[-0.00680322 -0.03210854 -0.07603306 ...  0.01941118  0.01894451
   0.07293022]
 [-0.09353668  0.02445965  0.0542811  ... -0.00352089  0.00328924
   0.02919106]
 [-0.05325568 -0.02429792  0.07449613 ...  0.02655532  0.04138298
   0.01907444]
 ...
 [-0.08915263 -0.03117684 -0.0455025  ...  0.04146345 -0.07119427
  -0.0311468 ]
 [-0.02002442 -0.08207341 -0.05529276 ...  0.04531726  0.05801632
  -0.02388216]
 [ 0.07782619 -0.07192723 -0.02591144 ...  0.10341734  0.05477972
  -0.03922713]]

transformer_blocks.9.feed_forward.0.bias — shape: torch.Size([128])
[ 0.00953352  0.05280762 -0.00370078  0.03936474  0.03110882  0.01687522
  0.03034823 -0.07667077  0.06371422  0.08442134 -0.02656719 -0.01228721
 -0.07414497 -0.03800529  0.02766888 -0.0505604  -0.000274    0.00155783
  0.08573034  0.07733107  0.06199051  0.00324619  0.053674    0.07826027
  0.09454623  0.04183929  0.0799685  -0.09316709  0.07299657  0.01388652
  0.01092326 -0.03177029 -0.02338862 -0.01326544  0.05632873  0.06234053
  0.03640907  0.06086526 -0.00217624  0.06587205 -0.04893379 -0.01226698
 -0.05584715  0.00050579  0.04315149 -0.05320265  0.06171624  0.08311483
  0.04061637 -0.08111769  0.06381403  0.02652627 -0.00796059 -0.00401046
  0.01081047 -0.06600568  0.09970425 -0.05783806  0.09573045 -0.03049613
  0.04305772  0.03860986 -0.02797669 -0.07735131 -0.00991014  0.02800237
  0.0425442   0.00895479 -0.05596363  0.02303629  0.09443145  0.03810733
 -0.01724736 -0.03574405 -0.03502831  0.05381517  0.04803172 -0.05435884
 -0.01166534 -0.02383331  0.05311943  0.00611735 -0.0004411  -0.05852119
  0.07307749 -0.02133353 -0.0461743  -0.08452978  0.06970678  0.04228614
  0.07628662 -0.00357899  0.03914626  0.05053632 -0.07089935 -0.02577785
  0.01235552  0.00251577  0.10545293  0.05395017 -0.02121992 -0.04383169
  0.06512914  0.05493768  0.05655734 -0.07222045 -0.01697726  0.01334272
  0.00816015  0.03554881  0.00718183  0.07741247 -0.05380095 -0.03779135
  0.04477722  0.02080012 -0.01234974 -0.02209123  0.09408423  0.04382327
  0.05315234  0.09685209 -0.01353593  0.00999007  0.03609938  0.03146882
  0.08660236 -0.0470437 ]

transformer_blocks.9.feed_forward.2.weight — shape: torch.Size([128, 128])
[[-0.0907343   0.06003691  0.00695695 ... -0.02484347  0.07589656
   0.00145728]
 [-0.06250091  0.0204758  -0.08398502 ... -0.03466835 -0.01359157
   0.11761193]
 [ 0.00186083 -0.05821304  0.04813076 ...  0.00969432 -0.06628132
   0.01761963]
 ...
 [ 0.02230619 -0.02870021  0.01504134 ... -0.02042587 -0.03470244
   0.06553464]
 [ 0.06515216  0.10249501  0.02737367 ... -0.10096183  0.06662795
   0.10167995]
 [ 0.07667467 -0.0014204  -0.04711976 ... -0.04640038 -0.05944907
   0.03427447]]

transformer_blocks.9.feed_forward.2.bias — shape: torch.Size([128])
[-0.03785552 -0.06995956 -0.08497281  0.02981436 -0.01109692  0.05636482
 -0.06180994  0.05547234 -0.03984018  0.06067327  0.01178481  0.07425574
  0.06166835  0.02613036 -0.00888595  0.03388001  0.06836964 -0.04634199
  0.02981581 -0.03697002  0.02285401  0.06175882 -0.00848753  0.00900235
  0.06370513 -0.06379195  0.04297874  0.02151106 -0.04834951  0.07103695
 -0.07088494 -0.0112459   0.05600146 -0.00621287 -0.07318532 -0.05418392
 -0.07163458 -0.06305075 -0.05250747  0.01779349 -0.02731325  0.0010725
  0.03260695  0.01652497  0.00555826 -0.07384085 -0.05012663 -0.08217033
 -0.06543303 -0.02315411 -0.06881018  0.03875673  0.0122171   0.02909078
 -0.08295549  0.02804939  0.05690227  0.03953831  0.00394441  0.04314702
 -0.04615946  0.05918037 -0.02092629  0.06748585 -0.06152286  0.06072355
 -0.01263781  0.07156195 -0.06824509  0.02914109  0.01617058 -0.02314375
  0.077461   -0.00509332 -0.07765228 -0.0012364  -0.07837206  0.04334027
  0.06473002 -0.06081934 -0.08623618  0.05945448  0.05796321 -0.09869874
 -0.08530236 -0.00723085  0.03606961  0.07456455 -0.07087961  0.08020329
  0.07772762  0.00856582 -0.01624196 -0.04361373 -0.06888442 -0.03901065
  0.00270724 -0.01921419 -0.04995048  0.07166112 -0.0667923   0.0241801
  0.00074553 -0.06024823  0.08096595  0.0775754  -0.0311995   0.03827528
 -0.01166594  0.07739964  0.0046304  -0.00696528 -0.07380524  0.07262924
  0.01430541 -0.04511195  0.05349264 -0.02609487 -0.06689416  0.04048607
  0.03963857 -0.01373462  0.00308626 -0.0926911   0.06742118 -0.05149617
 -0.04758175 -0.04293279]

final_layer_norm.weight — shape: torch.Size([128])
[1.0806384 1.0669144 1.086447  1.0930654 1.0851169 1.0932944 1.092589
 1.0891477 1.0908016 1.0860311 1.0861859 1.0778782 1.084484  1.0720686
 1.0868528 1.0870887 1.0784999 1.0876694 1.0792567 1.0908383 1.0786347
 1.0729913 1.0765245 1.088239  1.083031  1.0822753 1.0772063 1.0939825
 1.0732172 1.0717676 1.0866587 1.0863901 1.0806472 1.0891311 1.082847
 1.0863583 1.081705  1.0769324 1.0855826 1.0792736 1.0849644 1.0729862
 1.082773  1.081044  1.0749868 1.090058  1.0793966 1.0870351 1.0730408
 1.083599  1.0838362 1.0779461 1.0877559 1.0868765 1.0788429 1.0886378
 1.0782499 1.0761783 1.0778953 1.0726427 1.0821236 1.0762397 1.0767777
 1.0900022 1.0797793 1.0746286 1.0825185 1.0900815 1.0821432 1.074833
 1.0824125 1.0848535 1.087245  1.0843339 1.0818965 1.0929314 1.0883312
 1.0818129 1.0761086 1.0788572 1.0877389 1.0914366 1.0941979 1.0824715
 1.0799971 1.074039  1.0805353 1.0815467 1.0896934 1.0743718 1.0774503
 1.0880293 1.0786887 1.0924835 1.0771903 1.0853004 1.0865196 1.0829391
 1.0782254 1.0882477 1.0706629 1.0858895 1.0850835 1.0688096 1.084682
 1.0880352 1.0775224 1.0827693 1.0912375 1.0881531 1.0875844 1.093543
 1.0742327 1.0861136 1.0852358 1.0837172 1.0804356 1.0898535 1.076339
 1.0736614 1.0867083 1.0812435 1.0903778 1.0865023 1.0793741 1.0840024
 1.0760117 1.079114 ]

final_layer_norm.bias — shape: torch.Size([128])
[-0.08327529 -0.05149071 -0.0772079   0.07407005  0.0628651   0.09267154
  0.07826213  0.05764721  0.10639518 -0.06570573 -0.00917031  0.06099843
 -0.07330748  0.06346401  0.06131462 -0.05986967  0.06209822 -0.07001747
  0.06777806 -0.0700341  -0.0667166   0.06767509  0.06808014  0.06815321
 -0.06088212  0.06726073 -0.07636194  0.02680338 -0.0581356  -0.05582376
  0.06363431 -0.07699972  0.05654674 -0.08086099 -0.0590337  -0.06225273
  0.0685387  -0.06742519  0.07804949  0.06132343  0.06523521  0.05366648
  0.06378236 -0.04723026 -0.05159211 -0.04472866  0.05893221 -0.02781622
  0.05873127  0.06281842 -0.05505543  0.07414051 -0.05905814  0.08333146
 -0.06186362  0.06724848 -0.01580758 -0.06652295  0.05624681  0.05913566
 -0.07454137  0.05639199  0.06602948 -0.06254899 -0.06473853 -0.03348725
 -0.04027151  0.07002459 -0.0577599  -0.05966881  0.06068124 -0.05409783
  0.06349771 -0.06430933 -0.06084636  0.05352855 -0.07820312  0.05942564
  0.05991324 -0.05807263 -0.06135643  0.07613105 -0.07936265 -0.06611849
 -0.05895773  0.05351466  0.06527017 -0.06152743  0.05788704  0.02650381
 -0.05394425  0.06201791  0.05958823 -0.05546988  0.06315303 -0.06559878
  0.06258751  0.07990201  0.05859326 -0.07082264  0.05567423  0.07047866
 -0.05068446 -0.06581673 -0.06023675  0.05853664 -0.05206295 -0.06105969
 -0.06389327  0.06410919  0.0709273  -0.06462596 -0.06439033  0.05785364
 -0.06330984 -0.060477   -0.06410201  0.06604195 -0.06167212 -0.03352395
 -0.01076724 -0.06825905 -0.01236166 -0.06817859 -0.06447724 -0.06430686
  0.05453583 -0.05999855]

output_projection.weight — shape: torch.Size([211, 128])
[[ 0.06244342 -0.00768715  0.0277693  ... -0.01059735 -0.03485281
   0.10579326]
 [ 0.11531403  0.05716929  0.08641594 ...  0.09724806 -0.0341059
  -0.00861273]
 [-0.05119023  0.08667531 -0.09358885 ...  0.06739221 -0.06925114
   0.15030444]
 ...
 [-0.03315111 -0.09764883 -0.01641936 ...  0.10348352  0.01012832
  -0.00084794]
 [-0.0827071  -0.0264282  -0.06894877 ... -0.06679939  0.08676349
  -0.13362905]
 [ 0.04841084  0.11023849 -0.1179237  ...  0.04117826 -0.13300514
  -0.03483866]]

output_projection.bias — shape: torch.Size([211])
[-0.07956193 -0.01050501  0.08126801  0.06266287  0.09924414  0.00962353
 -0.06046297 -0.09181983  0.01512674 -0.04958764  0.09182338  0.04195928
 -0.07305948 -0.09296986  0.00875755  0.00831127 -0.06707243 -0.11846592
 -0.08807895 -0.10808893 -0.06917564 -0.08406415 -0.05373749  0.11634014
  0.04529094  0.00936774  0.00626933 -0.12302525 -0.05799067  0.01235364
 -0.04739242  0.02483666  0.00677691 -0.06054636  0.05253866 -0.04570712
  0.01660637 -0.10488383 -0.07142279 -0.09765597  0.03200115 -0.12160064
  0.01849329  0.02845345 -0.04721173 -0.04868303  0.09036992 -0.05523118
 -0.03759883 -0.00136111 -0.05089264 -0.11476176  0.10215768  0.02034187
  0.02282044 -0.07636042  0.02818048 -0.12552525 -0.10011595  0.01138936
 -0.07012492 -0.00033577 -0.07220843 -0.05147841  0.00435959  0.02763231
 -0.0626515   0.11990485 -0.01760825 -0.0292833  -0.04720433  0.07635829
  0.03725089  0.08050501 -0.00179869  0.12312623 -0.11983941 -0.03592961
  0.00262942 -0.03788013  0.0087639   0.03124724  0.04749317 -0.02832031
 -0.09332336 -0.08611338 -0.01161242 -0.12447128 -0.0022617  -0.10318977
 -0.09424077 -0.07636113 -0.10115054  0.04077443 -0.01896304 -0.00991582
 -0.02905172 -0.11737224 -0.06112964 -0.09881156 -0.02325742 -0.01193146
 -0.06292707  0.06295569 -0.00779123 -0.12030175 -0.10223678  0.03798593
  0.06773737  0.02316937  0.03954905  0.05926684 -0.03564019 -0.0888631
  0.00336323 -0.00312057 -0.12682573 -0.05266736 -0.11655389  0.0340585
  0.02349073  0.04517625 -0.04823203  0.00935503  0.0409594   0.02796629
  0.01150884 -0.08890712 -0.04214311  0.00178235 -0.00634277 -0.02869931
 -0.02854718  0.02026126 -0.04131776 -0.04103235 -0.045749   -0.07360379
 -0.01082585 -0.0914233  -0.08041591 -0.03810315 -0.08207426  0.01473428
  0.01921345 -0.11317775 -0.00245596  0.0458624  -0.11511328  0.03792102
 -0.07010867  0.04614338 -0.06616635 -0.03000338 -0.03912362 -0.09628417
  0.01969788 -0.02917286  0.04143042  0.09645863 -0.03625565  0.09889157
  0.01976255  0.04888423 -0.12500644  0.06144052  0.0321113  -0.12534048
  0.08715476  0.01026604  0.00090584  0.01850336  0.08545295  0.02195984
 -0.10842003 -0.00423499  0.07308022 -0.02711705  0.06952503  0.05335888
  0.07293467  0.08957268 -0.05452957  0.04440729 -0.05511907 -0.00147007
 -0.06345969  0.10024023  0.00754806 -0.03398692  0.02391431 -0.0024194
  0.06656849  0.01539553  0.06731231  0.02227566 -0.08719348 -0.04075861
  0.06012929  0.08457523 -0.04756888  0.1012563  -0.04442593  0.0395089
  0.08740856  0.01347524  0.06719781  0.07244114  0.02397875 -0.04075846
 -0.03851107]

